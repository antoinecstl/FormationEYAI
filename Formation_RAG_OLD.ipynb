{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eaa1feab",
   "metadata": {},
   "source": [
    "# üöÄ Formation IA : Cr√©er un Chatbot RAG\n",
    "## Retrieval-Augmented Generation pour l'analyse de documents PDF\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ **Objectifs de la formation**\n",
    "\n",
    "√Ä la fin de cette formation, vous saurez :\n",
    "- ‚úÖ Configurer un environnement IA sur Google Colab\n",
    "- ‚úÖ Installer et utiliser Ollama pour faire tourner des mod√®les localement\n",
    "- ‚úÖ Cr√©er un syst√®me RAG (Retrieval-Augmented Generation)\n",
    "- ‚úÖ D√©velopper une interface web avec Streamlit\n",
    "- ‚úÖ Analyser et questionner des documents PDF avec l'IA\n",
    "\n",
    "### üìö **Qu'est-ce que le RAG ?**\n",
    "\n",
    "Le **RAG (Retrieval-Augmented Generation)** est une technique qui combine :\n",
    "- üîç **Recherche** : Trouve les informations pertinentes dans vos documents\n",
    "- ü§ñ **G√©n√©ration** : Utilise un mod√®le IA pour formuler des r√©ponses bas√©es sur ces informations\n",
    "\n",
    "C'est comme avoir un assistant qui lit vos documents et r√©pond √† vos questions de mani√®re intelligente !\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baff6c37",
   "metadata": {},
   "source": [
    "## üîó Section 1 : Connexion √† Google Drive\n",
    "\n",
    "### üìò Description p√©dagogique (non technique)\n",
    "**Cette cellule permet de connecter Google Drive √† l'environnement de travail.** Cela permet d'utiliser des fichiers (ex : jeux de donn√©es, documents, mod√®les) qui sont stock√©s dans votre Google Drive directement dans le Notebook.\n",
    "\n",
    "### üí° Pourquoi on fait √ßa ?\n",
    "Cela √©vite de devoir uploader manuellement des fichiers √† chaque fois et permet de sauvegarder facilement notre travail.\n",
    "\n",
    "### üõ†Ô∏è Instructions :\n",
    "1. Ex√©cutez la cellule ci-dessous\n",
    "2. Cliquez sur le lien qui appara√Æt\n",
    "3. Connectez-vous √† votre compte Google\n",
    "4. Copiez le code d'autorisation\n",
    "5. Collez-le dans la zone de texte qui appara√Æt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ecdfd01d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgoogle\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcolab\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[32m      2\u001b[39m drive.mount(\u001b[33m\"\u001b[39m\u001b[33m/content/gdrive\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m‚úÖ Google Drive connect√© avec succ√®s !\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount(\"/content/gdrive\")\n",
    "print(\"‚úÖ Google Drive connect√© avec succ√®s !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de39dc2",
   "metadata": {},
   "source": [
    "## üìÅ Section 2 : Configuration de l'environnement de travail\n",
    "\n",
    "### üìò Description p√©dagogique (non technique)\n",
    "**Cette cellule permet de changer le dossier de travail du notebook pour aller dans le dossier principal de votre Google Drive** (Mon Drive).\n",
    "Cela facilite l'acc√®s direct aux fichiers sans avoir √† √©crire un chemin complet √† chaque fois.\n",
    "\n",
    "### üí° Pourquoi on fait √ßa ?\n",
    "En changeant de dossier, on peut lire et enregistrer des fichiers (ex : jeux de donn√©es, r√©sultats, images‚Ä¶) comme si on √©tait d√©j√† dans Google Drive, ce qui simplifie beaucoup les manipulations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7110ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd gdrive/MyDrive\n",
    "print(\"üìÇ R√©pertoire de travail chang√© vers Google Drive\")\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4096cf12",
   "metadata": {},
   "source": [
    "## üåê Section 3 : T√©l√©chargement des fichiers n√©cessaires depuis GitHub\n",
    "\n",
    "### üìò Description p√©dagogique (non technique)\n",
    "Cette cellule permet de copier un dossier contenant des fichiers depuis GitHub vers l'environnement de travail du notebook.\n",
    "\n",
    "### üí° Pourquoi on fait √ßa ?\n",
    "GitHub est une plateforme utilis√©e pour stocker, partager du code ou des fichiers de projet. Ici, on t√©l√©charge les ressources n√©cessaires √† la formation pour pouvoir les utiliser facilement dans les cellules suivantes.\n",
    "\n",
    "### üì¶ Ce qu'on t√©l√©charge :\n",
    "- Le code de l'application Streamlit\n",
    "- Les fichiers de configuration\n",
    "- Les mod√®les et outils n√©cessaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a65e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/antoinecstl/FormationEYAI.git\n",
    "print(\"‚úÖ Fichiers de formation t√©l√©charg√©s depuis GitHub\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a6d7ec",
   "metadata": {},
   "source": [
    "### üìÇ D√©placement vers le dossier de formation\n",
    "\n",
    "### üìò Description p√©dagogique (non technique)\n",
    "Cette cellule permet de se d√©placer dans le dossier FormationEYAI que vous venez de t√©l√©charger depuis GitHub.\n",
    "\n",
    "### üí° Pourquoi on fait √ßa ?\n",
    "En se pla√ßant dans ce dossier, on pourra acc√©der plus facilement aux fichiers utiles pour la suite de la formation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438afe08",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd FormationEYAI\n",
    "print(\"üìÅ Maintenant dans le dossier FormationEYAI\")\n",
    "!ls -la"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9bdff8",
   "metadata": {},
   "source": [
    "## üì¶ Section 4 : Installation des d√©pendances Python\n",
    "\n",
    "### üìò Description p√©dagogique (non technique)\n",
    "**Cette cellule permet d'installer automatiquement tous les outils n√©cessaires √† la formation √† partir d'un fichier sp√©cial appel√© requirements.txt.**\n",
    "Ce fichier contient la liste des modules Python dont on aura besoin pour que le code fonctionne.\n",
    "\n",
    "### üí° Pourquoi on fait √ßa ?\n",
    "Plut√¥t que d'installer chaque outil un par un, ce fichier centralise tout, ce qui fait gagner du temps et √©vite les erreurs d'oubli ou d'incompatibilit√©.\n",
    "\n",
    "### üîß Modules qui seront install√©s :\n",
    "- **Streamlit** : Pour cr√©er l'interface web\n",
    "- **Ollama** : Pour interagir avec les mod√®les IA\n",
    "- **FAISS** : Pour la recherche vectorielle rapide\n",
    "- **LangChain** : Pour le traitement de texte avanc√©\n",
    "- **PyPDF2** : Pour lire les fichiers PDF\n",
    "- **NumPy & scikit-learn** : Pour les calculs math√©matiques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93323186",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt\n",
    "print(\"‚úÖ Toutes les d√©pendances sont install√©es !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b66c26f",
   "metadata": {},
   "source": [
    "## ü§ñ Section 5 : Installation et lancement d'Ollama\n",
    "\n",
    "### üìò Description p√©dagogique (non technique)\n",
    "Cette cellule t√©l√©charge et installe Ollama, un outil qui permet de faire tourner des mod√®les d'intelligence artificielle (comme des LLMs) localement sur la machine, sans avoir besoin d'une connexion √† un service cloud.\n",
    "\n",
    "### üí° Pourquoi on fait √ßa ?\n",
    "Cela permet de tester des mod√®les d'IA de mani√®re autonome, sans d√©pendre de services externes. C'est particuli√®rement utile pour des raisons de confidentialit√©, de co√ªt ou de performance locale.\n",
    "\n",
    "### üîí Avantages d'Ollama :\n",
    "- **Gratuit** : Pas de co√ªts d'API\n",
    "- **Priv√©** : Vos donn√©es restent locales\n",
    "- **Rapide** : Pas de latence r√©seau\n",
    "- **Flexible** : Plusieurs mod√®les disponibles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89ac49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -fsSL https://ollama.com/install.sh | sh\n",
    "print(\"‚úÖ Ollama install√© avec succ√®s !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9aed63",
   "metadata": {},
   "source": [
    "### üöÄ D√©marrage du serveur Ollama\n",
    "\n",
    "### üìò Description p√©dagogique (non technique)\n",
    "Cette cellule sert √† lancer le serveur Ollama, c'est-√†-dire √† d√©marrer l'outil qui fera fonctionner un mod√®le d'IA localement.\n",
    "\n",
    "### üí° Pourquoi on fait √ßa ?\n",
    "Pour interagir avec un mod√®le d'intelligence artificielle install√© localement, il faut d'abord d√©marrer un service qui ¬´ √©coute ¬ª et attend nos demandes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a5434a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import time\n",
    "\n",
    "# D√©marrage du serveur Ollama en arri√®re-plan\n",
    "sub = subprocess.Popen(\"ollama serve\", shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "print(\"üöÄ Serveur Ollama d√©marr√© !\")\n",
    "print(\"‚è≥ Attente de 5 secondes pour que le serveur soit pr√™t...\")\n",
    "time.sleep(5)\n",
    "print(\"‚úÖ Serveur Ollama op√©rationnel !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221a922a",
   "metadata": {},
   "source": [
    "## üß† Section 6 : T√©l√©chargement des mod√®les d'IA\n",
    "\n",
    "### üìò Description p√©dagogique (non technique)\n",
    "Ces deux commandes permettent de t√©l√©charger des mod√®les d'intelligence artificielle (IA) que l'on utilisera ensuite dans le projet.\n",
    "\n",
    "### ü§ñ Mod√®les t√©l√©charg√©s :\n",
    "\n",
    "1. **llama3.2** : un mod√®le de langage (LLM), capable de r√©pondre √† des questions, g√©n√©rer du texte, r√©sumer, etc.\n",
    "\n",
    "2. **nomic-embed-text** : un mod√®le qui transforme des textes en ¬´ vecteurs ¬ª, une forme que les machines peuvent comprendre pour faire des recherches s√©mantiques ou des comparaisons de sens.\n",
    "\n",
    "### üí° Pourquoi on fait √ßa ?\n",
    "Les LLM sont pr√©-entra√Æn√©s. Pour pouvoir les utiliser, il faut d'abord les t√©l√©charger sur votre machine, un peu comme si vous installiez une application.\n",
    "\n",
    "Sans cela, le syst√®me ne saura pas quel mod√®le utiliser, ni comment r√©pondre aux questions ou traiter les textes.\n",
    "\n",
    "### ‚ö†Ô∏è Note importante :\n",
    "Le t√©l√©chargement peut prendre plusieurs minutes selon votre connexion internet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76affdd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üì• T√©l√©chargement du mod√®le de langage llama3.2...\")\n",
    "!ollama pull llama3.2\n",
    "print(\"‚úÖ Mod√®le llama3.2 t√©l√©charg√© !\")\n",
    "\n",
    "print(\"\\nüì• T√©l√©chargement du mod√®le d'embedding nomic-embed-text...\")\n",
    "!ollama pull nomic-embed-text\n",
    "print(\"‚úÖ Mod√®le nomic-embed-text t√©l√©charg√© !\")\n",
    "\n",
    "print(\"\\nüéâ Tous les mod√®les sont pr√™ts √† √™tre utilis√©s !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c6e488",
   "metadata": {},
   "source": [
    "## üñ•Ô∏è Section 7 : Cr√©ation de l'interface utilisateur avec Streamlit\n",
    "\n",
    "### üìò Description p√©dagogique (non technique)\n",
    "Nous allons maintenant cr√©er le fichier `app.py` qui contient tout le code de notre application RAG PDF Chat. Cette application permettra :\n",
    "\n",
    "### üéØ Fonctionnalit√©s de l'application :\n",
    "- üìÑ **Upload de PDF** : D√©poser des documents PDF\n",
    "- üîç **Indexation intelligente** : Analyser et d√©couper les documents\n",
    "- üí¨ **Chat interactif** : Poser des questions sur vos documents\n",
    "- üé® **Interface moderne** : Design professionnel avec Streamlit\n",
    "- üìä **Affichage des sources** : Voir d'o√π viennent les r√©ponses\n",
    "\n",
    "### üß† Comment √ßa marche ?\n",
    "1. **Extraction** : Le texte est extrait des PDF\n",
    "2. **D√©coupage** : Le texte est divis√© en chunks intelligents\n",
    "3. **Vectorisation** : Chaque chunk devient un vecteur math√©matique\n",
    "4. **Indexation** : Les vecteurs sont stock√©s dans une base de donn√©es rapide\n",
    "5. **Recherche** : Quand vous posez une question, on trouve les chunks pertinents\n",
    "6. **G√©n√©ration** : Le mod√®le IA formule une r√©ponse bas√©e sur ces chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c299f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©ation du fichier app.py avec le code complet de l'application RAG\n",
    "app_code = '''\n",
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import tempfile\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "\n",
    "import faiss  # type: ignore\n",
    "import numpy as np\n",
    "import streamlit as st\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import ollama  # pip install ollama-python\n",
    "\n",
    "# --------------------- UI & style -----------------------------------------\n",
    "st.set_page_config(page_title=\"ü§ñ RAG PDF Chat\", page_icon=\"ü§ñ\", layout=\"wide\")\n",
    "\n",
    "st.markdown(\n",
    "    \"\"\"\n",
    "<style>\n",
    "html, body {\n",
    "    font-family: 'Helvetica Neue', sans-serif;\n",
    "    background-color: #f4f7fa;\n",
    "}\n",
    ".sidebar .sidebar-content {\n",
    "    background-color: #ffffff;\n",
    "}\n",
    ".chat-container {\n",
    "    max-width: 1200px;\n",
    "    margin: auto;\n",
    "    padding: 1rem;\n",
    "}\n",
    ".user-msg {\n",
    "    background: #007bff;\n",
    "    color: white;\n",
    "    border-radius: 20px 20px 0px 20px;\n",
    "    padding: 1rem;\n",
    "    margin: 0.5rem 0;\n",
    "    align-self: flex-end;\n",
    "    max-width: 100%;\n",
    "    word-wrap: break-word;\n",
    "}\n",
    ".bot-msg {\n",
    "    background: #e9ecef;\n",
    "    color: #212529;\n",
    "    border-radius: 20px 20px 20px 0px;\n",
    "    padding: 1rem;\n",
    "    margin: 0.5rem 0;\n",
    "    align-self: flex-start;\n",
    "    max-width: 100%;\n",
    "    word-wrap: break-word;\n",
    "}\n",
    ".chat-area {\n",
    "    display: flex;\n",
    "    flex-direction: column;\n",
    "}\n",
    "</style>\n",
    "\"\"\",\n",
    "    unsafe_allow_html=True,\n",
    ")\n",
    "\n",
    "# --------------------- Config ---------------------------------------------\n",
    "MODEL_NAME = \"llama3.2:3b\"\n",
    "EMBEDDING_MODEL = \"nomic-embed-text:latest\"\n",
    "DOC_TOP_K = 3\n",
    "CHUNK_TOP_K = 5\n",
    "CANDIDATES_K = 20\n",
    "NEIGHBORS = 1\n",
    "LAMBDA_DIVERSITY = 0.3\n",
    "SIM_THRESHOLD = 0.25\n",
    "TEMPERATURE = 0.2\n",
    "MAX_TOKENS = 2048\n",
    "\n",
    "# --------------------- Helpers LLM ----------------------------------------\n",
    "\n",
    "def _call_llm(messages: List[Dict[str, str]], *, temperature: float = 0.1, max_tokens: int = 2048, stream: bool = False):\n",
    "    return ollama.chat(\n",
    "        model=MODEL_NAME,\n",
    "        messages=messages,\n",
    "        stream=stream,\n",
    "        options={\"temperature\": temperature, \"num_predict\": max_tokens},\n",
    "    )\n",
    "\n",
    "\n",
    "def embed_texts(texts: List[str]) -> np.ndarray:\n",
    "    return np.array([ollama.embeddings(model=EMBEDDING_MODEL, prompt=t)[\"embedding\"] for t in texts], dtype=\"float32\")\n",
    "\n",
    "# --------------------- PDF utils -----------------------------------------\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    import re\n",
    "    text = re.sub(r\"\\n{2,}\", \"\\n\\n\", text)\n",
    "    text = re.sub(r\"(\\w+)-\\n(\\w+)\", r\"\\1\\2\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def extract_pdf_text(path: str) -> str:\n",
    "    return clean_text(\"\\n\".join(page.extract_text() or \"\" for page in PdfReader(path).pages))\n",
    "\n",
    "# --------------------- Chunking & r√©sum√© ----------------------------------\n",
    "\n",
    "def auto_chunk_size(tokens: int) -> int:\n",
    "    return 1024 if tokens < 8000 else 768 if tokens < 20000 else 512\n",
    "\n",
    "\n",
    "def chunk_document(text: str) -> List[str]:\n",
    "    size = auto_chunk_size(len(text.split()))\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        separators=[\"\\n\\n\", \"\\n\", \". \"],\n",
    "        chunk_size=size,\n",
    "        chunk_overlap=int(size*0.25),\n",
    "        length_function=len,\n",
    "    )\n",
    "    return [c for c in splitter.split_text(text) if len(c) > 100]\n",
    "\n",
    "\n",
    "def make_summary(text: str) -> str:\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"Vous √™tes un expert en synth√®se documentaire. R√©sumez le texte suivant en trois parties : (1) Contexte, (2) Points cl√©s, (3) Conclusions. R√©pondez en fran√ßais.\"},\n",
    "        {\"role\": \"user\", \"content\": text[:120000]}\n",
    "    ]\n",
    "    return _call_llm(messages)[\"message\"][\"content\"].strip()\n",
    "\n",
    "# --------------------- Index hi√©rarchique ---------------------------------\n",
    "class RagIndex:\n",
    "    def __init__(self):\n",
    "        self.doc_index: Optional[faiss.IndexFlatIP] = None\n",
    "        self.chunk_index: Optional[faiss.Index] = None\n",
    "        self.doc_meta: List[Dict[str, Any]] = []\n",
    "        self.chunk_meta: List[Dict[str, Any]] = []\n",
    "        self.chunk_emb: Optional[np.ndarray] = None\n",
    "\n",
    "    def build(self, uploaded_files: List[st.runtime.uploaded_file_manager.UploadedFile]):\n",
    "        doc_embs, chunk_embs_list = [], []\n",
    "        for doc_id, uf in enumerate(uploaded_files):\n",
    "            with tempfile.NamedTemporaryFile(delete=False, suffix=\".pdf\") as tmp:\n",
    "                tmp.write(uf.getbuffer())\n",
    "                path = tmp.name\n",
    "            full_text = extract_pdf_text(path)\n",
    "            os.unlink(path)\n",
    "\n",
    "            summary = make_summary(full_text)\n",
    "            self.doc_meta.append({\"filename\": uf.name, \"summary\": summary})\n",
    "            doc_embs.append(embed_texts([summary])[0])\n",
    "\n",
    "            chunks = chunk_document(full_text)\n",
    "            chunk_embs = embed_texts(chunks)\n",
    "            chunk_embs_list.append(chunk_embs)\n",
    "            for i, txt in enumerate(chunks):\n",
    "                self.chunk_meta.append({\"doc_id\": doc_id, \"text\": txt, \"chunk_id\": i})\n",
    "\n",
    "        self.doc_index = faiss.IndexFlatIP(len(doc_embs[0]))\n",
    "        self.doc_index.add(np.vstack(doc_embs).astype(\"float32\"))\n",
    "\n",
    "        self.chunk_emb = np.vstack(chunk_embs_list).astype(\"float32\")\n",
    "        self.chunk_index = faiss.IndexHNSWFlat(self.chunk_emb.shape[1], 32)\n",
    "        self.chunk_index.add(self.chunk_emb)\n",
    "\n",
    "    def _is_global(self, query: str, thr: float = 0.78) -> bool:\n",
    "        examples = [\n",
    "            \"De quoi parle ce document ?\",\n",
    "            \"Quel est le sujet principal ?\",\n",
    "            \"Fais un r√©sum√© du document\",\n",
    "        ]\n",
    "        emb_q = embed_texts([query])[0]\n",
    "        emb_ex = embed_texts(examples)\n",
    "        sims = emb_ex @ emb_q / (np.linalg.norm(emb_ex, axis=1) * np.linalg.norm(emb_q) + 1e-6)\n",
    "        return float(np.max(sims)) >= thr\n",
    "\n",
    "    def _mmr(self, q: np.ndarray, cand: np.ndarray, k: int) -> List[int]:\n",
    "        selected, rest = [], list(range(len(cand)))\n",
    "        while len(selected) < min(k, len(rest)):\n",
    "            best, best_score = None, -1e9\n",
    "            for idx in rest:\n",
    "                sim_q = float(q @ cand[idx] / (np.linalg.norm(q) * np.linalg.norm(cand[idx]) + 1e-6))\n",
    "                sim_s = max(cosine_similarity(cand[idx][None, :], cand[selected])[0]) if selected else 0.\n",
    "                score = LAMBDA_DIVERSITY*sim_q - (1-LAMBDA_DIVERSITY)*sim_s\n",
    "                if score > best_score:\n",
    "                    best, best_score = idx, score\n",
    "            selected.append(best)\n",
    "            rest.remove(best)\n",
    "        return selected\n",
    "\n",
    "    def retrieve(self, query: str) -> Tuple[List[str], List[int], Optional[str]]:\n",
    "        q_emb = embed_texts([query])[0]\n",
    "        _, I_doc = self.doc_index.search(q_emb[None, :], DOC_TOP_K)\n",
    "        allowed = set(I_doc[0])\n",
    "\n",
    "        mask = [i for i, m in enumerate(self.chunk_meta) if m[\"doc_id\"] in allowed]\n",
    "        sub_emb = self.chunk_emb[mask]\n",
    "        sub_idx = faiss.IndexFlatIP(sub_emb.shape[1])\n",
    "        sub_idx.add(sub_emb)\n",
    "        D, I = sub_idx.search(q_emb[None, :], min(CANDIDATES_K, len(mask)))\n",
    "        pool = [mask[idx] for idx in I[0]]\n",
    "        pool = [idx for idx, d in zip(pool, D[0]) if 1-d <= SIM_THRESHOLD] or [mask[I[0][0]]]\n",
    "\n",
    "        cand_emb = self.chunk_emb[pool]\n",
    "        selected = [pool[i] for i in self._mmr(q_emb, cand_emb, CHUNK_TOP_K)]\n",
    "        expanded = {j for idx in selected for j in range(idx-NEIGHBORS, idx+NEIGHBORS+1)}\n",
    "        final = [i for i in expanded if 0 <= i < len(self.chunk_meta)][:CHUNK_TOP_K]\n",
    "\n",
    "        contexts = [self.chunk_meta[i][\"text\"] for i in final]\n",
    "        summary = self.doc_meta[int(I_doc[0][0])][\"summary\"] if self._is_global(query) else None\n",
    "        return contexts, final, summary\n",
    "\n",
    "# ---------------- Prompt builder -----------------------------------------\n",
    "def build_prompt(question: str, contexts: List[str], summary: Optional[str], history: List[Dict[str, str]] = None):\n",
    "    ctx_block = \"\\n\\n\".join(f\"[{i+1}] {c}\" for i, c in enumerate(contexts))\n",
    "    if summary:\n",
    "        ctx_block = f\"[R√©sum√©] {summary}\\n\\n\" + ctx_block\n",
    "    \n",
    "    system = (\n",
    "        \"Vous √™tes un assistant expert. Utilisez uniquement les informations suivantes pour r√©pondre en fran√ßais. \"\n",
    "        \"Citez vos sources avec les balises [n]. Si l'information n'est pas trouv√©e, informez-en l'utilisateur.\"\n",
    "    )\n",
    "    \n",
    "    messages = [{\"role\": \"system\", \"content\": system}]\n",
    "    \n",
    "    if history and len(history) > 0:\n",
    "        previous_messages = history[:-1] if history[-1][\"role\"] == \"user\" else history\n",
    "        messages.extend(previous_messages)\n",
    "    \n",
    "    current_user_content = f\"CONTEXTE(S):\\n{ctx_block}\\n\\nQUESTION: {question}\\n\\nR√©ponse:\"\n",
    "    messages.append({\"role\": \"user\", \"content\": current_user_content})\n",
    "    \n",
    "    return messages\n",
    "\n",
    "# ---------------- Etat Streamlit ----------------------------------------\n",
    "if \"messages\" not in st.session_state:\n",
    "    st.session_state.messages = []\n",
    "if \"rag\" not in st.session_state:\n",
    "    st.session_state.rag = None\n",
    "if \"processing\" not in st.session_state:\n",
    "    st.session_state.processing = False\n",
    "\n",
    "# ---------------- Sidebar upload ----------------------------------------\n",
    "with st.sidebar:\n",
    "    st.header(\"üìö Documents\")\n",
    "    files = st.file_uploader(\"D√©posez vos PDF\", type=[\"pdf\"], accept_multiple_files=True)\n",
    "    if st.button(\"üîÑ R√©initialiser\"):\n",
    "        st.session_state.clear()\n",
    "        st.rerun()\n",
    "\n",
    "# ---------------- Index construction ------------------------------------\n",
    "if files and st.session_state.rag is None:\n",
    "    with st.spinner(\"üìÑ Indexation en cours‚Ä¶\"):\n",
    "        rag = RagIndex()\n",
    "        rag.build(files)\n",
    "        st.session_state.rag = rag\n",
    "    st.success(f\"{len(files)} document(s) index√©(s) ! Posez vos questions.\")\n",
    "\n",
    "# ---------------- Chat display -----------------------------------------\n",
    "st.markdown(\"<div class='chat-container'>\", unsafe_allow_html=True)\n",
    "if not st.session_state.messages:\n",
    "    if st.session_state.rag is not None:\n",
    "        st.markdown(\n",
    "            \"\"\"\n",
    "            <div class=\"bot-msg\">\n",
    "            üëã Bonjour ! Je suis votre assistant IA documentaire.\n",
    "            <br>Posez-moi une question sur le contenu de vos documents.\n",
    "            </div>\n",
    "            \"\"\",\n",
    "            unsafe_allow_html=True\n",
    "        )\n",
    "    else:\n",
    "        st.markdown(\n",
    "            \"\"\"\n",
    "            <div class=\"bot-msg\">\n",
    "            üëã Bienvenue dans RAG PDF Chat !\n",
    "            <br>Commencez par t√©l√©charger un ou plusieurs documents PDF dans le panneau lat√©ral.\n",
    "            </div>\n",
    "            \"\"\",\n",
    "            unsafe_allow_html=True\n",
    "        )\n",
    "else:\n",
    "    st.markdown(\"<div class='chat-area'>\", unsafe_allow_html=True)\n",
    "    for msg in st.session_state.messages:\n",
    "        css = \"user-msg\" if msg[\"role\"] == \"user\" else \"bot-msg\"\n",
    "        st.markdown(f'<div class=\"{css}\">{msg[\"content\"]}</div>', unsafe_allow_html=True)\n",
    "    st.markdown(\"</div>\", unsafe_allow_html=True)\n",
    "st.markdown(\"</div>\", unsafe_allow_html=True)\n",
    "\n",
    "# ---------------- Chat input -------------------------------------------\n",
    "query = st.chat_input(\"Votre question‚Ä¶\", disabled=st.session_state.processing or st.session_state.rag is None)\n",
    "\n",
    "if query:\n",
    "    st.session_state.messages.append({\"role\": \"user\", \"content\": query})\n",
    "    st.markdown(f'<div class=\"user-msg\">{query}</div>', unsafe_allow_html=True)\n",
    "    st.session_state.processing = True\n",
    "\n",
    "    rag: RagIndex = st.session_state.rag  # type: ignore\n",
    "    contexts, indices, summary = rag.retrieve(query)\n",
    "    \n",
    "    prompt = build_prompt(query, contexts, summary, st.session_state.messages)\n",
    "\n",
    "    placeholder = st.empty()\n",
    "    collected_parts: List[str] = []\n",
    "\n",
    "    for chunk in _call_llm(prompt, temperature=TEMPERATURE, max_tokens=MAX_TOKENS, stream=True):\n",
    "        token = chunk[\"message\"][\"content\"]\n",
    "        collected_parts.append(token)\n",
    "        placeholder.markdown(f'<div class=\"bot-msg\">{\"\".join(collected_parts)}</div>', unsafe_allow_html=True)\n",
    "\n",
    "    full_answer = \"\".join(collected_parts)\n",
    "    st.session_state.messages.append({\"role\": \"assistant\", \"content\": full_answer})\n",
    "    st.session_state.processing = False\n",
    "\n",
    "    with st.expander(\"üîç Contextes\"):\n",
    "        for i, ctx in enumerate(contexts):\n",
    "            st.text_area(f\"[{i+1}]\", ctx, height=120)\n",
    "'''\n",
    "\n",
    "# √âcriture du fichier app.py\n",
    "with open('app.py', 'w', encoding='utf-8') as f:\n",
    "    f.write(app_code)\n",
    "\n",
    "print(\"‚úÖ Fichier app.py cr√©√© avec succ√®s !\")\n",
    "print(\"üìÅ L'application RAG PDF Chat est maintenant pr√™te.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26464fc6",
   "metadata": {},
   "source": [
    "## üß™ Section 8 : Test des composants principaux\n",
    "\n",
    "### üìò Description p√©dagogique (non technique)\n",
    "Avant de lancer l'application compl√®te, nous allons tester que tous les composants fonctionnent correctement :\n",
    "\n",
    "### üîç Tests √† effectuer :\n",
    "1. **Connexion Ollama** : V√©rifier que le serveur r√©pond\n",
    "2. **Mod√®les disponibles** : Lister les mod√®les t√©l√©charg√©s\n",
    "3. **Test simple** : G√©n√©rer une r√©ponse de test\n",
    "4. **Test d'embedding** : Transformer du texte en vecteurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05cb9f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "import time\n",
    "\n",
    "print(\"üîç Test de la connexion Ollama...\")\n",
    "try:\n",
    "    # Test de connexion\n",
    "    models = ollama.list()\n",
    "    print(\"‚úÖ Connexion Ollama r√©ussie !\")\n",
    "    print(f\"üìã Mod√®les disponibles : {len(models['models'])}\")\n",
    "    \n",
    "    for model in models['models']:\n",
    "        print(f\"  ü§ñ {model['name']}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erreur de connexion : {e}\")\n",
    "    print(\"‚è≥ Attente de 10 secondes suppl√©mentaires...\")\n",
    "    time.sleep(10)\n",
    "\n",
    "print(\"\\nüß™ Test de g√©n√©ration de texte...\")\n",
    "try:\n",
    "    response = ollama.chat(\n",
    "        model='llama3.2',\n",
    "        messages=[{\n",
    "            'role': 'user',\n",
    "            'content': 'Bonjour, peux-tu me dire en une phrase ce que tu peux faire ?'\n",
    "        }]\n",
    "    )\n",
    "    print(\"‚úÖ Test de g√©n√©ration r√©ussi !\")\n",
    "    print(f\"ü§ñ R√©ponse : {response['message']['content']}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erreur de g√©n√©ration : {e}\")\n",
    "\n",
    "print(\"\\nüî¢ Test d'embedding...\")\n",
    "try:\n",
    "    embedding = ollama.embeddings(\n",
    "        model='nomic-embed-text',\n",
    "        prompt='Ceci est un test d\\'embedding'\n",
    "    )\n",
    "    print(\"‚úÖ Test d'embedding r√©ussi !\")\n",
    "    print(f\"üìä Dimension du vecteur : {len(embedding['embedding'])}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erreur d'embedding : {e}\")\n",
    "\n",
    "print(\"\\nüéâ Tous les tests sont termin√©s !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb694fb3",
   "metadata": {},
   "source": [
    "## üöÄ Section 9 : Lancement de l'application RAG PDF Chat\n",
    "\n",
    "### üìò Description p√©dagogique (non technique)\n",
    "Cette section permet de lancer une interface web (une mini-application) et de la rendre accessible depuis Internet gr√¢ce √† un outil appel√© LocalTunnel.\n",
    "\n",
    "### üîß Voici ce que chaque ligne fait :\n",
    "\n",
    "**`!npm install localtunnel`**\n",
    "- Installe l'outil LocalTunnel, qui va permettre de partager l'application Streamlit via un lien web.\n",
    "\n",
    "**`import urllib + print(...)`**\n",
    "- Affiche l'adresse IP publique de votre environnement pour r√©f√©rence (souvent inutile c√¥t√© utilisateur, mais utile pour des logs ou du debug).\n",
    "\n",
    "**`!streamlit run app.py &>/content/logs.txt &`**\n",
    "- Lance l'application Streamlit (app.py) en arri√®re-plan. C'est cette application qui permet d'interagir avec le mod√®le IA via une interface utilisateur.\n",
    "\n",
    "**`npx localtunnel --port 8501`**\n",
    "- Cr√©e un lien temporaire et public vers l'application, utilisable depuis n'importe quel navigateur.\n",
    "\n",
    "### üí° Pourquoi on fait √ßa ?\n",
    "On cr√©e ici une interface simple et accessible (dans un navigateur) pour interagir avec le mod√®le IA, sans √©crire de code.\n",
    "Et comme Colab ou certains environnements locaux n'ont pas d'adresse web fixe, LocalTunnel sert de pont entre votre application et le reste du monde.\n",
    "\n",
    "### üåê Instructions importantes :\n",
    "1. **Cliquez sur le lien** qui appara√Ætra apr√®s l'ex√©cution\n",
    "2. **Entrez le mot de passe** qui s'affichera dans les logs\n",
    "3. **Profitez de votre application** RAG PDF Chat !\n",
    "\n",
    "### ‚ö†Ô∏è Attention :\n",
    "- Le lien LocalTunnel est temporaire et change √† chaque ex√©cution\n",
    "- L'application reste active tant que la cellule tourne\n",
    "- Pour arr√™ter, utilisez le bouton stop ou red√©marrez le runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9bb680e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!npm install localtunnel\n",
    "\n",
    "import urllib\n",
    "print(\"üîë Password/Endpoint IP for localtunnel is:\", urllib.request.urlopen('https://ipv4.icanhazip.com').read().decode('utf8').strip(\"\\n\"))\n",
    "print(\"\\nüöÄ Lancement de l'application RAG PDF Chat...\")\n",
    "print(\"üì± L'interface sera disponible via le lien LocalTunnel qui va appara√Ætre.\")\n",
    "print(\"‚è≥ Patientez quelques secondes...\")\n",
    "\n",
    "!streamlit run app.py &>/content/logs.txt & npx localtunnel --port 8501"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56dcdd0",
   "metadata": {},
   "source": [
    "## üéì Guide d'utilisation et conclusion\n",
    "\n",
    "### üéØ **F√©licitations !** \n",
    "\n",
    "Vous avez maintenant une application RAG compl√®tement fonctionnelle ! üéâ\n",
    "\n",
    "---\n",
    "\n",
    "### üìñ **Guide d'utilisation de votre application :**\n",
    "\n",
    "#### 1. **üìÑ Upload de documents**\n",
    "- Cliquez sur \"Browse files\" dans la sidebar\n",
    "- S√©lectionnez un ou plusieurs fichiers PDF\n",
    "- L'application va automatiquement les analyser et les indexer\n",
    "\n",
    "#### 2. **üí¨ Interaction avec l'IA**\n",
    "- Tapez votre question dans la zone de chat\n",
    "- L'IA va chercher les informations pertinentes dans vos documents\n",
    "- Elle formulera une r√©ponse bas√©e uniquement sur le contenu de vos PDF\n",
    "\n",
    "#### 3. **üîç V√©rification des sources**\n",
    "- Cliquez sur \"üîç Contextes\" pour voir les extraits utilis√©s\n",
    "- Chaque r√©ponse est sourc√©e et v√©rifiable\n",
    "\n",
    "---\n",
    "\n",
    "### üé® **Exemples de questions √† poser :**\n",
    "\n",
    "- \"De quoi parle ce document ?\"\n",
    "- \"Quels sont les points cl√©s ?\"\n",
    "- \"Peux-tu r√©sumer les conclusions ?\"\n",
    "- \"Que dit le document sur [sujet sp√©cifique] ?\"\n",
    "- \"Quelles sont les recommandations ?\"\n",
    "\n",
    "---\n",
    "\n",
    "### üõ†Ô∏è **Ce que vous avez appris :**\n",
    "\n",
    "‚úÖ **Configuration d'un environnement IA** sur Google Colab  \n",
    "‚úÖ **Installation et utilisation d'Ollama** pour les mod√®les locaux  \n",
    "‚úÖ **Cr√©ation d'un syst√®me RAG** avec indexation vectorielle  \n",
    "‚úÖ **D√©veloppement d'une interface web** avec Streamlit  \n",
    "‚úÖ **Traitement et analyse de documents PDF** avec l'IA  \n",
    "‚úÖ **Deployment avec LocalTunnel** pour partager votre application  \n",
    "\n",
    "---\n",
    "\n",
    "### üöÄ **Pour aller plus loin :**\n",
    "\n",
    "- **Personnaliser l'interface** : Modifier les couleurs et le style\n",
    "- **Ajouter d'autres formats** : Word, PowerPoint, etc.\n",
    "- **Am√©liorer les prompts** : Personnaliser les r√©ponses\n",
    "- **Optimiser les performances** : Ajuster les param√®tres RAG\n",
    "- **D√©ployer en production** : Utiliser des services cloud\n",
    "\n",
    "---\n",
    "\n",
    "### üìö **Ressources pour continuer :**\n",
    "\n",
    "- [Documentation Ollama](https://ollama.ai/)\n",
    "- [Documentation Streamlit](https://docs.streamlit.io/)\n",
    "- [Guide FAISS](https://github.com/facebookresearch/faiss)\n",
    "- [LangChain Documentation](https://docs.langchain.com/)\n",
    "\n",
    "---\n",
    "\n",
    "### ü§ù **Merci d'avoir suivi cette formation !**\n",
    "\n",
    "Vous ma√Ætrisez maintenant les bases du RAG et pouvez cr√©er vos propres applications d'IA documentaire. N'h√©sitez pas √† exp√©rimenter et √† adapter le code √† vos besoins sp√©cifiques !\n",
    "\n",
    "---\n",
    "\n",
    "**üí° Tip :** Sauvegardez ce notebook dans votre Google Drive pour pouvoir le r√©utiliser facilement !"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
