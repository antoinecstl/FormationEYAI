{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00b11ac5",
   "metadata": {},
   "source": [
    "# 🤖 Formation RAG – Notebook intégral"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e049e283",
   "metadata": {},
   "source": [
    "---\n",
    "## 🎯 Objectifs pédagogiques\n",
    "- Comprendre chaque composant d’un **RAG**\n",
    "- Manipuler le code dans Colab et exporter `app_base.py`\n",
    "- Installer **Ollama** et les modèles requis\n",
    "- Tester un prototype sur un PDF d’exemple\n",
    "- Découvrir les étapes de mise en production (Partie 2)\n",
    "\n",
    "> Les cellules `EXPORT` écrivent dans *app_base.py* via `%%writefile -a`. Vous pourrez ainsi le versionner, mais **on n’utilisera plus ce fichier en Partie 2** (remplacé par `app_finale.py`).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928f3a19",
   "metadata": {},
   "source": [
    "## 🚧 Séquence 1.0 – Installation d’Ollama & des modèles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02783f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title 🛠️ Installation Ollama (linux/Colab)\n",
    "!curl -fsSL https://ollama.com/install.sh | sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef6ba77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title 📥 Téléchargement des modèles\n",
    "!ollama pull llama3.2\n",
    "!ollama pull nomic-embed-text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08aca83",
   "metadata": {},
   "source": [
    "## 🔗 Séquence 1.1 – Bootstrap Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac33f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title 🔌 Connexion Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2bb2a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title 📥 Clone du dépôt\n",
    "%cd /content/gdrive/MyDrive\n",
    "!git clone https://github.com/antoinecstl/FormationEYAI.git\n",
    "%cd FormationEYAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7a32c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title 🛠️ Installation des dépendances\n",
    "!pip install -q -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d5143d",
   "metadata": {},
   "source": [
    "## 🔍 Séquence 1.2 – Bases du RAG : embeddings & similarité"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360710df",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile -a app_base.py\n",
    "import numpy as np, ollama\n",
    "\n",
    "EMBED_MODEL = \"nomic-embed-text:latest\"\n",
    "\n",
    "def embed_texts(texts):\n",
    "    \"\"\"Retourne un np.ndarray shape (n, d)\"\"\"\n",
    "    return np.array([ollama.embeddings(model=EMBED_MODEL, prompt=t)['embedding'] for t in texts], dtype='float32')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f380f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title 🔬 Test d'embedding\n",
    "phrase1 = \"Le soleil brille\"\n",
    "phrase2 = \"Aujourd'hui il fait beau\"\n",
    "# @markdown `phrase1` et `phrase2` sont des phrases à comparer.\n",
    "# @markdown La similarité entre les phrases est calculée en utilisant le produit scalaire des\n",
    "# @markdown embeddings normalisés de chaque phrase.\n",
    "# @markdown La similarité est un nombre entre -1 et 1, où 1 signifie que les phrases sont très similaires,\n",
    "# @markdown 0 signifie qu'elles ne sont pas similaires, et -1 signifie qu'elles sont opposées.\n",
    "# @markdown Vous pouvez modifier les phrases pour tester d'autres exemples.\n",
    "# @markdown Exécutez la cellule pour voir le résultat.\n",
    "vecs = embed_texts([phrase1, phrase2])\n",
    "sim = float(vecs[0] @ vecs[1] / (np.linalg.norm(vecs[0])*np.linalg.norm(vecs[1])))\n",
    "print(f\"Similarité : {sim:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad5dfe6",
   "metadata": {},
   "source": [
    "## 📐 Séquence 1.3 – Chunking & nettoyage d’un PDF d’exemple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdede86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile -a app_base.py\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "def auto_chunk_size(tok:int)->int:\n",
    "    return 1024 if tok<8000 else 768 if tok<20000 else 512\n",
    "\n",
    "def chunk_document(text:str):\n",
    "    size=auto_chunk_size(len(text.split()))\n",
    "    splitter=RecursiveCharacterTextSplitter(\n",
    "        separators=[\"\\n\\n\",\"\\n\",\". \"],\n",
    "        chunk_size=size,\n",
    "        chunk_overlap=size//4,\n",
    "        length_function=len,\n",
    "        )\n",
    "    return [c for c in splitter.split_text(text) if len(c)>100]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a544cc02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title 📖 Chargement du PDF d'exemple\n",
    "from PyPDF2 import PdfReader, PdfReadError\n",
    "\n",
    "sample_path = \"/rapport.pdf\"  # fourni dans le repo\n",
    "pages = PdfReader(sample_path).pages\n",
    "full_text = \"\\n\".join(p.extract_text() or \"\" for p in pages)\n",
    "\n",
    "print(f\"📄 Le document contient {len(full_text.split())} tokens environ.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d93bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title 🌳 Chunking du PDF\n",
    "chunks = chunk_document(full_text)\n",
    "print(f\"🌳 {len(chunks)} chunks créés. Aperçu :\\n{chunks[0][:300]}…\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5e0cab",
   "metadata": {},
   "source": [
    "## 📊 Séquence 1.4 – Index vectoriel FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce563806",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile -a app_base.py\n",
    "import faiss, numpy as np\n",
    "\n",
    "def build_faiss_index(vectors:np.ndarray)->faiss.IndexFlatIP:\n",
    "    d=vectors.shape[1]\n",
    "    idx=faiss.IndexFlatIP(d)\n",
    "    idx.add(vectors.astype('float32'))\n",
    "    return idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202c0cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title 🧪 Construction index chunks\n",
    "import numpy as np\n",
    "\n",
    "chunk_vecs = embed_texts(chunks)\n",
    "index = build_faiss_index(chunk_vecs)\n",
    "\n",
    "print(index.ntotal, \"vecteurs dans l'index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea4bdc9",
   "metadata": {},
   "source": [
    "## 🧮 Séquence 1.5 – Algorithme MMR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed16fd2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile -a app_base.py\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def mmr(query_vec:np.ndarray, cand:np.ndarray, k:int=5, λ:float=0.3):\n",
    "    selected, rest = [], list(range(len(cand)))\n",
    "    while len(selected)<min(k,len(rest)):\n",
    "        best, best_score = None, -1e9\n",
    "        for idx in rest:\n",
    "            sim_q = float(query_vec @ cand[idx]/(np.linalg.norm(query_vec)*np.linalg.norm(cand[idx])+1e-6))\n",
    "            sim_s = max(cosine_similarity(cand[idx][None,:], cand[selected])[0]) if selected else 0.\n",
    "            score = λ*sim_q - (1-λ)*sim_s\n",
    "            if score>best_score:\n",
    "                best, best_score = idx, score\n",
    "        selected.append(best); rest.remove(best)\n",
    "    return selected\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad559f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title 🔬 Test MMR\n",
    "import numpy as np\n",
    "\n",
    "q_vec = embed_texts([\"Sujet principal du rapport ?\"])[0]\n",
    "sel = mmr(q_vec, chunk_vecs, 3)\n",
    "print(sel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e41f64f",
   "metadata": {},
   "source": [
    "## 🧑‍🎤 Séquence 1.6 – Prompt engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7085934",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile -a app_base.py\n",
    "def build_prompt(question:str, ctxs:list[str]):\n",
    "    ctx_block=\"\\n\\n\".join(f\"[{i+1}] {c}\" for i,c in enumerate(ctxs))\n",
    "    system=\"Vous êtes un assistant expert. Utilisez uniquement les informations suivantes pour répondre en français. Citez les sources [n].\"\n",
    "    return [\n",
    "        {\"role\":\"system\",\"content\":system},\n",
    "        {\"role\":\"user\",\"content\":f\"CONTEXTE(S):\\n{ctx_block}\\n\\nQUESTION: {question}\\n\\nRéponse:\"}\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84c1dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title 🔬 Prompt test (sans LLM pour l'instant)\n",
    "print(build_prompt(\"Pourquoi le ciel est bleu ?\", [\"La diffusion Rayleigh explique la couleur du ciel.\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec4b714",
   "metadata": {},
   "source": [
    "## 🔗 Séquence 1.7 – Assemblage mini‑RAG (prototype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f90fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile -a app_base.py\n",
    "def ask(question:str, chunks:list[str], vecs:np.ndarray, top_k:int=3):\n",
    "    q_vec = embed_texts([question])[0]\n",
    "    idx = build_faiss_index(vecs)\n",
    "    _, I = idx.search(q_vec[None,:], top_k)\n",
    "    ctx = [chunks[i] for i in I[0]]\n",
    "    prompt = build_prompt(question, ctx)\n",
    "    # ⬇️ Remplacez ce stub par un vrai appel LLM si besoin\n",
    "    return \"[Stub] Réponse générée.\\nContextes utilisés :\" + str(I[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c72002e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title 🧪 Prototype RAG sur le PDF\n",
    "question = \"Quel est le thème principal de ce document ?\"\n",
    "print(ask(question, chunks, chunk_vecs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e742ecfb",
   "metadata": {},
   "source": [
    "### 📦 EXPORT – Générer `app_base.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e655c256",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title 🚚 Vérif contenu app_base.py\n",
    "!echo '--- Lignes et poids de app_base.py --'\n",
    "!wc -l app_base.py\n",
    "!du -h app_base.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8c177e",
   "metadata": {},
   "source": [
    "---\n",
    "# 🛠️ Partie 2 – Mise en production avec `app_finale.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cefaf496",
   "metadata": {},
   "source": [
    "> À partir d’ici **on n’utilise plus `app_base.py`**.  \n",
    "> Tout le travail se fait avec `app_finale.py` fourni dans le dépôt.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027db9b9",
   "metadata": {},
   "source": [
    "## 🏗️ Séquence 2.1 – Préparer l’environnement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a106056c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title ⚙️ Optionnel : créer un virtualenv local\n",
    "# !python -m venv venv && source venv/bin/activate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8131b6e",
   "metadata": {},
   "source": [
    "## 📦 Séquence 2.2 – Récupérer `app_finale.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec6a38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title 📥 Copier le script final\n",
    "%cp FormationEYAI/app_finale.py ./app_finale.py\n",
    "!ls -l app_finale.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a04218",
   "metadata": {},
   "source": [
    "## 🤖 Séquence 2.3 – Démarrer Ollama en production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf55a6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title 🚀 Lancement Ollama en arrière‑plan\n",
    "import subprocess, time, os, signal\n",
    "ollama_proc = subprocess.Popen(\"ollama serve\", shell=True)\n",
    "time.sleep(5)\n",
    "print('✅ Ollama est prêt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3063d997",
   "metadata": {},
   "source": [
    "## 🖥️ Séquence 2.4 – Lancer l’application Streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98cfe6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title 🎛️ Run Streamlit + LocalTunnel\n",
    "!pip install -q streamlit localtunnel\n",
    "!streamlit run app_finale.py &>/content/logs.txt & npx localtunnel --port 8501"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b0c256",
   "metadata": {},
   "source": [
    "## 📈 Séquence 2.5 – Observabilité"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c38cea7",
   "metadata": {},
   "source": [
    "- Temps de réponse, logs Streamlit (`tail -f content/logs.txt`)  \n",
    "- Sécurité des données\n",
    "- Coût GPU / CPU\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a5032b",
   "metadata": {},
   "source": [
    "## 🎓 Mini‑projet final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fcd64b",
   "metadata": {},
   "source": [
    "Ajoutez un **second modèle d’embedding** ou la prise en charge d’un format `.docx`, puis présentez vos résultats.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
