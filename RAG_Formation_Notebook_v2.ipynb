{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00b11ac5",
   "metadata": {},
   "source": [
    "# ðŸ¤– Formation RAG â€“ Notebook intÃ©gral"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e049e283",
   "metadata": {},
   "source": [
    "---\n",
    "## ðŸŽ¯ Objectifs pÃ©dagogiques\n",
    "- Comprendre chaque composant dâ€™un **RAG**\n",
    "- Manipuler le code dans Colab et exporter `app_base.py`\n",
    "- Installer **Ollama** et les modÃ¨les requis\n",
    "- Tester un prototype sur un PDF dâ€™exemple\n",
    "- DÃ©couvrir les Ã©tapes de mise en production (Partieâ€¯2)\n",
    "\n",
    "> Les cellules `EXPORT` Ã©crivent dans *app_base.py* via `%%writefile -a`. Vous pourrez ainsi le versionner, mais **on nâ€™utilisera plus ce fichier en Partieâ€¯2** (remplacÃ© par `app_finale.py`).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928f3a19",
   "metadata": {},
   "source": [
    "## ðŸš§ SÃ©quenceâ€¯1.0Â â€“ Installation dâ€™Ollama & des modÃ¨les"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02783f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title ðŸ› ï¸ Installation Ollama (linux/Colab)\n",
    "!curl -fsSL https://ollama.com/install.sh | sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef6ba77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title ðŸ“¥ TÃ©lÃ©chargement des modÃ¨les\n",
    "!ollama pull llama3.2\n",
    "!ollama pull nomic-embed-text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08aca83",
   "metadata": {},
   "source": [
    "## ðŸ”— SÃ©quenceâ€¯1.1Â â€“ Bootstrap Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac33f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title ðŸ”Œ Connexion GoogleÂ Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2bb2a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title ðŸ“¥ Clone du dÃ©pÃ´t\n",
    "%cd /content/gdrive/MyDrive\n",
    "!git clone https://github.com/antoinecstl/FormationEYAI.git\n",
    "%cd FormationEYAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7a32c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title ðŸ› ï¸ Installation des dÃ©pendances\n",
    "!pip install -q -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d5143d",
   "metadata": {},
   "source": [
    "## ðŸ” SÃ©quenceâ€¯1.2Â â€“ Bases du RAGÂ : embeddings & similaritÃ©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360710df",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile -a app_base.py\n",
    "import numpy as np, ollama\n",
    "\n",
    "EMBED_MODEL = \"nomic-embed-text:latest\"\n",
    "\n",
    "def embed_texts(texts):\n",
    "    \"\"\"Retourne un np.ndarray shape (n, d)\"\"\"\n",
    "    return np.array([ollama.embeddings(model=EMBED_MODEL, prompt=t)['embedding'] for t in texts], dtype='float32')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f380f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title ðŸ”¬ Test d'embedding\n",
    "phrase1 = \"Le soleil brille\"\n",
    "phrase2 = \"Aujourd'hui il fait beau\"\n",
    "# @markdown `phrase1` et `phrase2` sont des phrases Ã  comparer.\n",
    "# @markdown La similaritÃ© entre les phrases est calculÃ©e en utilisant le produit scalaire des\n",
    "# @markdown embeddings normalisÃ©s de chaque phrase.\n",
    "# @markdown La similaritÃ© est un nombre entre -1 et 1, oÃ¹ 1 signifie que les phrases sont trÃ¨s similaires,\n",
    "# @markdown 0 signifie qu'elles ne sont pas similaires, et -1 signifie qu'elles sont opposÃ©es.\n",
    "# @markdown Vous pouvez modifier les phrases pour tester d'autres exemples.\n",
    "# @markdown ExÃ©cutez la cellule pour voir le rÃ©sultat.\n",
    "vecs = embed_texts([phrase1, phrase2])\n",
    "sim = float(vecs[0] @ vecs[1] / (np.linalg.norm(vecs[0])*np.linalg.norm(vecs[1])))\n",
    "print(f\"SimilaritÃ© : {sim:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad5dfe6",
   "metadata": {},
   "source": [
    "## ðŸ“ SÃ©quenceâ€¯1.3Â â€“ Chunking & nettoyage dâ€™un PDF dâ€™exemple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdede86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile -a app_base.py\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "def auto_chunk_size(tok:int)->int:\n",
    "    return 1024 if tok<8000 else 768 if tok<20000 else 512\n",
    "\n",
    "def chunk_document(text:str):\n",
    "    size=auto_chunk_size(len(text.split()))\n",
    "    splitter=RecursiveCharacterTextSplitter(\n",
    "        separators=[\"\\n\\n\",\"\\n\",\". \"],\n",
    "        chunk_size=size,\n",
    "        chunk_overlap=size//4,\n",
    "        length_function=len,\n",
    "        )\n",
    "    return [c for c in splitter.split_text(text) if len(c)>100]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a544cc02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title ðŸ“– Chargement du PDF d'exemple\n",
    "from PyPDF2 import PdfReader, PdfReadError\n",
    "\n",
    "sample_path = \"/rapport.pdf\"  # fourni dans le repo\n",
    "pages = PdfReader(sample_path).pages\n",
    "full_text = \"\\n\".join(p.extract_text() or \"\" for p in pages)\n",
    "\n",
    "print(f\"ðŸ“„ Le document contient {len(full_text.split())} tokens environ.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d93bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title ðŸŒ³ Chunking du PDF\n",
    "chunks = chunk_document(full_text)\n",
    "print(f\"ðŸŒ³ {len(chunks)} chunks crÃ©Ã©s. AperÃ§u :\\n{chunks[0][:300]}â€¦\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5e0cab",
   "metadata": {},
   "source": [
    "## ðŸ“Š SÃ©quenceâ€¯1.4Â â€“ Index vectoriel FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce563806",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile -a app_base.py\n",
    "import faiss, numpy as np\n",
    "\n",
    "def build_faiss_index(vectors:np.ndarray)->faiss.IndexFlatIP:\n",
    "    d=vectors.shape[1]\n",
    "    idx=faiss.IndexFlatIP(d)\n",
    "    idx.add(vectors.astype('float32'))\n",
    "    return idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202c0cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title ðŸ§ª Construction index chunks\n",
    "import numpy as np\n",
    "\n",
    "chunk_vecs = embed_texts(chunks)\n",
    "index = build_faiss_index(chunk_vecs)\n",
    "\n",
    "print(index.ntotal, \"vecteurs dans l'index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea4bdc9",
   "metadata": {},
   "source": [
    "## ðŸ§® SÃ©quenceâ€¯1.5Â â€“ Algorithme MMR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed16fd2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile -a app_base.py\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def mmr(query_vec:np.ndarray, cand:np.ndarray, k:int=5, Î»:float=0.3):\n",
    "    selected, rest = [], list(range(len(cand)))\n",
    "    while len(selected)<min(k,len(rest)):\n",
    "        best, best_score = None, -1e9\n",
    "        for idx in rest:\n",
    "            sim_q = float(query_vec @ cand[idx]/(np.linalg.norm(query_vec)*np.linalg.norm(cand[idx])+1e-6))\n",
    "            sim_s = max(cosine_similarity(cand[idx][None,:], cand[selected])[0]) if selected else 0.\n",
    "            score = Î»*sim_q - (1-Î»)*sim_s\n",
    "            if score>best_score:\n",
    "                best, best_score = idx, score\n",
    "        selected.append(best); rest.remove(best)\n",
    "    return selected\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad559f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title ðŸ”¬ Test MMR\n",
    "import numpy as np\n",
    "\n",
    "q_vec = embed_texts([\"Sujet principal du rapport ?\"])[0]\n",
    "sel = mmr(q_vec, chunk_vecs, 3)\n",
    "print(sel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e41f64f",
   "metadata": {},
   "source": [
    "## ðŸ§‘â€ðŸŽ¤ SÃ©quenceâ€¯1.6Â â€“ Prompt engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7085934",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile -a app_base.py\n",
    "def build_prompt(question:str, ctxs:list[str]):\n",
    "    ctx_block=\"\\n\\n\".join(f\"[{i+1}] {c}\" for i,c in enumerate(ctxs))\n",
    "    system=\"Vous Ãªtes un assistant expert. Utilisez uniquement les informations suivantes pour rÃ©pondre en franÃ§ais. Citez les sources [n].\"\n",
    "    return [\n",
    "        {\"role\":\"system\",\"content\":system},\n",
    "        {\"role\":\"user\",\"content\":f\"CONTEXTE(S):\\n{ctx_block}\\n\\nQUESTION: {question}\\n\\nRÃ©ponse:\"}\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84c1dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Â @title ðŸ”¬Â Prompt test (sans LLM pour l'instant)\n",
    "print(build_prompt(\"Pourquoi le ciel est bleuÂ ?\", [\"La diffusion Rayleigh explique la couleur du ciel.\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec4b714",
   "metadata": {},
   "source": [
    "## ðŸ”— SÃ©quenceâ€¯1.7Â â€“ Assemblage miniâ€‘RAG (prototype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f90fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile -a app_base.py\n",
    "def ask(question:str, chunks:list[str], vecs:np.ndarray, top_k:int=3):\n",
    "    q_vec = embed_texts([question])[0]\n",
    "    idx = build_faiss_index(vecs)\n",
    "    _, I = idx.search(q_vec[None,:], top_k)\n",
    "    ctx = [chunks[i] for i in I[0]]\n",
    "    prompt = build_prompt(question, ctx)\n",
    "    # â¬‡ï¸ Remplacez ce stub par un vrai appel LLM si besoin\n",
    "    return \"[Stub] RÃ©ponse gÃ©nÃ©rÃ©e.\\nContextes utilisÃ©s :\" + str(I[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c72002e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title ðŸ§ª Prototype RAG sur le PDF\n",
    "question = \"Quel est le thÃ¨me principal de ce document ?\"\n",
    "print(ask(question, chunks, chunk_vecs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e742ecfb",
   "metadata": {},
   "source": [
    "### ðŸ“¦ EXPORT â€“ GÃ©nÃ©rer `app_base.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e655c256",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title ðŸšš VÃ©rif contenu app_base.py\n",
    "!echo '--- Lignes et poids de app_base.py --'\n",
    "!wc -l app_base.py\n",
    "!du -h app_base.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8c177e",
   "metadata": {},
   "source": [
    "---\n",
    "# ðŸ› ï¸ Partieâ€¯2Â â€“ Mise en production avec `app_finale.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cefaf496",
   "metadata": {},
   "source": [
    "> Ã€ partir dâ€™ici **on nâ€™utilise plus `app_base.py`**.  \n",
    "> Tout le travail se fait avec `app_finale.py` fourni dans le dÃ©pÃ´t.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027db9b9",
   "metadata": {},
   "source": [
    "## ðŸ—ï¸ SÃ©quenceâ€¯2.1Â â€“ PrÃ©parer lâ€™environnement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a106056c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title âš™ï¸ OptionnelÂ : crÃ©er un virtualenv local\n",
    "# !python -m venv venv && source venv/bin/activate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8131b6e",
   "metadata": {},
   "source": [
    "## ðŸ“¦ SÃ©quenceâ€¯2.2Â â€“ RÃ©cupÃ©rer `app_finale.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec6a38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title ðŸ“¥ Copier le script final\n",
    "%cp FormationEYAI/app_finale.py ./app_finale.py\n",
    "!ls -l app_finale.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a04218",
   "metadata": {},
   "source": [
    "## ðŸ¤– SÃ©quenceâ€¯2.3Â â€“ DÃ©marrer Ollama en production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf55a6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title ðŸš€ Lancement Ollama en arriÃ¨reâ€‘plan\n",
    "import subprocess, time, os, signal\n",
    "ollama_proc = subprocess.Popen(\"ollama serve\", shell=True)\n",
    "time.sleep(5)\n",
    "print('âœ… Ollama est prÃªt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3063d997",
   "metadata": {},
   "source": [
    "## ðŸ–¥ï¸ SÃ©quenceâ€¯2.4Â â€“ Lancer lâ€™application Streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98cfe6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title ðŸŽ›ï¸ Run Streamlit + LocalTunnel\n",
    "!pip install -q streamlit localtunnel\n",
    "!streamlit run app_finale.py &>/content/logs.txt & npx localtunnel --port 8501"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b0c256",
   "metadata": {},
   "source": [
    "## ðŸ“ˆ SÃ©quenceâ€¯2.5Â â€“ ObservabilitÃ©"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c38cea7",
   "metadata": {},
   "source": [
    "- Temps de rÃ©ponse, logs Streamlit (`tail -f content/logs.txt`)  \n",
    "- SÃ©curitÃ© des donnÃ©es\n",
    "- CoÃ»t GPU / CPU\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a5032b",
   "metadata": {},
   "source": [
    "## ðŸŽ“ Miniâ€‘projet final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fcd64b",
   "metadata": {},
   "source": [
    "Ajoutez un **second modÃ¨le dâ€™embedding** ou la prise en charge dâ€™un format `.docx`, puis prÃ©sentez vos rÃ©sultats.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
