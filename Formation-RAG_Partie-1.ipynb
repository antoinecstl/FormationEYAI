{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "00b11ac5",
      "metadata": {
        "id": "00b11ac5"
      },
      "source": [
        "# ü§ñ Formation RAG ‚Äì Partie 1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e049e283",
      "metadata": {
        "id": "e049e283"
      },
      "source": [
        "---\n",
        "## üéØ Objectifs p√©dagogiques\n",
        "\n",
        "Cette formation a pour but de vous initier au concept de **RAG (Retrieval-Augmented Generation)**. √Ä la fin de cette formation, vous serez capable de :\n",
        "\n",
        "- Comprendre les composants essentiels d‚Äôun syst√®me RAG\n",
        "- Manipuler du code Python sur Colab\n",
        "- Installer et utiliser **Ollama** pour faire tourner un mod√®le LLM localement\n",
        "- Tester un prototype sur un document PDF\n",
        "- Explorer les √©tapes vers une mise en production (abord√©es en deuxi√®me partie)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "928f3a19",
      "metadata": {
        "id": "928f3a19"
      },
      "source": [
        "# üöß S√©quence‚ÄØ1.0¬†‚Äì Setup du Projet (Expliqu√© en partie 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02783f32",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "02783f32",
        "outputId": "574e7812-87c2-4d1d-9940-51941cb14f3b"
      },
      "outputs": [],
      "source": [
        "!curl -fsSL https://ollama.com/install.sh | sh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30d2d146",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "30d2d146",
        "outputId": "a2160331-8895-4fe0-aa39-6ad18b0102b5"
      },
      "outputs": [],
      "source": [
        "import subprocess, time\n",
        "ollama_proc = subprocess.Popen(\"ollama serve\", shell=True)\n",
        "time.sleep(2)\n",
        "print('‚úÖ Ollama est pr√™t')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aef6ba77",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aef6ba77",
        "outputId": "a7c74c16-3bbb-4201-9d09-6bc200bafc19"
      },
      "outputs": [],
      "source": [
        "!ollama pull llama3.2:latest\n",
        "!ollama pull nomic-embed-text:latest\n",
        "!ollama pull bge-m3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fac33f5e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fac33f5e",
        "outputId": "84ae48ae-3954-479e-afbb-f49416bbac67"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2bb2a33",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e2bb2a33",
        "outputId": "62d932a5-503f-4510-89c7-a2688ba82f2e"
      },
      "outputs": [],
      "source": [
        "%cd /content/gdrive/MyDrive\n",
        "!git clone https://github.com/antoinecstl/FormationEYAI.git\n",
        "%cd FormationEYAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec7a32c4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ec7a32c4",
        "outputId": "7914ed86-4fc3-4bfe-9489-fef11d9b135c"
      },
      "outputs": [],
      "source": [
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "64d5143d",
      "metadata": {
        "id": "64d5143d"
      },
      "source": [
        "# üîç S√©quence‚ÄØ1.1¬†‚Äì Bases du RAG¬†: embeddings & similarit√© (SLIDE)\n",
        "\n",
        "## üß† Cr√©ation d'embeddings avec un mod√®le local\n",
        "\n",
        "Pour pouvoir comparer des phrases ou retrouver des documents pertinents, on doit **transformer du texte en vecteurs num√©riques** (embeddings). Ces vecteurs capturent le sens des mots ou des phrases dans un espace math√©matique.\n",
        "\n",
        "### üîß Que fait cette cellule ?\n",
        "- Elle d√©finit une fonction `embed_texts` qui prend une **liste de phrases** en entr√©e.\n",
        "- Chaque phrase est transform√©e en vecteur via le mod√®le `nomic-embed-text` install√© localement avec Ollama. (Nous pr√©parons ici deux mod√®les d'embeeding `nomic-embed-text` et `bge-m3`que nous utiliserons par la suite)\n",
        "- Elle retourne un tableau `numpy` contenant les vecteurs (`shape = (n, d)`), o√π :\n",
        "  - `n` est le nombre de phrases\n",
        "  - `d` est la dimension de l‚Äôespace d‚Äôembedding\n",
        "\n",
        "Ces vecteurs seront utiles pour calculer des similarit√©s ou faire de la recherche s√©mantique.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "360710df",
      "metadata": {
        "id": "360710df"
      },
      "outputs": [],
      "source": [
        "import numpy as np, ollama\n",
        "\n",
        "EMBED_MODEL1 = \"nomic-embed-text:latest\"\n",
        "EMBED_MODEL2 = \"bge-m3\"\n",
        "\n",
        "def embed_texts(texts, embed_model):\n",
        "    \"\"\"Retourne un np.ndarray shape (n, d)\"\"\"\n",
        "    return np.array([ollama.embeddings(model=embed_model, prompt=t)['embedding'] for t in texts], dtype='float32')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5Zl1kvvpNgYT",
      "metadata": {
        "id": "5Zl1kvvpNgYT"
      },
      "source": [
        "## üî¨ Test d'embedding : comparaison de phrases\n",
        "\n",
        "Ici, on mesure la **similarit√©** entre deux phrases √† l‚Äôaide de leurs embeddings.\n",
        "\n",
        "#### üîç Que fait cette cellule ?\n",
        "- Elle convertit chaque phrase en vecteur (embedding)\n",
        "- Elle mesure leur proximit√© √† l‚Äôaide d‚Äôun **produit scalaire**\n",
        "- Le score obtenu indique le **niveau de similarit√© s√©mantique**\n",
        "\n",
        "#### üìä Comment lire le score ?\n",
        "- `1` : phrases tr√®s proches (sens similaire)\n",
        "- `0` : phrases sans lien\n",
        "- `-1` : phrases oppos√©es (Les mod√®les que nous utilisons sont tr√®s g√©n√©rique et n'arrivent que tr√®s rarement √† aller en dessous de 0. En effet, les embeddings de mod√®les r√©cents sont faits pour maximiser la similarit√© entre phrases proches, pas pour maximiser la dissimilarit√©.)\n",
        "\n",
        "‚úèÔ∏è Vous pouvez modifier les phrases pour tester diff√©rents cas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "15f380f8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "15f380f8",
        "outputId": "e2bb8c32-6617-444b-d9a1-fa587a06bd08"
      },
      "outputs": [],
      "source": [
        "phrase1 = \"Cite moi les meilleures √©curie de Formule 1\"\n",
        "phrase2 = \"Pourquoi le ciel est bleu ?\"\n",
        "\n",
        "vecs = embed_texts([phrase1, phrase2], EMBED_MODEL1)\n",
        "sim = float(vecs[0] @ vecs[1] / (np.linalg.norm(vecs[0])*np.linalg.norm(vecs[1])))\n",
        "print(f\"Similarit√© : {sim:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cad5dfe6",
      "metadata": {
        "id": "cad5dfe6"
      },
      "source": [
        "# üìê S√©quence‚ÄØ1.2¬†‚Äì Chunking & nettoyage d‚Äôun PDF d‚Äôexemple (SLIDE)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2xvCloYbN6BK",
      "metadata": {
        "id": "2xvCloYbN6BK"
      },
      "source": [
        "#### üîç Que fait cette cellule ?\n",
        "\n",
        "Ici, ce bloc de code sert √† d√©couper un texte en morceaux adapt√©s au LLM. \n",
        "\n",
        "La fonction `auto_chunk_size` d√©termine la taille des morceaux (chunks) en fonction de la longueur du texte, afin de rester efficace. Plus le texte est long et plus les chunks sont petit.\n",
        "\n",
        "La fonction `chunk_document` d√©coupe un texte en morceaux exploitables par un LLM.\n",
        "    1. Calcule la taille des chunks avec `auto_chunk_size`.\n",
        "    2. Cr√©er un d√©coupeur intelligent `splitter` qui coupe aux paragraphes, puis lignes, puis phrases.\n",
        "    3. Filtre les morceaux trop courts (< 100 caract√®res>)\n",
        "\n",
        "Cela permet de faciliter l'analyse de texte long par le LLM en pr√©servant le sens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bdede86d",
      "metadata": {
        "id": "bdede86d"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "def auto_chunk_size(tok:int)->int:\n",
        "    return 1024 if tok<8000 else 768 if tok<20000 else 512\n",
        "\n",
        "def chunk_document(text:str):\n",
        "    size=auto_chunk_size(len(text.split()))\n",
        "    splitter=RecursiveCharacterTextSplitter(\n",
        "        separators=[\"\\n\\n\",\"\\n\",\". \"],\n",
        "        chunk_size=size,\n",
        "        chunk_overlap=size//4,\n",
        "        length_function=len,\n",
        "        )\n",
        "    return [c for c in splitter.split_text(text) if len(c)>100]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Q-9o6RJFOKwf",
      "metadata": {
        "id": "Q-9o6RJFOKwf"
      },
      "source": [
        "## üìñ Chargement du PDF d'exemple\n",
        "\n",
        "On commence par **extraire le texte brut du PDF** page par page gr√¢ce √† la librairie PyPDF2.\n",
        "\n",
        "### üí° Pourquoi faire √ßa ?  \n",
        "- Cela permet de r√©cup√©rer tout le contenu textuel du document.  \n",
        "- On peut ensuite estimer sa taille en nombre de mots (tokens), ce qui aide √† adapter les traitements (chunking, embeddings, etc.)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a544cc02",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a544cc02",
        "outputId": "8b2eaf2a-6dec-45d1-c999-4eb8522ff8d0"
      },
      "outputs": [],
      "source": [
        "from PyPDF2 import PdfReader\n",
        "\n",
        "sample_path = \"/content/gdrive/MyDrive/FormationEYAI/Anonymized_Rapport.pdf\"\n",
        "pages = PdfReader(sample_path).pages\n",
        "full_text = \"\\n\".join(p.extract_text() or \"\" for p in pages)\n",
        "\n",
        "print(f\"üìÑ Le document contient {len(full_text.split())} tokens environ.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "V2h-KwA6wfXL",
      "metadata": {
        "id": "V2h-KwA6wfXL"
      },
      "source": [
        "### üå≥ Chunking du PDF\n",
        "Ici, le text extrait du pdf est d√©coup√© en plus petit segments (chunks), afin de pr√©parer le texte pour de l'indexation.\n",
        "\n",
        "\n",
        "### Pourquoi cr√©er des chunks ?  \n",
        "- Les mod√®les ne peuvent pas traiter de tr√®s longs textes d‚Äôun coup.  \n",
        "- Le chunking permet de diviser le contenu en morceaux coh√©rents et exploitables.  \n",
        "- On peut ensuite traiter chaque chunk ind√©pendamment (calcul d‚Äôembeddings, recherche, etc.).\n",
        "\n",
        "\n",
        "‚úèÔ∏è Parcourer la lise de chunks g√©n√©r√©s afin de valider le bon d√©coupage du contenu."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "62d93bc0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "62d93bc0",
        "outputId": "552672ad-1ee7-4ceb-9e58-77799dd6b423"
      },
      "outputs": [],
      "source": [
        "chunks = chunk_document(full_text)\n",
        "print(f\"{len(chunks)} chunks cr√©√©s.\\nAper√ßu :\\n{chunks[0][:300]}‚Ä¶\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd5e0cab",
      "metadata": {
        "id": "bd5e0cab"
      },
      "source": [
        "# üìä S√©quence‚ÄØ1.3¬†‚Äì Index vectoriel FAISS (SLIDE)\n",
        "\n",
        "\n",
        "Les vecteurs obtenus √† partir des chunks sont rang√©s dans une structure appel√©e **index FAISS**.\n",
        "\n",
        "### Qu‚Äôest-ce que FAISS ?  \n",
        "- Un outil tr√®s rapide pour rechercher les vecteurs proches dans un grand ensemble.  \n",
        "- Permet de retrouver rapidement les documents les plus similaires √† une requ√™te.\n",
        "\n",
        "### üí° Pourquoi cr√©er cet index ?  \n",
        "- Pour acc√©l√©rer les recherches dans la base de documents vectoris√©s.  \n",
        "- C‚Äôest indispensable d√®s qu‚Äôon a beaucoup de donn√©es √† parcourir."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce563806",
      "metadata": {
        "id": "ce563806"
      },
      "outputs": [],
      "source": [
        "import faiss, numpy as np\n",
        "\n",
        "def build_faiss_index(vectors:np.ndarray)->faiss.IndexFlatIP:\n",
        "    d=vectors.shape[1]\n",
        "    idx=faiss.IndexFlatIP(d)\n",
        "    idx.add(vectors.astype('float32'))\n",
        "    return idx"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "CoOUtCzD2l8m",
      "metadata": {
        "id": "CoOUtCzD2l8m"
      },
      "source": [
        "## üß™ Construction index chunks\n",
        "\n",
        "\n",
        "Chaque chunk est transform√© en vecteur num√©rique (embedding), puis ajout√© √† l‚Äôindex FAISS.\n",
        "\n",
        "### Ce que √ßa signifie :  \n",
        "- On passe de textes √† vecteurs.  \n",
        "- On construit une base efficace pour retrouver les chunks les plus pertinents rapidement."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "202c0cbf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "202c0cbf",
        "outputId": "18f6fae4-78f0-46b5-b089-8cb399a33ad8"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "chunk_vecs = embed_texts(chunks, EMBED_MODEL2)\n",
        "index = build_faiss_index(chunk_vecs)\n",
        "\n",
        "print(index.ntotal, \"Nombre de vecteurs total dans l'index\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2ea4bdc9",
      "metadata": {
        "id": "2ea4bdc9"
      },
      "source": [
        "# üßÆ S√©quence‚ÄØ1.4¬†‚Äì Algorithme MMR\n",
        "\n",
        "MMR permet de s√©lectionner des passages √† la fois **pertinents** et **diversifi√©s** pour une requ√™te donn√©e.\n",
        "\n",
        "### Pourquoi c‚Äôest important ?  \n",
        "- S√©lectionner uniquement les passages les plus similaires peut donner des r√©sultats redondants.  \n",
        "- MMR √©quilibre la similarit√© √† la question et la diversit√© entre passages s√©lectionn√©s.\n",
        "\n",
        "Cet algorithme am√©liore la qualit√© des r√©sultats en √©vitant les r√©p√©titions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed16fd2f",
      "metadata": {
        "id": "ed16fd2f"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def mmr(query_vec:np.ndarray, cand:np.ndarray, k:int=5, Œª:float=0.3):\n",
        "    selected, rest = [], list(range(len(cand)))\n",
        "    while len(selected)<min(k,len(rest)):\n",
        "        best, best_score = None, -1e9\n",
        "        for idx in rest:\n",
        "            sim_q = float(query_vec @ cand[idx]/(np.linalg.norm(query_vec)*np.linalg.norm(cand[idx])+1e-6))\n",
        "            sim_s = max(cosine_similarity(cand[idx][None,:], cand[selected])[0]) if selected else 0.\n",
        "            score = Œª*sim_q - (1-Œª)*sim_s\n",
        "            if score>best_score:\n",
        "                best, best_score = idx, score\n",
        "        selected.append(best); rest.remove(best)\n",
        "    return selected"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "q20EfzKw3rPL",
      "metadata": {
        "id": "q20EfzKw3rPL"
      },
      "source": [
        "## üî¨ Test MMR\n",
        "Ici, l'algorithme MMR est appel√© pour trouver les 3 passages les plus pertinents et vari√©s en r√©ponse √† la question.\n",
        "\n",
        "‚úèÔ∏èModifier la question pour s'assurer que MMR s√©lectionne bien des passages diff√©rents mais li√©s √† la question."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad559f5c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ad559f5c",
        "outputId": "091658c1-50c3-4b35-b98b-cefaf38869de"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "q_vec = embed_texts([\"Sujet principal du rapport ?\"], EMBED_MODEL2)[0]\n",
        "sel = mmr(q_vec, chunk_vecs, 3)\n",
        "print(sel)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0e41f64f",
      "metadata": {
        "id": "0e41f64f"
      },
      "source": [
        "# üßë‚Äçüé§ S√©quence‚ÄØ1.5¬†‚Äì Prompt engineering (SLIDE)\n",
        "\n",
        "## üß† Construction du prompt\n",
        "\n",
        "Apr√®s avoir s√©lectionn√© les passages du document les plus pertinents et vari√©s par rapport √† la question,  \n",
        "on construit un prompt clair et structur√© avec `build_prompt`.\n",
        "\n",
        "`ctxs` repr√©sente les informations du document jug√©es pertinentes √† la `question` pos√©e.\n",
        "\n",
        "### üí° Pourquoi on fait √ßa ?  \n",
        "Cette √©tape est essentielle pour que le mod√®le fournisse une r√©ponse cibl√©e et fiable,  \n",
        "en s‚Äôappuyant uniquement sur les donn√©es extraites du document."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d7085934",
      "metadata": {
        "id": "d7085934"
      },
      "outputs": [],
      "source": [
        "def build_prompt(question:str, ctxs:list[str]):\n",
        "    ctx_block=\"\\n\\n\".join(f\"[{i+1}] {c}\" for i,c in enumerate(ctxs))\n",
        "    system=\"Vous √™tes un assistant expert. Utilisez uniquement les informations suivantes pour r√©pondre en fran√ßais. Citez les sources [n].\"\n",
        "    return [\n",
        "        {\"role\":\"system\",\"content\":system},\n",
        "        {\"role\":\"user\",\"content\":f\"CONTEXTE(S):\\n{ctx_block}\\n\\nQUESTION: {question}\\n\\nR√©ponse:\"}\n",
        "    ]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f84c1dd9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f84c1dd9",
        "outputId": "2d93ca96-7e1b-4b51-877e-54635e8745d8"
      },
      "outputs": [],
      "source": [
        "print(build_prompt(\"Pourquoi le ciel est bleu¬†?\", [\"La diffusion Rayleigh explique la couleur du ciel.\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fda2bf49",
      "metadata": {
        "id": "fda2bf49"
      },
      "source": [
        "## ü§ñ S√©quence‚ÄØ1.6 Premier Appel au LLM (SLIDE)\n",
        "\n",
        "Ce code envoie une liste de messages au mod√®le `llama3.2:3B` via la fonction `_call_llm`.\n",
        "\n",
        "- Le premier message d√©finit le r√¥le ou le comportement attendu du mod√®le.  \n",
        "- Le second contient la question pos√©e.\n",
        "\n",
        "On peut ajuster la cr√©ativit√© (`temperature`) et la longueur de la r√©ponse (`max_tokens`).\n",
        "\n",
        "‚úèÔ∏è Modifie ces param√®tres et la question pour tester et comprendre l‚Äôimpact sur les r√©ponses.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d9d80ac7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d9d80ac7",
        "outputId": "bfcba364-d53a-41ba-b2f4-d4197a8b3c89"
      },
      "outputs": [],
      "source": [
        "from typing import List, Dict\n",
        "\n",
        "MODEL_NAME = \"llama3.2:3B\"\n",
        "\n",
        "def _call_llm(messages: List[Dict[str, str]], *, temperature: float = 0.1, max_tokens: int = 2048, stream: bool = False):\n",
        "    \"\"\"Enveloppe simple autour de ollama.chat pour usage direct.\"\"\"\n",
        "    return ollama.chat(\n",
        "        model=MODEL_NAME,\n",
        "        messages=messages,\n",
        "        stream=stream,\n",
        "        options={\"temperature\": temperature, \"num_predict\": max_tokens},\n",
        "    )\n",
        "\n",
        "# üß™ Exemple d'appel\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"Tu es un assistant concis\"},\n",
        "    {\"role\": \"user\", \"content\": \"Donne-moi la capitale de l‚ÄôItalie\"}\n",
        "]\n",
        "print(_call_llm(messages)[\"message\"][\"content\"].strip())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ec4b714",
      "metadata": {
        "id": "4ec4b714"
      },
      "source": [
        "# üèóÔ∏è S√©quence‚ÄØ1.7¬†‚Äì Assemblage mini‚ÄëRAG (prototype)\n",
        "\n",
        "Ici, on combine toutes les √©tapes vues pr√©c√©demment pour cr√©er un syst√®me simple de RAG qui r√©pond √† une question √† partir d‚Äôun document.\n",
        "\n",
        "1. Transformer la question en vecteur (embedding) avec `embed_texts`.  \n",
        "2. Chercher les passages les plus proches dans l‚Äôindex FAISS (`index.search`).  \n",
        "3. R√©cup√©rer les textes correspondants √† ces passages.  \n",
        "4. Construire un prompt structur√© avec `build_prompt`.  \n",
        "5. Appeler le mod√®le de langage avec `_call_llm` pour g√©n√©rer la r√©ponse.  \n",
        "6. Retourner la r√©ponse et les passages utilis√©s.\n",
        "\n",
        "Cete fonction montre comment utiliser les embeddings et l‚Äôindexation pour alimenter un LLM en contexte pr√©cis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e7f90fe1",
      "metadata": {
        "id": "e7f90fe1"
      },
      "outputs": [],
      "source": [
        "def ask(question: str, chunks: List[str], vecs: np.ndarray, top_k: int = 3):\n",
        "    # Recherche des chunks pertinents\n",
        "    q_vec = embed_texts([question], EMBED_MODEL2)[0]\n",
        "    _, I = index.search(q_vec[None, :], top_k)\n",
        "    ctx = [chunks[i] for i in I[0]]\n",
        "    # Pr√©paration du prompt\n",
        "    prompt = build_prompt(question, ctx)\n",
        "    # Appel LLM et retour de la r√©ponse\n",
        "    answer = _call_llm(prompt)[\"message\"][\"content\"].strip()\n",
        "    return answer, I[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ntxeby3U7VQq",
      "metadata": {
        "id": "ntxeby3U7VQq"
      },
      "source": [
        "## üß™ Prototype RAG sur le PDF\n",
        "\n",
        "\n",
        "Maintenant, on peut poser une question sur le PDF `rapport.pdf` et obtenir une r√©ponse sourc√©e bas√©e sur le contenu r√©el du document.\n",
        "\n",
        "‚úèÔ∏è Change la question pour explorer diff√©rentes r√©ponses !\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c72002e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2c72002e",
        "outputId": "bcb9413e-2c6f-4513-c5e8-7dedaacdca9d"
      },
      "outputs": [],
      "source": [
        "question = \"Qui est le prestataire de la mission ?\"\n",
        "print(ask(question, chunks, chunk_vecs))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
