{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e8d71fa",
   "metadata": {},
   "source": [
    "# ü§ñ Formation : Construction d'une Application RAG PDF Chat\n",
    "\n",
    "## Objectifs d'apprentissage\n",
    "- Comprendre les concepts de RAG (Retrieval-Augmented Generation)\n",
    "- Impl√©menter un syst√®me de recherche vectorielle avec FAISS\n",
    "- Int√©grer un mod√®le de langage local avec Ollama\n",
    "- Cr√©er une interface utilisateur avec Streamlit\n",
    "- G√©rer le traitement de documents PDF et la segmentation de texte\n",
    "\n",
    "## Architecture du syst√®me\n",
    "```\n",
    "PDF ‚Üí Chunking ‚Üí Embeddings ‚Üí Vector Store ‚Üí Retrieval ‚Üí LLM ‚Üí Response\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b193b6",
   "metadata": {},
   "source": [
    "## 1. Installation et Imports\n",
    "\n",
    "Commen√ßons par installer les d√©pendances n√©cessaires et importer les modules requis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c07338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation des d√©pendances (√† ex√©cuter une seule fois)\n",
    "# !pip install streamlit faiss-cpu numpy PyPDF2 langchain scikit-learn ollama-python\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import tempfile\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "\n",
    "import faiss  # type: ignore\n",
    "import numpy as np\n",
    "import streamlit as st\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import ollama  # pip install ollama-python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc0aad7",
   "metadata": {},
   "source": [
    "## 2. Configuration de l'Interface Utilisateur\n",
    "\n",
    "### üéØ **Exercice 1** : Interface Streamlit\n",
    "Compl√©tez la configuration de l'interface utilisateur en ajoutant :\n",
    "1. Le CSS pour styliser les messages de chat\n",
    "2. Les styles pour les conteneurs de chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c80aa4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration de la page Streamlit\n",
    "st.set_page_config(page_title=\"ü§ñ RAG PDF Chat\", page_icon=\"ü§ñ\", layout=\"wide\")\n",
    "\n",
    "# TODO: Ajoutez le CSS pour styliser l'interface\n",
    "# Indice: Utilisez st.markdown() avec unsafe_allow_html=True\n",
    "# Cr√©ez des styles pour .user-msg et .bot-msg avec des couleurs diff√©rentes\n",
    "\n",
    "st.markdown(\n",
    "    \"\"\"\n",
    "<style>\n",
    "html, body {\n",
    "    font-family: 'Helvetica Neue', sans-serif;\n",
    "    background-color: #f4f7fa;\n",
    "}\n",
    ".sidebar .sidebar-content {\n",
    "    background-color: #ffffff;\n",
    "}\n",
    ".chat-container {\n",
    "    max-width: 1200px;\n",
    "    margin: auto;\n",
    "    padding: 1rem;\n",
    "}\n",
    "/* TODO: Compl√©tez les styles CSS pour les messages */\n",
    "/* .user-msg { ... } */\n",
    "/* .bot-msg { ... } */\n",
    "</style>\n",
    "\"\"\",\n",
    "    unsafe_allow_html=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d548b486",
   "metadata": {},
   "source": [
    "## 3. Configuration des Mod√®les et Param√®tres\n",
    "\n",
    "### Concepts cl√©s :\n",
    "- **Mod√®le de langage** : llama3.2:3b pour la g√©n√©ration de r√©ponses\n",
    "- **Mod√®le d'embeddings** : nomic-embed-text pour cr√©er les repr√©sentations vectorielles\n",
    "- **Param√®tres de recherche** : TOP_K pour limiter les r√©sultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23fe8cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration des mod√®les et param√®tres\n",
    "MODEL_NAME = \"llama3.2:3b\"\n",
    "EMBEDDING_MODEL = \"nomic-embed-text:latest\"\n",
    "DOC_TOP_K = 3\n",
    "CHUNK_TOP_K = 5\n",
    "CANDIDATES_K = 20\n",
    "NEIGHBORS = 1\n",
    "LAMBDA_DIVERSITY = 0.3\n",
    "SIM_THRESHOLD = 0.25\n",
    "TEMPERATURE = 0.2\n",
    "MAX_TOKENS = 2048"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b124eb",
   "metadata": {},
   "source": [
    "## 4. Fonctions d'Interaction avec les Mod√®les\n",
    "\n",
    "### üéØ **Exercice 2** : Impl√©mentation des appels LLM\n",
    "Compl√©tez les fonctions pour interagir avec Ollama :\n",
    "1. `_call_llm()` : Appel au mod√®le de langage\n",
    "2. `embed_texts()` : G√©n√©ration d'embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb62204d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _call_llm(messages: List[Dict[str, str]], *, temperature: float = 0.1, max_tokens: int = 2048, stream: bool = False):\n",
    "    \"\"\"\n",
    "    Fonction pour appeler le mod√®le de langage via Ollama\n",
    "    \n",
    "    Args:\n",
    "        messages: Liste des messages au format [{\"role\": \"user/assistant\", \"content\": \"...\"}]\n",
    "        temperature: Param√®tre de cr√©ativit√© (0.0 = d√©terministe, 1.0 = cr√©atif)\n",
    "        max_tokens: Nombre maximum de tokens √† g√©n√©rer\n",
    "        stream: Si True, retourne un stream pour l'affichage en temps r√©el\n",
    "    \"\"\"\n",
    "    # TODO: Impl√©mentez l'appel √† ollama.chat()\n",
    "    # Indice: Utilisez ollama.chat() avec les param√®tres model, messages, stream, options\n",
    "    pass\n",
    "\n",
    "def embed_texts(texts: List[str]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    G√©n√®re les embeddings pour une liste de textes\n",
    "    \n",
    "    Args:\n",
    "        texts: Liste des textes √† vectoriser\n",
    "        \n",
    "    Returns:\n",
    "        Array numpy contenant les embeddings\n",
    "    \"\"\"\n",
    "    # TODO: Impl√©mentez la g√©n√©ration d'embeddings\n",
    "    # Indice: Utilisez ollama.embeddings() dans une liste comprehension\n",
    "    # Retournez un np.array avec dtype=\"float32\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af343fd4",
   "metadata": {},
   "source": [
    "## 5. Traitement des Documents PDF\n",
    "\n",
    "### Concepts techniques :\n",
    "- **Extraction de texte** : PyPDF2 pour lire les PDFs\n",
    "- **Nettoyage** : Expressions r√©guli√®res pour nettoyer le texte\n",
    "- **Normalisation** : Suppression des espaces et caract√®res ind√©sirables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c093e015",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"Nettoie le texte extrait du PDF\"\"\"\n",
    "    import re\n",
    "    text = re.sub(r\"\\n{2,}\", \"\\n\\n\", text)  # Normalise les sauts de ligne\n",
    "    text = re.sub(r\"(\\w+)-\\n(\\w+)\", r\"\\1\\2\", text)  # R√©pare les mots coup√©s\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)  # Normalise les espaces\n",
    "    return text.strip()\n",
    "\n",
    "def extract_pdf_text(path: str) -> str:\n",
    "    \"\"\"Extrait le texte d'un fichier PDF\"\"\"\n",
    "    return clean_text(\"\\n\".join(page.extract_text() or \"\" for page in PdfReader(path).pages))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e372ab",
   "metadata": {},
   "source": [
    "## 6. Segmentation de Documents (Chunking)\n",
    "\n",
    "### üéØ **Exercice 3** : Impl√©mentation du chunking adaptatif\n",
    "Le chunking est crucial pour le RAG. Impl√©mentez :\n",
    "1. `auto_chunk_size()` : Taille adapt√©e selon la longueur du document\n",
    "2. `chunk_document()` : Segmentation du texte avec RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50145425",
   "metadata": {},
   "outputs": [],
   "source": [
    "def auto_chunk_size(tokens: int) -> int:\n",
    "    \"\"\"\n",
    "    D√©termine la taille optimale des chunks selon la longueur du document\n",
    "    \n",
    "    Args:\n",
    "        tokens: Nombre approximatif de tokens dans le document\n",
    "        \n",
    "    Returns:\n",
    "        Taille recommand√©e pour les chunks\n",
    "    \"\"\"\n",
    "    # TODO: Impl√©mentez la logique de taille adaptative\n",
    "    # R√®gle: <8000 tokens ‚Üí 1024, <20000 ‚Üí 768, sinon 512\n",
    "    pass\n",
    "\n",
    "def chunk_document(text: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Segmente un document en chunks avec chevauchement\n",
    "    \n",
    "    Args:\n",
    "        text: Texte complet du document\n",
    "        \n",
    "    Returns:\n",
    "        Liste des chunks de texte\n",
    "    \"\"\"\n",
    "    size = auto_chunk_size(len(text.split()))\n",
    "    \n",
    "    # TODO: Cr√©ez un RecursiveCharacterTextSplitter\n",
    "    # Param√®tres: separators=[\"\\n\\n\", \"\\n\", \". \"], chunk_size=size, \n",
    "    #            chunk_overlap=25% de size, length_function=len\n",
    "    \n",
    "    # TODO: Utilisez split_text() et filtrez les chunks < 100 caract√®res\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa7f46f",
   "metadata": {},
   "source": [
    "## 7. G√©n√©ration de R√©sum√©s\n",
    "\n",
    "### Concept : R√©sum√© hi√©rarchique\n",
    "Pour les questions globales, nous g√©n√©rons un r√©sum√© du document complet qui sera utilis√© comme contexte suppl√©mentaire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d181bc67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_summary(text: str) -> str:\n",
    "    \"\"\"G√©n√®re un r√©sum√© structur√© du document\"\"\"\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\", \n",
    "            \"content\": \"Vous √™tes un expert en synth√®se documentaire. R√©sumez le texte suivant en trois parties : (1) Contexte, (2) Points cl√©s, (3) Conclusions. R√©pondez en fran√ßais.\"\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": text[:120000]}  # Limite pour √©viter de d√©passer le contexte\n",
    "    ]\n",
    "    return _call_llm(messages)[\"message\"][\"content\"].strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f5b888",
   "metadata": {},
   "source": [
    "## 8. Index de Recherche Vectorielle\n",
    "\n",
    "### üéØ **Exercice 4** : Classe RagIndex\n",
    "Impl√©mentez la classe principale pour g√©rer l'indexation et la recherche :\n",
    "\n",
    "### Architecture :\n",
    "- **Index hi√©rarchique** : Documents ‚Üí Chunks\n",
    "- **FAISS** : Recherche vectorielle rapide\n",
    "- **MMR** : Maximum Marginal Relevance pour la diversit√©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c11f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RagIndex:\n",
    "    \"\"\"Classe principale pour l'indexation et la recherche RAG\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.doc_index: Optional[faiss.IndexFlatIP] = None\n",
    "        self.chunk_index: Optional[faiss.Index] = None\n",
    "        self.doc_meta: List[Dict[str, Any]] = []\n",
    "        self.chunk_meta: List[Dict[str, Any]] = []\n",
    "        self.chunk_emb: Optional[np.ndarray] = None\n",
    "\n",
    "    def build(self, uploaded_files: List[st.runtime.uploaded_file_manager.UploadedFile]):\n",
    "        \"\"\"\n",
    "        Construit l'index √† partir des fichiers upload√©s\n",
    "        \n",
    "        Args:\n",
    "            uploaded_files: Liste des fichiers PDF upload√©s via Streamlit\n",
    "        \"\"\"\n",
    "        doc_embs, chunk_embs_list = [], []\n",
    "        \n",
    "        for doc_id, uf in enumerate(uploaded_files):\n",
    "            # Sauvegarde temporaire du fichier\n",
    "            with tempfile.NamedTemporaryFile(delete=False, suffix=\".pdf\") as tmp:\n",
    "                tmp.write(uf.getbuffer())\n",
    "                path = tmp.name\n",
    "            \n",
    "            # TODO: Extraire le texte du PDF\n",
    "            # TODO: G√©n√©rer le r√©sum√© du document\n",
    "            # TODO: Stocker les m√©tadonn√©es du document\n",
    "            # TODO: Cr√©er l'embedding du r√©sum√©\n",
    "            \n",
    "            # TODO: Segmenter le texte en chunks\n",
    "            # TODO: G√©n√©rer les embeddings des chunks\n",
    "            # TODO: Stocker les m√©tadonn√©es des chunks\n",
    "            \n",
    "            os.unlink(path)  # Nettoyage\n",
    "\n",
    "        # TODO: Cr√©er l'index FAISS pour les documents (IndexFlatIP)\n",
    "        # TODO: Cr√©er l'index FAISS pour les chunks (IndexHNSWFlat)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56933c56",
   "metadata": {},
   "source": [
    "## 9. D√©tection de Questions Globales\n",
    "\n",
    "### Concept : Classification automatique des questions\n",
    "Certaines questions portent sur l'ensemble du document (r√©sum√©, sujet principal) plut√¥t que sur des d√©tails sp√©cifiques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161fc84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def _is_global(self, query: str, thr: float = 0.78) -> bool:\n",
    "        \"\"\"\n",
    "        D√©termine si une question porte sur l'ensemble du document\n",
    "        \n",
    "        Args:\n",
    "            query: Question de l'utilisateur\n",
    "            thr: Seuil de similarit√©\n",
    "            \n",
    "        Returns:\n",
    "            True si la question est globale\n",
    "        \"\"\"\n",
    "        examples = [\n",
    "            \"De quoi parle ce document ?\",\n",
    "            \"Quel est le sujet principal ?\",\n",
    "            \"Fais un r√©sum√© du document\",\n",
    "        ]\n",
    "        \n",
    "        emb_q = embed_texts([query])[0]\n",
    "        emb_ex = embed_texts(examples)\n",
    "        \n",
    "        # Calcul de similarit√© cosinus\n",
    "        sims = emb_ex @ emb_q / (np.linalg.norm(emb_ex, axis=1) * np.linalg.norm(emb_q) + 1e-6)\n",
    "        return float(np.max(sims)) >= thr\n",
    "\n",
    "# Ajout √† la classe RagIndex\n",
    "RagIndex._is_global = _is_global"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37107b79",
   "metadata": {},
   "source": [
    "## 10. Maximum Marginal Relevance (MMR)\n",
    "\n",
    "### üéØ **Exercice 5** : Algorithme MMR\n",
    "Impl√©mentez l'algorithme MMR pour √©quilibrer pertinence et diversit√© dans les r√©sultats.\n",
    "\n",
    "### Formule MMR :\n",
    "`Score = Œª √ó Sim(query, doc) - (1-Œª) √ó max(Sim(doc, selected))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba0fe3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def _mmr(self, q: np.ndarray, cand: np.ndarray, k: int) -> List[int]:\n",
    "        \"\"\"\n",
    "        Maximum Marginal Relevance pour diversifier les r√©sultats\n",
    "        \n",
    "        Args:\n",
    "            q: Embedding de la question\n",
    "            cand: Embeddings des candidats\n",
    "            k: Nombre de r√©sultats √† s√©lectionner\n",
    "            \n",
    "        Returns:\n",
    "            Indices des candidats s√©lectionn√©s\n",
    "        \"\"\"\n",
    "        selected, rest = [], list(range(len(cand)))\n",
    "        \n",
    "        while len(selected) < min(k, len(rest)):\n",
    "            best, best_score = None, -1e9\n",
    "            \n",
    "            for idx in rest:\n",
    "                # TODO: Calculer la similarit√© avec la question\n",
    "                # sim_q = cosine_similarity(q, cand[idx])\n",
    "                \n",
    "                # TODO: Calculer la similarit√© maximale avec les documents d√©j√† s√©lectionn√©s\n",
    "                # sim_s = max(cosine_similarity(cand[idx], selected)) if selected else 0\n",
    "                \n",
    "                # TODO: Calculer le score MMR\n",
    "                # score = LAMBDA_DIVERSITY * sim_q - (1-LAMBDA_DIVERSITY) * sim_s\n",
    "                \n",
    "                # TODO: Garder le meilleur candidat\n",
    "                pass\n",
    "            \n",
    "            # TODO: Ajouter le meilleur √† la s√©lection et le retirer des candidats\n",
    "            \n",
    "        return selected\n",
    "\n",
    "# Ajout √† la classe RagIndex\n",
    "RagIndex._mmr = _mmr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad677cf",
   "metadata": {},
   "source": [
    "## 11. Fonction de Recherche Principale\n",
    "\n",
    "### üéØ **Exercice 6** : M√©thode retrieve()\n",
    "Impl√©mentez la logique de recherche compl√®te :\n",
    "1. Recherche des documents pertinents\n",
    "2. Filtrage des chunks par document\n",
    "3. Application de MMR\n",
    "4. Expansion du contexte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da733387",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def retrieve(self, query: str) -> Tuple[List[str], List[int], Optional[str]]:\n",
    "        \"\"\"\n",
    "        Recherche les contextes pertinents pour une question\n",
    "        \n",
    "        Args:\n",
    "            query: Question de l'utilisateur\n",
    "            \n",
    "        Returns:\n",
    "            Tuple (contextes, indices, r√©sum√©_si_global)\n",
    "        \"\"\"\n",
    "        # TODO: G√©n√©rer l'embedding de la question\n",
    "        \n",
    "        # TODO: Rechercher les documents les plus pertinents (doc_index.search)\n",
    "        \n",
    "        # TODO: Filtrer les chunks appartenant aux documents s√©lectionn√©s\n",
    "        \n",
    "        # TODO: Cr√©er un sous-index avec les chunks filtr√©s\n",
    "        \n",
    "        # TODO: Rechercher les chunks candidats\n",
    "        \n",
    "        # TODO: Appliquer le seuil de similarit√©\n",
    "        \n",
    "        # TODO: Appliquer MMR pour diversifier\n",
    "        \n",
    "        # TODO: Expansion du contexte (chunks voisins)\n",
    "        \n",
    "        # TODO: Construire les contextes finaux\n",
    "        \n",
    "        # TODO: Ajouter le r√©sum√© si question globale\n",
    "        \n",
    "        pass\n",
    "\n",
    "# Ajout √† la classe RagIndex\n",
    "RagIndex.retrieve = retrieve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6cac55c",
   "metadata": {},
   "source": [
    "## 12. Construction des Prompts\n",
    "\n",
    "### Concept : Ing√©nierie des prompts\n",
    "Un prompt bien structur√© am√©liore significativement la qualit√© des r√©ponses du mod√®le."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71e89ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(question: str, contexts: List[str], summary: Optional[str], history: List[Dict[str, str]] = None):\n",
    "    \"\"\"\n",
    "    Construit le prompt pour le mod√®le de langage\n",
    "    \n",
    "    Args:\n",
    "        question: Question de l'utilisateur\n",
    "        contexts: Contextes r√©cup√©r√©s\n",
    "        summary: R√©sum√© du document si question globale\n",
    "        history: Historique de conversation\n",
    "        \n",
    "    Returns:\n",
    "        Messages format√©s pour le LLM\n",
    "    \"\"\"\n",
    "    # Construction du bloc de contexte avec num√©rotation\n",
    "    ctx_block = \"\\n\\n\".join(f\"[{i+1}] {c}\" for i, c in enumerate(contexts))\n",
    "    \n",
    "    if summary:\n",
    "        ctx_block = f\"[R√©sum√©] {summary}\\n\\n\" + ctx_block\n",
    "    \n",
    "    system = (\n",
    "        \"Vous √™tes un assistant expert. Utilisez uniquement les informations suivantes pour r√©pondre en fran√ßais. \"\n",
    "        \"Citez vos sources avec les balises [n]. Si l'information n'est pas trouv√©e, informez-en l'utilisateur.\"\n",
    "    )\n",
    "    \n",
    "    messages = [{\"role\": \"system\", \"content\": system}]\n",
    "    \n",
    "    # Ajout de l'historique de conversation\n",
    "    if history and len(history) > 0:\n",
    "        previous_messages = history[:-1] if history[-1][\"role\"] == \"user\" else history\n",
    "        messages.extend(previous_messages)\n",
    "    \n",
    "    # Message utilisateur avec contexte\n",
    "    current_user_content = f\"CONTEXTE(S):\\n{ctx_block}\\n\\nQUESTION: {question}\\n\\nR√©ponse:\"\n",
    "    messages.append({\"role\": \"user\", \"content\": current_user_content})\n",
    "    \n",
    "    return messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab31e790",
   "metadata": {},
   "source": [
    "## 13. Interface Streamlit Compl√®te\n",
    "\n",
    "### üéØ **Exercice 7** : Application finale\n",
    "Assemblons tous les composants pour cr√©er l'application compl√®te."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e213b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialisation de l'√©tat Streamlit\n",
    "if \"messages\" not in st.session_state:\n",
    "    st.session_state.messages = []\n",
    "if \"rag\" not in st.session_state:\n",
    "    st.session_state.rag = None\n",
    "if \"processing\" not in st.session_state:\n",
    "    st.session_state.processing = False\n",
    "\n",
    "# Sidebar pour l'upload de fichiers\n",
    "with st.sidebar:\n",
    "    st.header(\"üìö Documents\")\n",
    "    files = st.file_uploader(\"D√©posez vos PDF\", type=[\"pdf\"], accept_multiple_files=True)\n",
    "    if st.button(\"üîÑ R√©initialiser\"):\n",
    "        st.session_state.clear()\n",
    "        st.rerun()\n",
    "\n",
    "# Construction de l'index\n",
    "if files and st.session_state.rag is None:\n",
    "    with st.spinner(\"üìÑ Indexation en cours‚Ä¶\"):\n",
    "        rag = RagIndex()\n",
    "        rag.build(files)\n",
    "        st.session_state.rag = rag\n",
    "    st.success(f\"{len(files)} document(s) index√©(s) ! Posez vos questions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59fce67",
   "metadata": {},
   "source": [
    "## 14. Affichage du Chat et Gestion des Interactions\n",
    "\n",
    "### üéØ **Exercice 8** : Interface de chat compl√®te\n",
    "Impl√©mentez l'affichage des messages et la gestion des interactions utilisateur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616dda95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Affichage des messages de chat\n",
    "# Indice: Parcourir st.session_state.messages et afficher avec les styles CSS\n",
    "\n",
    "# TODO: Gestion de l'input utilisateur\n",
    "# Indice: Utiliser st.chat_input() avec les bonnes conditions de d√©sactivation\n",
    "\n",
    "# TODO: Traitement de la question\n",
    "# 1. Ajouter le message utilisateur √† l'historique\n",
    "# 2. R√©cup√©rer les contextes avec rag.retrieve()\n",
    "# 3. Construire le prompt\n",
    "# 4. Appeler le LLM en streaming\n",
    "# 5. Afficher la r√©ponse en temps r√©el\n",
    "# 6. Ajouter la r√©ponse √† l'historique\n",
    "\n",
    "# TODO: Affichage des contextes dans un expander"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898eca87",
   "metadata": {},
   "source": [
    "## 15. Test de l'Application\n",
    "\n",
    "### üéØ **Exercice 9** : Tests et Validation\n",
    "Une fois votre code compl√©t√©, testez l'application avec diff√©rents types de questions :\n",
    "\n",
    "1. **Questions sp√©cifiques** : \"Que dit le document sur [sujet pr√©cis] ?\"\n",
    "2. **Questions globales** : \"De quoi parle ce document ?\"\n",
    "3. **Questions de comparaison** : \"Quelles sont les diff√©rences entre X et Y ?\"\n",
    "4. **Questions factuelles** : \"Quels sont les chiffres mentionn√©s ?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4da353f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pour tester l'application, ex√©cutez dans votre terminal :\n",
    "# streamlit run formation_rag_chat.py\n",
    "\n",
    "# Conseils de d√©bogage :\n",
    "print(\"‚úÖ Configuration termin√©e\")\n",
    "print(f\"üìä Mod√®le LLM : {MODEL_NAME}\")\n",
    "print(f\"üîç Mod√®le embeddings : {EMBEDDING_MODEL}\")\n",
    "print(\"üöÄ Pr√™t pour les tests !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c42c3b9",
   "metadata": {},
   "source": [
    "## 16. Points d'Am√©lioration et Extensions\n",
    "\n",
    "### Concepts avanc√©s √† explorer :\n",
    "\n",
    "1. **Chunking s√©mantique** : Utiliser des mod√®les pour segmenter selon le sens\n",
    "2. **Re-ranking** : Am√©liorer l'ordre des r√©sultats avec des mod√®les d√©di√©s  \n",
    "3. **M√©moire conversationnelle** : Maintenir le contexte sur plusieurs √©changes\n",
    "4. **Multimodalit√©** : Supporter images et tableaux dans les PDFs\n",
    "5. **√âvaluation automatique** : M√©triques RAGAS pour mesurer la qualit√©\n",
    "6. **Cache intelligent** : Optimiser les performances avec mise en cache\n",
    "7. **Agents RAG** : Ajouter des capacit√©s de raisonnement et d'action\n",
    "\n",
    "### Ressources pour aller plus loin :\n",
    "- Documentation LangChain : https://python.langchain.com/\n",
    "- Tutoriels FAISS : https://github.com/facebookresearch/faiss\n",
    "- Guide Ollama : https://ollama.ai/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d29bf65",
   "metadata": {},
   "source": [
    "## üéâ F√©licitations !\n",
    "\n",
    "Vous avez construit une application RAG compl√®te capable de :\n",
    "- ‚úÖ Traiter des documents PDF\n",
    "- ‚úÖ Cr√©er des embeddings vectoriels\n",
    "- ‚úÖ Effectuer une recherche s√©mantique\n",
    "- ‚úÖ G√©n√©rer des r√©ponses contextualis√©es\n",
    "- ‚úÖ Maintenir une interface utilisateur intuitive\n",
    "\n",
    "**Prochaines √©tapes** : D√©ployez votre application et testez-la avec vos propres documents !"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
