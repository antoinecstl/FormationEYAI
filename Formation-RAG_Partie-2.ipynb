{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJccchCa97wj"
      },
      "source": [
        "# 🤖 Formation RAG - Partie 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sK8g2XKHw1M7"
      },
      "source": [
        "# 🚧 Séquence 2.0 – Installation d’Ollama & des modèles\n",
        "\n",
        "## 🛠️ Installation d’Ollama\n",
        "\n",
        "Cette cellule télécharge et installe **Ollama**, un outil qui permet de faire tourner des modèles d’intelligence artificielle (comme des LLMs) localement sur la machine, sans avoir besoin d’une connexion à un service cloud.\n",
        "\n",
        "Utiliser des modèles localement présente plusieurs avantages :\n",
        "- **Confidentialité** : les données ne sortent pas de votre machine\n",
        "- **Coût** : pas besoin de serveur distant ou d’API payante\n",
        "- **Performance** : temps de réponse plus rapide dans certains cas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KyeSnz3wR3qP",
        "outputId": "067ea842-8592-47fa-f2d0-eefbf1a94254"
      },
      "outputs": [],
      "source": [
        "!curl -fsSL https://ollama.com/install.sh | sh"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nF7e5ksNVvhM"
      },
      "source": [
        "## 🚀 Lancement d'Ollama en arrière‑plan\n",
        "Cette cellule sert à lancer le serveur Ollama, c’est-à-dire à démarrer l’outil qui fera fonctionner un modèle d’IA localement.\n",
        "\n",
        "###💡 Pourquoi on fait ça ?\n",
        "Pour interagir avec un modèle d’intelligence artificielle installé localement, il faut d’abord démarrer un service en fond qui « écoute » et attend nos demandes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q_nUDGlJSCDp",
        "outputId": "8553c6ee-a714-4783-ee35-0a27b4441919"
      },
      "outputs": [],
      "source": [
        "import subprocess, time\n",
        "ollama_proc = subprocess.Popen(\"ollama serve\", shell=True)\n",
        "time.sleep(2)\n",
        "print('✅ Ollama est prêt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ns--aRdLVwbF"
      },
      "source": [
        "## 📥 Téléchargement des modèles\n",
        "\n",
        "Ici, on télécharge deux modèles LLM nécessaires pour la suite :\n",
        "\n",
        "- **Llama 3.2 (3B)** : un modèle de génération de texte développé en open source par Meta\n",
        "- **Nomic Embed Text** : un modèle spécialisé pour convertir du texte en vecteurs numériques (embeddings), utilisé plus tard dans la partie RAG\n",
        "\n",
        "Ces modèles sont stockés localement pour être utilisés sans connexion externe.\n",
        "\n",
        "### 💡 Pourquoi on fait ça ?\n",
        "Les LLM sont pré-entraînés. Pour pouvoir les utiliser, il faut d’abord les télécharger sur votre machine, un peu comme si vous installiez une application.\n",
        "\n",
        "Sans cela, le système ne saura pas quel modèle utiliser, ni comment répondre aux questions ou traiter les textes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iBmmJZf-SGO5",
        "outputId": "e3537d28-cd3a-42c6-ac16-dfa85c5fd7b9"
      },
      "outputs": [],
      "source": [
        "!ollama pull llama3.2:3B\n",
        "!ollama pull nomic-embed-text:latest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5vrqz0aS5jm"
      },
      "source": [
        "# 🔗 Séquence 2.1 – Bootstrap Colab\n",
        "\n",
        "## 🔌 Connexion à Google Drive\n",
        "**Cette cellule permet de connecter Google Drive à l’environnement de travail.** Cela permet d’utiliser des fichiers (ex : jeux de données, documents, modèles) qui sont stockés dans votre Google Drive directement dans le Notebook.\n",
        "\n",
        "### 💡 Pourquoi on fait ça ?\n",
        "Cela évite de devoir uploader manuellement des fichiers à chaque fois."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6CmQxsyVPrpv",
        "outputId": "38d171eb-260a-40b1-8a71-1f65699aceea"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fieAI20cUY3e"
      },
      "source": [
        "## 👨‍💻 Clonage du repertoire Github\n",
        "Cette cellule permet de copier un dossier contenant des fichiers depuis GitHub vers l’environnement de travail du notebook.\n",
        "\n",
        "### 💡 Pourquoi on fait ça ?\n",
        "GitHub est une plateforme utilisée pour stocker, partager du code ou des fichiers de projet. Ici, on télécharge les ressources nécessaires à la formation pour pouvoir les utiliser facilement dans les cellules suivantes.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zaxikizoQODh",
        "outputId": "2bebe824-6c04-46f4-8be3-27182c20107f"
      },
      "outputs": [],
      "source": [
        "%cd /content/gdrive/MyDrive\n",
        "!git clone https://github.com/antoinecstl/FormationEYAI.git\n",
        "%cd FormationEYAI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XzyDRQQYVPsd"
      },
      "source": [
        "## 🛠️ Installation des dépendances\n",
        "\n",
        "Cette cellule installe toutes les **bibliothèques Python** nécessaires à l'exécution du notebook, à partir du fichier `requirements.txt`.\n",
        "\n",
        "\n",
        "### 💡 Pourquoi on fait ça ?\n",
        "Plutôt que d’installer chaque outil un par un, ce fichier centralise tout, ce qui fait gagner du temps et évite les erreurs d’oubli ou d’incompatibilité."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GTwv_1H2RGRw",
        "outputId": "a6cba70b-bdf9-4b7a-a554-5fd82eb2778e"
      },
      "outputs": [],
      "source": [
        "%pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wUouUZJ1VxJ1"
      },
      "source": [
        "# 🏎️ Séquence 2.2 – Run de notre application"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fz74pMy_8hvq"
      },
      "source": [
        "## 📘 Description pédagogique\n",
        "Cette section permet de lancer votre application dans une interface web grace à Streamlit et de la rendre accessible depuis votre navigateur Internet grâce à un outil appelé LocalTunnel.\n",
        "\n",
        "*   L’outil LocalTunnel, va permettre de partager l’application Streamlit via un lien web (nécessaire ici car nous somme dans un environnement google colab).\n",
        "\n",
        "`import urllib + print(...)`\n",
        "*   Affiche l’adresse IP publique de votre environnement pour référence (souvent inutile côté utilisateur, mais utile pour des logs ou du debug).\n",
        "\n",
        "`!streamlit run app.py &>/content/logs.txt &`\n",
        "*   Lance l’application Streamlit (app.py) en arrière-plan. C’est cette application qui permet d'interagir avec le modèle IA via une interface utilisateur.\n",
        "\n",
        "`npx localtunnel --port 8501`\n",
        "*   Crée un lien temporaire et public vers l'application, utilisable depuis n’importe quel navigateur.\n",
        "\n",
        "# 💡 Pourquoi on fait ça ?\n",
        "On crée ici une interface simple et accessible (dans un navigateur) pour interagir avec le modèle IA, sans écrire de code.\n",
        "Et comme Google Colab ou certains environnements locaux n’ont pas d’adresse web fixe, LocalTunnel sert de pont entre votre application et le reste du monde."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B-0NSfDrSOEY",
        "outputId": "7360a3e6-39de-4bc8-d891-b0ff6bb12029"
      },
      "outputs": [],
      "source": [
        "import urllib\n",
        "print(\"Password/Enpoint IP for localtunnel is:\",urllib.request.urlopen('https://ipv4.icanhazip.com').read().decode('utf8').strip(\"\\n\"))\n",
        "!streamlit run app_finale.py &>/content/logs.txt & npx localtunnel --port 8501"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
