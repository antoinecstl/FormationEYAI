{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJccchCa97wj"
      },
      "source": [
        "# ðŸ¤– Formation RAG - Partie 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sK8g2XKHw1M7"
      },
      "source": [
        "# ðŸš§ SÃ©quenceâ€¯2.0Â â€“ Installation dâ€™Ollama & des modÃ¨les\n",
        "\n",
        "## ðŸ› ï¸ Installation dâ€™Ollama\n",
        "\n",
        "Cette cellule tÃ©lÃ©charge et installe **Ollama**, un outil qui permet de faire tourner des modÃ¨les dâ€™intelligence artificielle (comme des LLMs) localement sur la machine, sans avoir besoin dâ€™une connexion Ã  un service cloud.\n",
        "\n",
        "Utiliser des modÃ¨les localement prÃ©sente plusieurs avantages :\n",
        "- **ConfidentialitÃ©** : les donnÃ©es ne sortent pas de votre machine\n",
        "- **CoÃ»t** : pas besoin de serveur distant ou dâ€™API payante\n",
        "- **Performance** : temps de rÃ©ponse plus rapide dans certains cas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KyeSnz3wR3qP",
        "outputId": "067ea842-8592-47fa-f2d0-eefbf1a94254"
      },
      "outputs": [],
      "source": [
        "!curl -fsSL https://ollama.com/install.sh | sh"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nF7e5ksNVvhM"
      },
      "source": [
        "## ðŸš€ Lancement d'Ollama en arriÃ¨reâ€‘plan\n",
        "Cette cellule sert Ã  lancer le serveur Ollama, câ€™est-Ã -dire Ã  dÃ©marrer lâ€™outil qui fera fonctionner un modÃ¨le dâ€™IA localement.\n",
        "\n",
        "###ðŸ’¡ Pourquoi on fait Ã§a ?\n",
        "Pour interagir avec un modÃ¨le dâ€™intelligence artificielle installÃ© localement, il faut dâ€™abord dÃ©marrer un service en fond qui Â« Ã©coute Â» et attend nos demandes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q_nUDGlJSCDp",
        "outputId": "8553c6ee-a714-4783-ee35-0a27b4441919"
      },
      "outputs": [],
      "source": [
        "import subprocess, time\n",
        "ollama_proc = subprocess.Popen(\"ollama serve\", shell=True)\n",
        "time.sleep(2)\n",
        "print('âœ… Ollama est prÃªt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ns--aRdLVwbF"
      },
      "source": [
        "## ðŸ“¥ TÃ©lÃ©chargement des modÃ¨les\n",
        "\n",
        "Ici, on tÃ©lÃ©charge deux modÃ¨les LLM nÃ©cessaires pour la suite :\n",
        "\n",
        "- **Llama 3.2 (3B)** : un modÃ¨le de gÃ©nÃ©ration de texte dÃ©veloppÃ© en open source par Meta\n",
        "- **Nomic Embed Text** : un modÃ¨le spÃ©cialisÃ© pour convertir du texte en vecteurs numÃ©riques (embeddings), utilisÃ© plus tard dans la partie RAG\n",
        "\n",
        "Ces modÃ¨les sont stockÃ©s localement pour Ãªtre utilisÃ©s sans connexion externe.\n",
        "\n",
        "### ðŸ’¡ Pourquoi on fait Ã§a ?\n",
        "Les LLM sont prÃ©-entraÃ®nÃ©s. Pour pouvoir les utiliser, il faut dâ€™abord les tÃ©lÃ©charger sur votre machine, un peu comme si vous installiez une application.\n",
        "\n",
        "Sans cela, le systÃ¨me ne saura pas quel modÃ¨le utiliser, ni comment rÃ©pondre aux questions ou traiter les textes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iBmmJZf-SGO5",
        "outputId": "e3537d28-cd3a-42c6-ac16-dfa85c5fd7b9"
      },
      "outputs": [],
      "source": [
        "!ollama pull llama3.2:3B\n",
        "!ollama pull nomic-embed-text:latest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5vrqz0aS5jm"
      },
      "source": [
        "# ðŸ”— SÃ©quenceâ€¯2.1Â â€“ Bootstrap Colab\n",
        "\n",
        "## ðŸ”Œ Connexion Ã  Google Drive\n",
        "**Cette cellule permet de connecter Google Drive Ã  lâ€™environnement de travail.** Cela permet dâ€™utiliser des fichiers (ex : jeux de donnÃ©es, documents, modÃ¨les) qui sont stockÃ©s dans votre Google Drive directement dans le Notebook.\n",
        "\n",
        "### ðŸ’¡ Pourquoi on fait Ã§a ?\n",
        "Cela Ã©vite de devoir uploader manuellement des fichiers Ã  chaque fois."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6CmQxsyVPrpv",
        "outputId": "38d171eb-260a-40b1-8a71-1f65699aceea"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fieAI20cUY3e"
      },
      "source": [
        "## ðŸ‘¨â€ðŸ’» Clonage du repertoire Github\n",
        "Cette cellule permet de copier un dossier contenant des fichiers depuis GitHub vers lâ€™environnement de travail du notebook.\n",
        "\n",
        "### ðŸ’¡ Pourquoi on fait Ã§a ?\n",
        "GitHub est une plateforme utilisÃ©e pour stocker, partager du code ou des fichiers de projet. Ici, on tÃ©lÃ©charge les ressources nÃ©cessaires Ã  la formation pour pouvoir les utiliser facilement dans les cellules suivantes.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zaxikizoQODh",
        "outputId": "2bebe824-6c04-46f4-8be3-27182c20107f"
      },
      "outputs": [],
      "source": [
        "%cd /content/gdrive/MyDrive\n",
        "!git clone https://github.com/antoinecstl/FormationEYAI.git\n",
        "%cd FormationEYAI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XzyDRQQYVPsd"
      },
      "source": [
        "## ðŸ› ï¸ Installation des dÃ©pendances\n",
        "\n",
        "Cette cellule installe toutes les **bibliothÃ¨ques Python** nÃ©cessaires Ã  l'exÃ©cution du notebook, Ã  partir du fichier `requirements.txt`.\n",
        "\n",
        "\n",
        "### ðŸ’¡ Pourquoi on fait Ã§a ?\n",
        "PlutÃ´t que dâ€™installer chaque outil un par un, ce fichier centralise tout, ce qui fait gagner du temps et Ã©vite les erreurs dâ€™oubli ou dâ€™incompatibilitÃ©."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GTwv_1H2RGRw",
        "outputId": "a6cba70b-bdf9-4b7a-a554-5fd82eb2778e"
      },
      "outputs": [],
      "source": [
        "%pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wUouUZJ1VxJ1"
      },
      "source": [
        "# ðŸŽï¸ SÃ©quenceâ€¯2.2Â â€“ Run de notre application"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fz74pMy_8hvq"
      },
      "source": [
        "## ðŸ“˜ Description pÃ©dagogique\n",
        "Cette section permet de lancer votre application dans une interface web grace Ã  Streamlit et de la rendre accessible depuis votre navigateur Internet grÃ¢ce Ã  un outil appelÃ© LocalTunnel.\n",
        "\n",
        "*   Lâ€™outil LocalTunnel, va permettre de partager lâ€™application Streamlit via un lien web (nÃ©cessaire ici car nous somme dans un environnement google colab).\n",
        "\n",
        "`import urllib + print(...)`\n",
        "*   Affiche lâ€™adresse IP publique de votre environnement pour rÃ©fÃ©rence (souvent inutile cÃ´tÃ© utilisateur, mais utile pour des logs ou du debug).\n",
        "\n",
        "`!streamlit run app.py &>/content/logs.txt &`\n",
        "*   Lance lâ€™application Streamlit (app.py) en arriÃ¨re-plan. Câ€™est cette application qui permet d'interagir avec le modÃ¨le IA via une interface utilisateur.\n",
        "\n",
        "`npx localtunnel --port 8501`\n",
        "*   CrÃ©e un lien temporaire et public vers l'application, utilisable depuis nâ€™importe quel navigateur.\n",
        "\n",
        "# ðŸ’¡ Pourquoi on fait Ã§a ?\n",
        "On crÃ©e ici une interface simple et accessible (dans un navigateur) pour interagir avec le modÃ¨le IA, sans Ã©crire de code.\n",
        "Et comme Google Colab ou certains environnements locaux nâ€™ont pas dâ€™adresse web fixe, LocalTunnel sert de pont entre votre application et le reste du monde."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B-0NSfDrSOEY",
        "outputId": "7360a3e6-39de-4bc8-d891-b0ff6bb12029"
      },
      "outputs": [],
      "source": [
        "import urllib\n",
        "print(\"Password/Enpoint IP for localtunnel is:\",urllib.request.urlopen('https://ipv4.icanhazip.com').read().decode('utf8').strip(\"\\n\"))\n",
        "!streamlit run app_finale.py &>/content/logs.txt & npx localtunnel --port 8501"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
