{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eaa1feab",
   "metadata": {},
   "source": [
    "# 🚀 Formation IA : Créer un Chatbot RAG\n",
    "## Retrieval-Augmented Generation pour l'analyse de documents PDF\n",
    "\n",
    "---\n",
    "\n",
    "### 🎯 **Objectifs de la formation**\n",
    "\n",
    "À la fin de cette formation, vous saurez :\n",
    "- ✅ Configurer un environnement IA sur Google Colab\n",
    "- ✅ Installer et utiliser Ollama pour faire tourner des modèles localement\n",
    "- ✅ Créer un système RAG (Retrieval-Augmented Generation)\n",
    "- ✅ Développer une interface web avec Streamlit\n",
    "- ✅ Analyser et questionner des documents PDF avec l'IA\n",
    "\n",
    "### 📚 **Qu'est-ce que le RAG ?**\n",
    "\n",
    "Le **RAG (Retrieval-Augmented Generation)** est une technique qui combine :\n",
    "- 🔍 **Recherche** : Trouve les informations pertinentes dans vos documents\n",
    "- 🤖 **Génération** : Utilise un modèle IA pour formuler des réponses basées sur ces informations\n",
    "\n",
    "C'est comme avoir un assistant qui lit vos documents et répond à vos questions de manière intelligente !\n",
    "\n",
    "---\n",
    "\n",
    "### ⏱️ **Durée estimée** : 2-3 heures\n",
    "### 💻 **Plateforme** : Google Colab (gratuit)\n",
    "### 🧠 **Niveau** : Débutant à intermédiaire\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baff6c37",
   "metadata": {},
   "source": [
    "## 🔗 Section 1 : Connexion à Google Drive\n",
    "\n",
    "### 📘 Description pédagogique (non technique)\n",
    "**Cette cellule permet de connecter Google Drive à l'environnement de travail.** Cela permet d'utiliser des fichiers (ex : jeux de données, documents, modèles) qui sont stockés dans votre Google Drive directement dans le Notebook.\n",
    "\n",
    "### 💡 Pourquoi on fait ça ?\n",
    "Cela évite de devoir uploader manuellement des fichiers à chaque fois et permet de sauvegarder facilement notre travail.\n",
    "\n",
    "### 🛠️ Instructions :\n",
    "1. Exécutez la cellule ci-dessous\n",
    "2. Cliquez sur le lien qui apparaît\n",
    "3. Connectez-vous à votre compte Google\n",
    "4. Copiez le code d'autorisation\n",
    "5. Collez-le dans la zone de texte qui apparaît"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecdfd01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount(\"/content/gdrive\")\n",
    "print(\"✅ Google Drive connecté avec succès !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de39dc2",
   "metadata": {},
   "source": [
    "## 📁 Section 2 : Configuration de l'environnement de travail\n",
    "\n",
    "### 📘 Description pédagogique (non technique)\n",
    "**Cette cellule permet de changer le dossier de travail du notebook pour aller dans le dossier principal de votre Google Drive** (Mon Drive).\n",
    "Cela facilite l'accès direct aux fichiers sans avoir à écrire un chemin complet à chaque fois.\n",
    "\n",
    "### 💡 Pourquoi on fait ça ?\n",
    "En changeant de dossier, on peut lire et enregistrer des fichiers (ex : jeux de données, résultats, images…) comme si on était déjà dans Google Drive, ce qui simplifie beaucoup les manipulations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7110ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd gdrive/MyDrive\n",
    "print(\"📂 Répertoire de travail changé vers Google Drive\")\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4096cf12",
   "metadata": {},
   "source": [
    "## 🌐 Section 3 : Téléchargement des fichiers nécessaires depuis GitHub\n",
    "\n",
    "### 📘 Description pédagogique (non technique)\n",
    "Cette cellule permet de copier un dossier contenant des fichiers depuis GitHub vers l'environnement de travail du notebook.\n",
    "\n",
    "### 💡 Pourquoi on fait ça ?\n",
    "GitHub est une plateforme utilisée pour stocker, partager du code ou des fichiers de projet. Ici, on télécharge les ressources nécessaires à la formation pour pouvoir les utiliser facilement dans les cellules suivantes.\n",
    "\n",
    "### 📦 Ce qu'on télécharge :\n",
    "- Le code de l'application Streamlit\n",
    "- Les fichiers de configuration\n",
    "- Les modèles et outils nécessaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a65e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/antoinecstl/FormationEYAI.git\n",
    "print(\"✅ Fichiers de formation téléchargés depuis GitHub\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a6d7ec",
   "metadata": {},
   "source": [
    "### 📂 Déplacement vers le dossier de formation\n",
    "\n",
    "### 📘 Description pédagogique (non technique)\n",
    "Cette cellule permet de se déplacer dans le dossier FormationEYAI que vous venez de télécharger depuis GitHub.\n",
    "\n",
    "### 💡 Pourquoi on fait ça ?\n",
    "En se plaçant dans ce dossier, on pourra accéder plus facilement aux fichiers utiles pour la suite de la formation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438afe08",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd FormationEYAI\n",
    "print(\"📁 Maintenant dans le dossier FormationEYAI\")\n",
    "!ls -la"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9bdff8",
   "metadata": {},
   "source": [
    "## 📦 Section 4 : Installation des dépendances Python\n",
    "\n",
    "### 📘 Description pédagogique (non technique)\n",
    "**Cette cellule permet d'installer automatiquement tous les outils nécessaires à la formation à partir d'un fichier spécial appelé requirements.txt.**\n",
    "Ce fichier contient la liste des modules Python dont on aura besoin pour que le code fonctionne.\n",
    "\n",
    "### 💡 Pourquoi on fait ça ?\n",
    "Plutôt que d'installer chaque outil un par un, ce fichier centralise tout, ce qui fait gagner du temps et évite les erreurs d'oubli ou d'incompatibilité.\n",
    "\n",
    "### 🔧 Modules qui seront installés :\n",
    "- **Streamlit** : Pour créer l'interface web\n",
    "- **Ollama** : Pour interagir avec les modèles IA\n",
    "- **FAISS** : Pour la recherche vectorielle rapide\n",
    "- **LangChain** : Pour le traitement de texte avancé\n",
    "- **PyPDF2** : Pour lire les fichiers PDF\n",
    "- **NumPy & scikit-learn** : Pour les calculs mathématiques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93323186",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt\n",
    "print(\"✅ Toutes les dépendances sont installées !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b66c26f",
   "metadata": {},
   "source": [
    "## 🤖 Section 5 : Installation et lancement d'Ollama\n",
    "\n",
    "### 📘 Description pédagogique (non technique)\n",
    "Cette cellule télécharge et installe Ollama, un outil qui permet de faire tourner des modèles d'intelligence artificielle (comme des LLMs) localement sur la machine, sans avoir besoin d'une connexion à un service cloud.\n",
    "\n",
    "### 💡 Pourquoi on fait ça ?\n",
    "Cela permet de tester des modèles d'IA de manière autonome, sans dépendre de services externes. C'est particulièrement utile pour des raisons de confidentialité, de coût ou de performance locale.\n",
    "\n",
    "### 🔒 Avantages d'Ollama :\n",
    "- **Gratuit** : Pas de coûts d'API\n",
    "- **Privé** : Vos données restent locales\n",
    "- **Rapide** : Pas de latence réseau\n",
    "- **Flexible** : Plusieurs modèles disponibles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89ac49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -fsSL https://ollama.com/install.sh | sh\n",
    "print(\"✅ Ollama installé avec succès !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9aed63",
   "metadata": {},
   "source": [
    "### 🚀 Démarrage du serveur Ollama\n",
    "\n",
    "### 📘 Description pédagogique (non technique)\n",
    "Cette cellule sert à lancer le serveur Ollama, c'est-à-dire à démarrer l'outil qui fera fonctionner un modèle d'IA localement.\n",
    "\n",
    "### 💡 Pourquoi on fait ça ?\n",
    "Pour interagir avec un modèle d'intelligence artificielle installé localement, il faut d'abord démarrer un service qui « écoute » et attend nos demandes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a5434a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import time\n",
    "\n",
    "# Démarrage du serveur Ollama en arrière-plan\n",
    "sub = subprocess.Popen(\"ollama serve\", shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "print(\"🚀 Serveur Ollama démarré !\")\n",
    "print(\"⏳ Attente de 5 secondes pour que le serveur soit prêt...\")\n",
    "time.sleep(5)\n",
    "print(\"✅ Serveur Ollama opérationnel !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221a922a",
   "metadata": {},
   "source": [
    "## 🧠 Section 6 : Téléchargement des modèles d'IA\n",
    "\n",
    "### 📘 Description pédagogique (non technique)\n",
    "Ces deux commandes permettent de télécharger des modèles d'intelligence artificielle (IA) que l'on utilisera ensuite dans le projet.\n",
    "\n",
    "### 🤖 Modèles téléchargés :\n",
    "\n",
    "1. **llama3.2** : un modèle de langage (LLM), capable de répondre à des questions, générer du texte, résumer, etc.\n",
    "\n",
    "2. **nomic-embed-text** : un modèle qui transforme des textes en « vecteurs », une forme que les machines peuvent comprendre pour faire des recherches sémantiques ou des comparaisons de sens.\n",
    "\n",
    "### 💡 Pourquoi on fait ça ?\n",
    "Les LLM sont pré-entraînés. Pour pouvoir les utiliser, il faut d'abord les télécharger sur votre machine, un peu comme si vous installiez une application.\n",
    "\n",
    "Sans cela, le système ne saura pas quel modèle utiliser, ni comment répondre aux questions ou traiter les textes.\n",
    "\n",
    "### ⚠️ Note importante :\n",
    "Le téléchargement peut prendre plusieurs minutes selon votre connexion internet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76affdd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"📥 Téléchargement du modèle de langage llama3.2...\")\n",
    "!ollama pull llama3.2\n",
    "print(\"✅ Modèle llama3.2 téléchargé !\")\n",
    "\n",
    "print(\"\\n📥 Téléchargement du modèle d'embedding nomic-embed-text...\")\n",
    "!ollama pull nomic-embed-text\n",
    "print(\"✅ Modèle nomic-embed-text téléchargé !\")\n",
    "\n",
    "print(\"\\n🎉 Tous les modèles sont prêts à être utilisés !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c6e488",
   "metadata": {},
   "source": [
    "## 🖥️ Section 7 : Création de l'interface utilisateur avec Streamlit\n",
    "\n",
    "### 📘 Description pédagogique (non technique)\n",
    "Nous allons maintenant créer le fichier `app.py` qui contient tout le code de notre application RAG PDF Chat. Cette application permettra :\n",
    "\n",
    "### 🎯 Fonctionnalités de l'application :\n",
    "- 📄 **Upload de PDF** : Déposer des documents PDF\n",
    "- 🔍 **Indexation intelligente** : Analyser et découper les documents\n",
    "- 💬 **Chat interactif** : Poser des questions sur vos documents\n",
    "- 🎨 **Interface moderne** : Design professionnel avec Streamlit\n",
    "- 📊 **Affichage des sources** : Voir d'où viennent les réponses\n",
    "\n",
    "### 🧠 Comment ça marche ?\n",
    "1. **Extraction** : Le texte est extrait des PDF\n",
    "2. **Découpage** : Le texte est divisé en chunks intelligents\n",
    "3. **Vectorisation** : Chaque chunk devient un vecteur mathématique\n",
    "4. **Indexation** : Les vecteurs sont stockés dans une base de données rapide\n",
    "5. **Recherche** : Quand vous posez une question, on trouve les chunks pertinents\n",
    "6. **Génération** : Le modèle IA formule une réponse basée sur ces chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c299f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création du fichier app.py avec le code complet de l'application RAG\n",
    "app_code = '''\n",
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import tempfile\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "\n",
    "import faiss  # type: ignore\n",
    "import numpy as np\n",
    "import streamlit as st\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import ollama  # pip install ollama-python\n",
    "\n",
    "# --------------------- UI & style -----------------------------------------\n",
    "st.set_page_config(page_title=\"🤖 RAG PDF Chat\", page_icon=\"🤖\", layout=\"wide\")\n",
    "\n",
    "st.markdown(\n",
    "    \"\"\"\n",
    "<style>\n",
    "html, body {\n",
    "    font-family: 'Helvetica Neue', sans-serif;\n",
    "    background-color: #f4f7fa;\n",
    "}\n",
    ".sidebar .sidebar-content {\n",
    "    background-color: #ffffff;\n",
    "}\n",
    ".chat-container {\n",
    "    max-width: 1200px;\n",
    "    margin: auto;\n",
    "    padding: 1rem;\n",
    "}\n",
    ".user-msg {\n",
    "    background: #007bff;\n",
    "    color: white;\n",
    "    border-radius: 20px 20px 0px 20px;\n",
    "    padding: 1rem;\n",
    "    margin: 0.5rem 0;\n",
    "    align-self: flex-end;\n",
    "    max-width: 100%;\n",
    "    word-wrap: break-word;\n",
    "}\n",
    ".bot-msg {\n",
    "    background: #e9ecef;\n",
    "    color: #212529;\n",
    "    border-radius: 20px 20px 20px 0px;\n",
    "    padding: 1rem;\n",
    "    margin: 0.5rem 0;\n",
    "    align-self: flex-start;\n",
    "    max-width: 100%;\n",
    "    word-wrap: break-word;\n",
    "}\n",
    ".chat-area {\n",
    "    display: flex;\n",
    "    flex-direction: column;\n",
    "}\n",
    "</style>\n",
    "\"\"\",\n",
    "    unsafe_allow_html=True,\n",
    ")\n",
    "\n",
    "# --------------------- Config ---------------------------------------------\n",
    "MODEL_NAME = \"llama3.2:3b\"\n",
    "EMBEDDING_MODEL = \"nomic-embed-text:latest\"\n",
    "DOC_TOP_K = 3\n",
    "CHUNK_TOP_K = 5\n",
    "CANDIDATES_K = 20\n",
    "NEIGHBORS = 1\n",
    "LAMBDA_DIVERSITY = 0.3\n",
    "SIM_THRESHOLD = 0.25\n",
    "TEMPERATURE = 0.2\n",
    "MAX_TOKENS = 2048\n",
    "\n",
    "# --------------------- Helpers LLM ----------------------------------------\n",
    "\n",
    "def _call_llm(messages: List[Dict[str, str]], *, temperature: float = 0.1, max_tokens: int = 2048, stream: bool = False):\n",
    "    return ollama.chat(\n",
    "        model=MODEL_NAME,\n",
    "        messages=messages,\n",
    "        stream=stream,\n",
    "        options={\"temperature\": temperature, \"num_predict\": max_tokens},\n",
    "    )\n",
    "\n",
    "\n",
    "def embed_texts(texts: List[str]) -> np.ndarray:\n",
    "    return np.array([ollama.embeddings(model=EMBEDDING_MODEL, prompt=t)[\"embedding\"] for t in texts], dtype=\"float32\")\n",
    "\n",
    "# --------------------- PDF utils -----------------------------------------\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    import re\n",
    "    text = re.sub(r\"\\n{2,}\", \"\\n\\n\", text)\n",
    "    text = re.sub(r\"(\\w+)-\\n(\\w+)\", r\"\\1\\2\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def extract_pdf_text(path: str) -> str:\n",
    "    return clean_text(\"\\n\".join(page.extract_text() or \"\" for page in PdfReader(path).pages))\n",
    "\n",
    "# --------------------- Chunking & résumé ----------------------------------\n",
    "\n",
    "def auto_chunk_size(tokens: int) -> int:\n",
    "    return 1024 if tokens < 8000 else 768 if tokens < 20000 else 512\n",
    "\n",
    "\n",
    "def chunk_document(text: str) -> List[str]:\n",
    "    size = auto_chunk_size(len(text.split()))\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        separators=[\"\\n\\n\", \"\\n\", \". \"],\n",
    "        chunk_size=size,\n",
    "        chunk_overlap=int(size*0.25),\n",
    "        length_function=len,\n",
    "    )\n",
    "    return [c for c in splitter.split_text(text) if len(c) > 100]\n",
    "\n",
    "\n",
    "def make_summary(text: str) -> str:\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"Vous êtes un expert en synthèse documentaire. Résumez le texte suivant en trois parties : (1) Contexte, (2) Points clés, (3) Conclusions. Répondez en français.\"},\n",
    "        {\"role\": \"user\", \"content\": text[:120000]}\n",
    "    ]\n",
    "    return _call_llm(messages)[\"message\"][\"content\"].strip()\n",
    "\n",
    "# --------------------- Index hiérarchique ---------------------------------\n",
    "class RagIndex:\n",
    "    def __init__(self):\n",
    "        self.doc_index: Optional[faiss.IndexFlatIP] = None\n",
    "        self.chunk_index: Optional[faiss.Index] = None\n",
    "        self.doc_meta: List[Dict[str, Any]] = []\n",
    "        self.chunk_meta: List[Dict[str, Any]] = []\n",
    "        self.chunk_emb: Optional[np.ndarray] = None\n",
    "\n",
    "    def build(self, uploaded_files: List[st.runtime.uploaded_file_manager.UploadedFile]):\n",
    "        doc_embs, chunk_embs_list = [], []\n",
    "        for doc_id, uf in enumerate(uploaded_files):\n",
    "            with tempfile.NamedTemporaryFile(delete=False, suffix=\".pdf\") as tmp:\n",
    "                tmp.write(uf.getbuffer())\n",
    "                path = tmp.name\n",
    "            full_text = extract_pdf_text(path)\n",
    "            os.unlink(path)\n",
    "\n",
    "            summary = make_summary(full_text)\n",
    "            self.doc_meta.append({\"filename\": uf.name, \"summary\": summary})\n",
    "            doc_embs.append(embed_texts([summary])[0])\n",
    "\n",
    "            chunks = chunk_document(full_text)\n",
    "            chunk_embs = embed_texts(chunks)\n",
    "            chunk_embs_list.append(chunk_embs)\n",
    "            for i, txt in enumerate(chunks):\n",
    "                self.chunk_meta.append({\"doc_id\": doc_id, \"text\": txt, \"chunk_id\": i})\n",
    "\n",
    "        self.doc_index = faiss.IndexFlatIP(len(doc_embs[0]))\n",
    "        self.doc_index.add(np.vstack(doc_embs).astype(\"float32\"))\n",
    "\n",
    "        self.chunk_emb = np.vstack(chunk_embs_list).astype(\"float32\")\n",
    "        self.chunk_index = faiss.IndexHNSWFlat(self.chunk_emb.shape[1], 32)\n",
    "        self.chunk_index.add(self.chunk_emb)\n",
    "\n",
    "    def _is_global(self, query: str, thr: float = 0.78) -> bool:\n",
    "        examples = [\n",
    "            \"De quoi parle ce document ?\",\n",
    "            \"Quel est le sujet principal ?\",\n",
    "            \"Fais un résumé du document\",\n",
    "        ]\n",
    "        emb_q = embed_texts([query])[0]\n",
    "        emb_ex = embed_texts(examples)\n",
    "        sims = emb_ex @ emb_q / (np.linalg.norm(emb_ex, axis=1) * np.linalg.norm(emb_q) + 1e-6)\n",
    "        return float(np.max(sims)) >= thr\n",
    "\n",
    "    def _mmr(self, q: np.ndarray, cand: np.ndarray, k: int) -> List[int]:\n",
    "        selected, rest = [], list(range(len(cand)))\n",
    "        while len(selected) < min(k, len(rest)):\n",
    "            best, best_score = None, -1e9\n",
    "            for idx in rest:\n",
    "                sim_q = float(q @ cand[idx] / (np.linalg.norm(q) * np.linalg.norm(cand[idx]) + 1e-6))\n",
    "                sim_s = max(cosine_similarity(cand[idx][None, :], cand[selected])[0]) if selected else 0.\n",
    "                score = LAMBDA_DIVERSITY*sim_q - (1-LAMBDA_DIVERSITY)*sim_s\n",
    "                if score > best_score:\n",
    "                    best, best_score = idx, score\n",
    "            selected.append(best)\n",
    "            rest.remove(best)\n",
    "        return selected\n",
    "\n",
    "    def retrieve(self, query: str) -> Tuple[List[str], List[int], Optional[str]]:\n",
    "        q_emb = embed_texts([query])[0]\n",
    "        _, I_doc = self.doc_index.search(q_emb[None, :], DOC_TOP_K)\n",
    "        allowed = set(I_doc[0])\n",
    "\n",
    "        mask = [i for i, m in enumerate(self.chunk_meta) if m[\"doc_id\"] in allowed]\n",
    "        sub_emb = self.chunk_emb[mask]\n",
    "        sub_idx = faiss.IndexFlatIP(sub_emb.shape[1])\n",
    "        sub_idx.add(sub_emb)\n",
    "        D, I = sub_idx.search(q_emb[None, :], min(CANDIDATES_K, len(mask)))\n",
    "        pool = [mask[idx] for idx in I[0]]\n",
    "        pool = [idx for idx, d in zip(pool, D[0]) if 1-d <= SIM_THRESHOLD] or [mask[I[0][0]]]\n",
    "\n",
    "        cand_emb = self.chunk_emb[pool]\n",
    "        selected = [pool[i] for i in self._mmr(q_emb, cand_emb, CHUNK_TOP_K)]\n",
    "        expanded = {j for idx in selected for j in range(idx-NEIGHBORS, idx+NEIGHBORS+1)}\n",
    "        final = [i for i in expanded if 0 <= i < len(self.chunk_meta)][:CHUNK_TOP_K]\n",
    "\n",
    "        contexts = [self.chunk_meta[i][\"text\"] for i in final]\n",
    "        summary = self.doc_meta[int(I_doc[0][0])][\"summary\"] if self._is_global(query) else None\n",
    "        return contexts, final, summary\n",
    "\n",
    "# ---------------- Prompt builder -----------------------------------------\n",
    "def build_prompt(question: str, contexts: List[str], summary: Optional[str], history: List[Dict[str, str]] = None):\n",
    "    ctx_block = \"\\n\\n\".join(f\"[{i+1}] {c}\" for i, c in enumerate(contexts))\n",
    "    if summary:\n",
    "        ctx_block = f\"[Résumé] {summary}\\n\\n\" + ctx_block\n",
    "    \n",
    "    system = (\n",
    "        \"Vous êtes un assistant expert. Utilisez uniquement les informations suivantes pour répondre en français. \"\n",
    "        \"Citez vos sources avec les balises [n]. Si l'information n'est pas trouvée, informez-en l'utilisateur.\"\n",
    "    )\n",
    "    \n",
    "    messages = [{\"role\": \"system\", \"content\": system}]\n",
    "    \n",
    "    if history and len(history) > 0:\n",
    "        previous_messages = history[:-1] if history[-1][\"role\"] == \"user\" else history\n",
    "        messages.extend(previous_messages)\n",
    "    \n",
    "    current_user_content = f\"CONTEXTE(S):\\n{ctx_block}\\n\\nQUESTION: {question}\\n\\nRéponse:\"\n",
    "    messages.append({\"role\": \"user\", \"content\": current_user_content})\n",
    "    \n",
    "    return messages\n",
    "\n",
    "# ---------------- Etat Streamlit ----------------------------------------\n",
    "if \"messages\" not in st.session_state:\n",
    "    st.session_state.messages = []\n",
    "if \"rag\" not in st.session_state:\n",
    "    st.session_state.rag = None\n",
    "if \"processing\" not in st.session_state:\n",
    "    st.session_state.processing = False\n",
    "\n",
    "# ---------------- Sidebar upload ----------------------------------------\n",
    "with st.sidebar:\n",
    "    st.header(\"📚 Documents\")\n",
    "    files = st.file_uploader(\"Déposez vos PDF\", type=[\"pdf\"], accept_multiple_files=True)\n",
    "    if st.button(\"🔄 Réinitialiser\"):\n",
    "        st.session_state.clear()\n",
    "        st.rerun()\n",
    "\n",
    "# ---------------- Index construction ------------------------------------\n",
    "if files and st.session_state.rag is None:\n",
    "    with st.spinner(\"📄 Indexation en cours…\"):\n",
    "        rag = RagIndex()\n",
    "        rag.build(files)\n",
    "        st.session_state.rag = rag\n",
    "    st.success(f\"{len(files)} document(s) indexé(s) ! Posez vos questions.\")\n",
    "\n",
    "# ---------------- Chat display -----------------------------------------\n",
    "st.markdown(\"<div class='chat-container'>\", unsafe_allow_html=True)\n",
    "if not st.session_state.messages:\n",
    "    if st.session_state.rag is not None:\n",
    "        st.markdown(\n",
    "            \"\"\"\n",
    "            <div class=\"bot-msg\">\n",
    "            👋 Bonjour ! Je suis votre assistant IA documentaire.\n",
    "            <br>Posez-moi une question sur le contenu de vos documents.\n",
    "            </div>\n",
    "            \"\"\",\n",
    "            unsafe_allow_html=True\n",
    "        )\n",
    "    else:\n",
    "        st.markdown(\n",
    "            \"\"\"\n",
    "            <div class=\"bot-msg\">\n",
    "            👋 Bienvenue dans RAG PDF Chat !\n",
    "            <br>Commencez par télécharger un ou plusieurs documents PDF dans le panneau latéral.\n",
    "            </div>\n",
    "            \"\"\",\n",
    "            unsafe_allow_html=True\n",
    "        )\n",
    "else:\n",
    "    st.markdown(\"<div class='chat-area'>\", unsafe_allow_html=True)\n",
    "    for msg in st.session_state.messages:\n",
    "        css = \"user-msg\" if msg[\"role\"] == \"user\" else \"bot-msg\"\n",
    "        st.markdown(f'<div class=\"{css}\">{msg[\"content\"]}</div>', unsafe_allow_html=True)\n",
    "    st.markdown(\"</div>\", unsafe_allow_html=True)\n",
    "st.markdown(\"</div>\", unsafe_allow_html=True)\n",
    "\n",
    "# ---------------- Chat input -------------------------------------------\n",
    "query = st.chat_input(\"Votre question…\", disabled=st.session_state.processing or st.session_state.rag is None)\n",
    "\n",
    "if query:\n",
    "    st.session_state.messages.append({\"role\": \"user\", \"content\": query})\n",
    "    st.markdown(f'<div class=\"user-msg\">{query}</div>', unsafe_allow_html=True)\n",
    "    st.session_state.processing = True\n",
    "\n",
    "    rag: RagIndex = st.session_state.rag  # type: ignore\n",
    "    contexts, indices, summary = rag.retrieve(query)\n",
    "    \n",
    "    prompt = build_prompt(query, contexts, summary, st.session_state.messages)\n",
    "\n",
    "    placeholder = st.empty()\n",
    "    collected_parts: List[str] = []\n",
    "\n",
    "    for chunk in _call_llm(prompt, temperature=TEMPERATURE, max_tokens=MAX_TOKENS, stream=True):\n",
    "        token = chunk[\"message\"][\"content\"]\n",
    "        collected_parts.append(token)\n",
    "        placeholder.markdown(f'<div class=\"bot-msg\">{\"\".join(collected_parts)}</div>', unsafe_allow_html=True)\n",
    "\n",
    "    full_answer = \"\".join(collected_parts)\n",
    "    st.session_state.messages.append({\"role\": \"assistant\", \"content\": full_answer})\n",
    "    st.session_state.processing = False\n",
    "\n",
    "    with st.expander(\"🔍 Contextes\"):\n",
    "        for i, ctx in enumerate(contexts):\n",
    "            st.text_area(f\"[{i+1}]\", ctx, height=120)\n",
    "'''\n",
    "\n",
    "# Écriture du fichier app.py\n",
    "with open('app.py', 'w', encoding='utf-8') as f:\n",
    "    f.write(app_code)\n",
    "\n",
    "print(\"✅ Fichier app.py créé avec succès !\")\n",
    "print(\"📁 L'application RAG PDF Chat est maintenant prête.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26464fc6",
   "metadata": {},
   "source": [
    "## 🧪 Section 8 : Test des composants principaux\n",
    "\n",
    "### 📘 Description pédagogique (non technique)\n",
    "Avant de lancer l'application complète, nous allons tester que tous les composants fonctionnent correctement :\n",
    "\n",
    "### 🔍 Tests à effectuer :\n",
    "1. **Connexion Ollama** : Vérifier que le serveur répond\n",
    "2. **Modèles disponibles** : Lister les modèles téléchargés\n",
    "3. **Test simple** : Générer une réponse de test\n",
    "4. **Test d'embedding** : Transformer du texte en vecteurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05cb9f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "import time\n",
    "\n",
    "print(\"🔍 Test de la connexion Ollama...\")\n",
    "try:\n",
    "    # Test de connexion\n",
    "    models = ollama.list()\n",
    "    print(\"✅ Connexion Ollama réussie !\")\n",
    "    print(f\"📋 Modèles disponibles : {len(models['models'])}\")\n",
    "    \n",
    "    for model in models['models']:\n",
    "        print(f\"  🤖 {model['name']}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ Erreur de connexion : {e}\")\n",
    "    print(\"⏳ Attente de 10 secondes supplémentaires...\")\n",
    "    time.sleep(10)\n",
    "\n",
    "print(\"\\n🧪 Test de génération de texte...\")\n",
    "try:\n",
    "    response = ollama.chat(\n",
    "        model='llama3.2',\n",
    "        messages=[{\n",
    "            'role': 'user',\n",
    "            'content': 'Bonjour, peux-tu me dire en une phrase ce que tu peux faire ?'\n",
    "        }]\n",
    "    )\n",
    "    print(\"✅ Test de génération réussi !\")\n",
    "    print(f\"🤖 Réponse : {response['message']['content']}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Erreur de génération : {e}\")\n",
    "\n",
    "print(\"\\n🔢 Test d'embedding...\")\n",
    "try:\n",
    "    embedding = ollama.embeddings(\n",
    "        model='nomic-embed-text',\n",
    "        prompt='Ceci est un test d\\'embedding'\n",
    "    )\n",
    "    print(\"✅ Test d'embedding réussi !\")\n",
    "    print(f\"📊 Dimension du vecteur : {len(embedding['embedding'])}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Erreur d'embedding : {e}\")\n",
    "\n",
    "print(\"\\n🎉 Tous les tests sont terminés !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb694fb3",
   "metadata": {},
   "source": [
    "## 🚀 Section 9 : Lancement de l'application RAG PDF Chat\n",
    "\n",
    "### 📘 Description pédagogique (non technique)\n",
    "Cette section permet de lancer une interface web (une mini-application) et de la rendre accessible depuis Internet grâce à un outil appelé LocalTunnel.\n",
    "\n",
    "### 🔧 Voici ce que chaque ligne fait :\n",
    "\n",
    "**`!npm install localtunnel`**\n",
    "- Installe l'outil LocalTunnel, qui va permettre de partager l'application Streamlit via un lien web.\n",
    "\n",
    "**`import urllib + print(...)`**\n",
    "- Affiche l'adresse IP publique de votre environnement pour référence (souvent inutile côté utilisateur, mais utile pour des logs ou du debug).\n",
    "\n",
    "**`!streamlit run app.py &>/content/logs.txt &`**\n",
    "- Lance l'application Streamlit (app.py) en arrière-plan. C'est cette application qui permet d'interagir avec le modèle IA via une interface utilisateur.\n",
    "\n",
    "**`npx localtunnel --port 8501`**\n",
    "- Crée un lien temporaire et public vers l'application, utilisable depuis n'importe quel navigateur.\n",
    "\n",
    "### 💡 Pourquoi on fait ça ?\n",
    "On crée ici une interface simple et accessible (dans un navigateur) pour interagir avec le modèle IA, sans écrire de code.\n",
    "Et comme Colab ou certains environnements locaux n'ont pas d'adresse web fixe, LocalTunnel sert de pont entre votre application et le reste du monde.\n",
    "\n",
    "### 🌐 Instructions importantes :\n",
    "1. **Cliquez sur le lien** qui apparaîtra après l'exécution\n",
    "2. **Entrez le mot de passe** qui s'affichera dans les logs\n",
    "3. **Profitez de votre application** RAG PDF Chat !\n",
    "\n",
    "### ⚠️ Attention :\n",
    "- Le lien LocalTunnel est temporaire et change à chaque exécution\n",
    "- L'application reste active tant que la cellule tourne\n",
    "- Pour arrêter, utilisez le bouton stop ou redémarrez le runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9bb680e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!npm install localtunnel\n",
    "\n",
    "import urllib\n",
    "print(\"🔑 Password/Endpoint IP for localtunnel is:\", urllib.request.urlopen('https://ipv4.icanhazip.com').read().decode('utf8').strip(\"\\n\"))\n",
    "print(\"\\n🚀 Lancement de l'application RAG PDF Chat...\")\n",
    "print(\"📱 L'interface sera disponible via le lien LocalTunnel qui va apparaître.\")\n",
    "print(\"⏳ Patientez quelques secondes...\")\n",
    "\n",
    "!streamlit run app.py &>/content/logs.txt & npx localtunnel --port 8501"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56dcdd0",
   "metadata": {},
   "source": [
    "## 🎓 Guide d'utilisation et conclusion\n",
    "\n",
    "### 🎯 **Félicitations !** \n",
    "\n",
    "Vous avez maintenant une application RAG complètement fonctionnelle ! 🎉\n",
    "\n",
    "---\n",
    "\n",
    "### 📖 **Guide d'utilisation de votre application :**\n",
    "\n",
    "#### 1. **📄 Upload de documents**\n",
    "- Cliquez sur \"Browse files\" dans la sidebar\n",
    "- Sélectionnez un ou plusieurs fichiers PDF\n",
    "- L'application va automatiquement les analyser et les indexer\n",
    "\n",
    "#### 2. **💬 Interaction avec l'IA**\n",
    "- Tapez votre question dans la zone de chat\n",
    "- L'IA va chercher les informations pertinentes dans vos documents\n",
    "- Elle formulera une réponse basée uniquement sur le contenu de vos PDF\n",
    "\n",
    "#### 3. **🔍 Vérification des sources**\n",
    "- Cliquez sur \"🔍 Contextes\" pour voir les extraits utilisés\n",
    "- Chaque réponse est sourcée et vérifiable\n",
    "\n",
    "---\n",
    "\n",
    "### 🎨 **Exemples de questions à poser :**\n",
    "\n",
    "- \"De quoi parle ce document ?\"\n",
    "- \"Quels sont les points clés ?\"\n",
    "- \"Peux-tu résumer les conclusions ?\"\n",
    "- \"Que dit le document sur [sujet spécifique] ?\"\n",
    "- \"Quelles sont les recommandations ?\"\n",
    "\n",
    "---\n",
    "\n",
    "### 🛠️ **Ce que vous avez appris :**\n",
    "\n",
    "✅ **Configuration d'un environnement IA** sur Google Colab  \n",
    "✅ **Installation et utilisation d'Ollama** pour les modèles locaux  \n",
    "✅ **Création d'un système RAG** avec indexation vectorielle  \n",
    "✅ **Développement d'une interface web** avec Streamlit  \n",
    "✅ **Traitement et analyse de documents PDF** avec l'IA  \n",
    "✅ **Deployment avec LocalTunnel** pour partager votre application  \n",
    "\n",
    "---\n",
    "\n",
    "### 🚀 **Pour aller plus loin :**\n",
    "\n",
    "- **Personnaliser l'interface** : Modifier les couleurs et le style\n",
    "- **Ajouter d'autres formats** : Word, PowerPoint, etc.\n",
    "- **Améliorer les prompts** : Personnaliser les réponses\n",
    "- **Optimiser les performances** : Ajuster les paramètres RAG\n",
    "- **Déployer en production** : Utiliser des services cloud\n",
    "\n",
    "---\n",
    "\n",
    "### 📚 **Ressources pour continuer :**\n",
    "\n",
    "- [Documentation Ollama](https://ollama.ai/)\n",
    "- [Documentation Streamlit](https://docs.streamlit.io/)\n",
    "- [Guide FAISS](https://github.com/facebookresearch/faiss)\n",
    "- [LangChain Documentation](https://docs.langchain.com/)\n",
    "\n",
    "---\n",
    "\n",
    "### 🤝 **Merci d'avoir suivi cette formation !**\n",
    "\n",
    "Vous maîtrisez maintenant les bases du RAG et pouvez créer vos propres applications d'IA documentaire. N'hésitez pas à expérimenter et à adapter le code à vos besoins spécifiques !\n",
    "\n",
    "---\n",
    "\n",
    "**💡 Tip :** Sauvegardez ce notebook dans votre Google Drive pour pouvoir le réutiliser facilement !"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
