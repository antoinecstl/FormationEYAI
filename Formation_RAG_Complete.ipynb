{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eaa1feab",
   "metadata": {},
   "source": [
    "# ğŸš€ Formation IA : CrÃ©er un Chatbot RAG\n",
    "## Retrieval-Augmented Generation pour l'analyse de documents PDF\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ¯ **Objectifs de la formation**\n",
    "\n",
    "Ã€ la fin de cette formation, vous saurez :\n",
    "- âœ… Configurer un environnement IA sur Google Colab\n",
    "- âœ… Installer et utiliser Ollama pour faire tourner des modÃ¨les localement\n",
    "- âœ… CrÃ©er un systÃ¨me RAG (Retrieval-Augmented Generation)\n",
    "- âœ… DÃ©velopper une interface web avec Streamlit\n",
    "- âœ… Analyser et questionner des documents PDF avec l'IA\n",
    "\n",
    "### ğŸ“š **Qu'est-ce que le RAG ?**\n",
    "\n",
    "Le **RAG (Retrieval-Augmented Generation)** est une technique qui combine :\n",
    "- ğŸ” **Recherche** : Trouve les informations pertinentes dans vos documents\n",
    "- ğŸ¤– **GÃ©nÃ©ration** : Utilise un modÃ¨le IA pour formuler des rÃ©ponses basÃ©es sur ces informations\n",
    "\n",
    "C'est comme avoir un assistant qui lit vos documents et rÃ©pond Ã  vos questions de maniÃ¨re intelligente !\n",
    "\n",
    "---\n",
    "\n",
    "### â±ï¸ **DurÃ©e estimÃ©e** : 2-3 heures\n",
    "### ğŸ’» **Plateforme** : Google Colab (gratuit)\n",
    "### ğŸ§  **Niveau** : DÃ©butant Ã  intermÃ©diaire\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baff6c37",
   "metadata": {},
   "source": [
    "## ğŸ”— Section 1 : Connexion Ã  Google Drive\n",
    "\n",
    "### ğŸ“˜ Description pÃ©dagogique (non technique)\n",
    "**Cette cellule permet de connecter Google Drive Ã  l'environnement de travail.** Cela permet d'utiliser des fichiers (ex : jeux de donnÃ©es, documents, modÃ¨les) qui sont stockÃ©s dans votre Google Drive directement dans le Notebook.\n",
    "\n",
    "### ğŸ’¡ Pourquoi on fait Ã§a ?\n",
    "Cela Ã©vite de devoir uploader manuellement des fichiers Ã  chaque fois et permet de sauvegarder facilement notre travail.\n",
    "\n",
    "### ğŸ› ï¸ Instructions :\n",
    "1. ExÃ©cutez la cellule ci-dessous\n",
    "2. Cliquez sur le lien qui apparaÃ®t\n",
    "3. Connectez-vous Ã  votre compte Google\n",
    "4. Copiez le code d'autorisation\n",
    "5. Collez-le dans la zone de texte qui apparaÃ®t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecdfd01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount(\"/content/gdrive\")\n",
    "print(\"âœ… Google Drive connectÃ© avec succÃ¨s !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de39dc2",
   "metadata": {},
   "source": [
    "## ğŸ“ Section 2 : Configuration de l'environnement de travail\n",
    "\n",
    "### ğŸ“˜ Description pÃ©dagogique (non technique)\n",
    "**Cette cellule permet de changer le dossier de travail du notebook pour aller dans le dossier principal de votre Google Drive** (Mon Drive).\n",
    "Cela facilite l'accÃ¨s direct aux fichiers sans avoir Ã  Ã©crire un chemin complet Ã  chaque fois.\n",
    "\n",
    "### ğŸ’¡ Pourquoi on fait Ã§a ?\n",
    "En changeant de dossier, on peut lire et enregistrer des fichiers (ex : jeux de donnÃ©es, rÃ©sultats, imagesâ€¦) comme si on Ã©tait dÃ©jÃ  dans Google Drive, ce qui simplifie beaucoup les manipulations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7110ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd gdrive/MyDrive\n",
    "print(\"ğŸ“‚ RÃ©pertoire de travail changÃ© vers Google Drive\")\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4096cf12",
   "metadata": {},
   "source": [
    "## ğŸŒ Section 3 : TÃ©lÃ©chargement des fichiers nÃ©cessaires depuis GitHub\n",
    "\n",
    "### ğŸ“˜ Description pÃ©dagogique (non technique)\n",
    "Cette cellule permet de copier un dossier contenant des fichiers depuis GitHub vers l'environnement de travail du notebook.\n",
    "\n",
    "### ğŸ’¡ Pourquoi on fait Ã§a ?\n",
    "GitHub est une plateforme utilisÃ©e pour stocker, partager du code ou des fichiers de projet. Ici, on tÃ©lÃ©charge les ressources nÃ©cessaires Ã  la formation pour pouvoir les utiliser facilement dans les cellules suivantes.\n",
    "\n",
    "### ğŸ“¦ Ce qu'on tÃ©lÃ©charge :\n",
    "- Le code de l'application Streamlit\n",
    "- Les fichiers de configuration\n",
    "- Les modÃ¨les et outils nÃ©cessaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a65e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/antoinecstl/FormationEYAI.git\n",
    "print(\"âœ… Fichiers de formation tÃ©lÃ©chargÃ©s depuis GitHub\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a6d7ec",
   "metadata": {},
   "source": [
    "### ğŸ“‚ DÃ©placement vers le dossier de formation\n",
    "\n",
    "### ğŸ“˜ Description pÃ©dagogique (non technique)\n",
    "Cette cellule permet de se dÃ©placer dans le dossier FormationEYAI que vous venez de tÃ©lÃ©charger depuis GitHub.\n",
    "\n",
    "### ğŸ’¡ Pourquoi on fait Ã§a ?\n",
    "En se plaÃ§ant dans ce dossier, on pourra accÃ©der plus facilement aux fichiers utiles pour la suite de la formation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438afe08",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd FormationEYAI\n",
    "print(\"ğŸ“ Maintenant dans le dossier FormationEYAI\")\n",
    "!ls -la"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9bdff8",
   "metadata": {},
   "source": [
    "## ğŸ“¦ Section 4 : Installation des dÃ©pendances Python\n",
    "\n",
    "### ğŸ“˜ Description pÃ©dagogique (non technique)\n",
    "**Cette cellule permet d'installer automatiquement tous les outils nÃ©cessaires Ã  la formation Ã  partir d'un fichier spÃ©cial appelÃ© requirements.txt.**\n",
    "Ce fichier contient la liste des modules Python dont on aura besoin pour que le code fonctionne.\n",
    "\n",
    "### ğŸ’¡ Pourquoi on fait Ã§a ?\n",
    "PlutÃ´t que d'installer chaque outil un par un, ce fichier centralise tout, ce qui fait gagner du temps et Ã©vite les erreurs d'oubli ou d'incompatibilitÃ©.\n",
    "\n",
    "### ğŸ”§ Modules qui seront installÃ©s :\n",
    "- **Streamlit** : Pour crÃ©er l'interface web\n",
    "- **Ollama** : Pour interagir avec les modÃ¨les IA\n",
    "- **FAISS** : Pour la recherche vectorielle rapide\n",
    "- **LangChain** : Pour le traitement de texte avancÃ©\n",
    "- **PyPDF2** : Pour lire les fichiers PDF\n",
    "- **NumPy & scikit-learn** : Pour les calculs mathÃ©matiques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93323186",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt\n",
    "print(\"âœ… Toutes les dÃ©pendances sont installÃ©es !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b66c26f",
   "metadata": {},
   "source": [
    "## ğŸ¤– Section 5 : Installation et lancement d'Ollama\n",
    "\n",
    "### ğŸ“˜ Description pÃ©dagogique (non technique)\n",
    "Cette cellule tÃ©lÃ©charge et installe Ollama, un outil qui permet de faire tourner des modÃ¨les d'intelligence artificielle (comme des LLMs) localement sur la machine, sans avoir besoin d'une connexion Ã  un service cloud.\n",
    "\n",
    "### ğŸ’¡ Pourquoi on fait Ã§a ?\n",
    "Cela permet de tester des modÃ¨les d'IA de maniÃ¨re autonome, sans dÃ©pendre de services externes. C'est particuliÃ¨rement utile pour des raisons de confidentialitÃ©, de coÃ»t ou de performance locale.\n",
    "\n",
    "### ğŸ”’ Avantages d'Ollama :\n",
    "- **Gratuit** : Pas de coÃ»ts d'API\n",
    "- **PrivÃ©** : Vos donnÃ©es restent locales\n",
    "- **Rapide** : Pas de latence rÃ©seau\n",
    "- **Flexible** : Plusieurs modÃ¨les disponibles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89ac49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -fsSL https://ollama.com/install.sh | sh\n",
    "print(\"âœ… Ollama installÃ© avec succÃ¨s !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9aed63",
   "metadata": {},
   "source": [
    "### ğŸš€ DÃ©marrage du serveur Ollama\n",
    "\n",
    "### ğŸ“˜ Description pÃ©dagogique (non technique)\n",
    "Cette cellule sert Ã  lancer le serveur Ollama, c'est-Ã -dire Ã  dÃ©marrer l'outil qui fera fonctionner un modÃ¨le d'IA localement.\n",
    "\n",
    "### ğŸ’¡ Pourquoi on fait Ã§a ?\n",
    "Pour interagir avec un modÃ¨le d'intelligence artificielle installÃ© localement, il faut d'abord dÃ©marrer un service qui Â« Ã©coute Â» et attend nos demandes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a5434a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import time\n",
    "\n",
    "# DÃ©marrage du serveur Ollama en arriÃ¨re-plan\n",
    "sub = subprocess.Popen(\"ollama serve\", shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "print(\"ğŸš€ Serveur Ollama dÃ©marrÃ© !\")\n",
    "print(\"â³ Attente de 5 secondes pour que le serveur soit prÃªt...\")\n",
    "time.sleep(5)\n",
    "print(\"âœ… Serveur Ollama opÃ©rationnel !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221a922a",
   "metadata": {},
   "source": [
    "## ğŸ§  Section 6 : TÃ©lÃ©chargement des modÃ¨les d'IA\n",
    "\n",
    "### ğŸ“˜ Description pÃ©dagogique (non technique)\n",
    "Ces deux commandes permettent de tÃ©lÃ©charger des modÃ¨les d'intelligence artificielle (IA) que l'on utilisera ensuite dans le projet.\n",
    "\n",
    "### ğŸ¤– ModÃ¨les tÃ©lÃ©chargÃ©s :\n",
    "\n",
    "1. **llama3.2** : un modÃ¨le de langage (LLM), capable de rÃ©pondre Ã  des questions, gÃ©nÃ©rer du texte, rÃ©sumer, etc.\n",
    "\n",
    "2. **nomic-embed-text** : un modÃ¨le qui transforme des textes en Â« vecteurs Â», une forme que les machines peuvent comprendre pour faire des recherches sÃ©mantiques ou des comparaisons de sens.\n",
    "\n",
    "### ğŸ’¡ Pourquoi on fait Ã§a ?\n",
    "Les LLM sont prÃ©-entraÃ®nÃ©s. Pour pouvoir les utiliser, il faut d'abord les tÃ©lÃ©charger sur votre machine, un peu comme si vous installiez une application.\n",
    "\n",
    "Sans cela, le systÃ¨me ne saura pas quel modÃ¨le utiliser, ni comment rÃ©pondre aux questions ou traiter les textes.\n",
    "\n",
    "### âš ï¸ Note importante :\n",
    "Le tÃ©lÃ©chargement peut prendre plusieurs minutes selon votre connexion internet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76affdd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ“¥ TÃ©lÃ©chargement du modÃ¨le de langage llama3.2...\")\n",
    "!ollama pull llama3.2\n",
    "print(\"âœ… ModÃ¨le llama3.2 tÃ©lÃ©chargÃ© !\")\n",
    "\n",
    "print(\"\\nğŸ“¥ TÃ©lÃ©chargement du modÃ¨le d'embedding nomic-embed-text...\")\n",
    "!ollama pull nomic-embed-text\n",
    "print(\"âœ… ModÃ¨le nomic-embed-text tÃ©lÃ©chargÃ© !\")\n",
    "\n",
    "print(\"\\nğŸ‰ Tous les modÃ¨les sont prÃªts Ã  Ãªtre utilisÃ©s !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c6e488",
   "metadata": {},
   "source": [
    "## ğŸ–¥ï¸ Section 7 : CrÃ©ation de l'interface utilisateur avec Streamlit\n",
    "\n",
    "### ğŸ“˜ Description pÃ©dagogique (non technique)\n",
    "Nous allons maintenant crÃ©er le fichier `app.py` qui contient tout le code de notre application RAG PDF Chat. Cette application permettra :\n",
    "\n",
    "### ğŸ¯ FonctionnalitÃ©s de l'application :\n",
    "- ğŸ“„ **Upload de PDF** : DÃ©poser des documents PDF\n",
    "- ğŸ” **Indexation intelligente** : Analyser et dÃ©couper les documents\n",
    "- ğŸ’¬ **Chat interactif** : Poser des questions sur vos documents\n",
    "- ğŸ¨ **Interface moderne** : Design professionnel avec Streamlit\n",
    "- ğŸ“Š **Affichage des sources** : Voir d'oÃ¹ viennent les rÃ©ponses\n",
    "\n",
    "### ğŸ§  Comment Ã§a marche ?\n",
    "1. **Extraction** : Le texte est extrait des PDF\n",
    "2. **DÃ©coupage** : Le texte est divisÃ© en chunks intelligents\n",
    "3. **Vectorisation** : Chaque chunk devient un vecteur mathÃ©matique\n",
    "4. **Indexation** : Les vecteurs sont stockÃ©s dans une base de donnÃ©es rapide\n",
    "5. **Recherche** : Quand vous posez une question, on trouve les chunks pertinents\n",
    "6. **GÃ©nÃ©ration** : Le modÃ¨le IA formule une rÃ©ponse basÃ©e sur ces chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c299f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CrÃ©ation du fichier app.py avec le code complet de l'application RAG\n",
    "app_code = '''\n",
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import tempfile\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "\n",
    "import faiss  # type: ignore\n",
    "import numpy as np\n",
    "import streamlit as st\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import ollama  # pip install ollama-python\n",
    "\n",
    "# --------------------- UI & style -----------------------------------------\n",
    "st.set_page_config(page_title=\"ğŸ¤– RAG PDF Chat\", page_icon=\"ğŸ¤–\", layout=\"wide\")\n",
    "\n",
    "st.markdown(\n",
    "    \"\"\"\n",
    "<style>\n",
    "html, body {\n",
    "    font-family: 'Helvetica Neue', sans-serif;\n",
    "    background-color: #f4f7fa;\n",
    "}\n",
    ".sidebar .sidebar-content {\n",
    "    background-color: #ffffff;\n",
    "}\n",
    ".chat-container {\n",
    "    max-width: 1200px;\n",
    "    margin: auto;\n",
    "    padding: 1rem;\n",
    "}\n",
    ".user-msg {\n",
    "    background: #007bff;\n",
    "    color: white;\n",
    "    border-radius: 20px 20px 0px 20px;\n",
    "    padding: 1rem;\n",
    "    margin: 0.5rem 0;\n",
    "    align-self: flex-end;\n",
    "    max-width: 100%;\n",
    "    word-wrap: break-word;\n",
    "}\n",
    ".bot-msg {\n",
    "    background: #e9ecef;\n",
    "    color: #212529;\n",
    "    border-radius: 20px 20px 20px 0px;\n",
    "    padding: 1rem;\n",
    "    margin: 0.5rem 0;\n",
    "    align-self: flex-start;\n",
    "    max-width: 100%;\n",
    "    word-wrap: break-word;\n",
    "}\n",
    ".chat-area {\n",
    "    display: flex;\n",
    "    flex-direction: column;\n",
    "}\n",
    "</style>\n",
    "\"\"\",\n",
    "    unsafe_allow_html=True,\n",
    ")\n",
    "\n",
    "# --------------------- Config ---------------------------------------------\n",
    "MODEL_NAME = \"llama3.2:3b\"\n",
    "EMBEDDING_MODEL = \"nomic-embed-text:latest\"\n",
    "DOC_TOP_K = 3\n",
    "CHUNK_TOP_K = 5\n",
    "CANDIDATES_K = 20\n",
    "NEIGHBORS = 1\n",
    "LAMBDA_DIVERSITY = 0.3\n",
    "SIM_THRESHOLD = 0.25\n",
    "TEMPERATURE = 0.2\n",
    "MAX_TOKENS = 2048\n",
    "\n",
    "# --------------------- Helpers LLM ----------------------------------------\n",
    "\n",
    "def _call_llm(messages: List[Dict[str, str]], *, temperature: float = 0.1, max_tokens: int = 2048, stream: bool = False):\n",
    "    return ollama.chat(\n",
    "        model=MODEL_NAME,\n",
    "        messages=messages,\n",
    "        stream=stream,\n",
    "        options={\"temperature\": temperature, \"num_predict\": max_tokens},\n",
    "    )\n",
    "\n",
    "\n",
    "def embed_texts(texts: List[str]) -> np.ndarray:\n",
    "    return np.array([ollama.embeddings(model=EMBEDDING_MODEL, prompt=t)[\"embedding\"] for t in texts], dtype=\"float32\")\n",
    "\n",
    "# --------------------- PDF utils -----------------------------------------\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    import re\n",
    "    text = re.sub(r\"\\n{2,}\", \"\\n\\n\", text)\n",
    "    text = re.sub(r\"(\\w+)-\\n(\\w+)\", r\"\\1\\2\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def extract_pdf_text(path: str) -> str:\n",
    "    return clean_text(\"\\n\".join(page.extract_text() or \"\" for page in PdfReader(path).pages))\n",
    "\n",
    "# --------------------- Chunking & rÃ©sumÃ© ----------------------------------\n",
    "\n",
    "def auto_chunk_size(tokens: int) -> int:\n",
    "    return 1024 if tokens < 8000 else 768 if tokens < 20000 else 512\n",
    "\n",
    "\n",
    "def chunk_document(text: str) -> List[str]:\n",
    "    size = auto_chunk_size(len(text.split()))\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        separators=[\"\\n\\n\", \"\\n\", \". \"],\n",
    "        chunk_size=size,\n",
    "        chunk_overlap=int(size*0.25),\n",
    "        length_function=len,\n",
    "    )\n",
    "    return [c for c in splitter.split_text(text) if len(c) > 100]\n",
    "\n",
    "\n",
    "def make_summary(text: str) -> str:\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"Vous Ãªtes un expert en synthÃ¨se documentaire. RÃ©sumez le texte suivant en trois parties : (1) Contexte, (2) Points clÃ©s, (3) Conclusions. RÃ©pondez en franÃ§ais.\"},\n",
    "        {\"role\": \"user\", \"content\": text[:120000]}\n",
    "    ]\n",
    "    return _call_llm(messages)[\"message\"][\"content\"].strip()\n",
    "\n",
    "# --------------------- Index hiÃ©rarchique ---------------------------------\n",
    "class RagIndex:\n",
    "    def __init__(self):\n",
    "        self.doc_index: Optional[faiss.IndexFlatIP] = None\n",
    "        self.chunk_index: Optional[faiss.Index] = None\n",
    "        self.doc_meta: List[Dict[str, Any]] = []\n",
    "        self.chunk_meta: List[Dict[str, Any]] = []\n",
    "        self.chunk_emb: Optional[np.ndarray] = None\n",
    "\n",
    "    def build(self, uploaded_files: List[st.runtime.uploaded_file_manager.UploadedFile]):\n",
    "        doc_embs, chunk_embs_list = [], []\n",
    "        for doc_id, uf in enumerate(uploaded_files):\n",
    "            with tempfile.NamedTemporaryFile(delete=False, suffix=\".pdf\") as tmp:\n",
    "                tmp.write(uf.getbuffer())\n",
    "                path = tmp.name\n",
    "            full_text = extract_pdf_text(path)\n",
    "            os.unlink(path)\n",
    "\n",
    "            summary = make_summary(full_text)\n",
    "            self.doc_meta.append({\"filename\": uf.name, \"summary\": summary})\n",
    "            doc_embs.append(embed_texts([summary])[0])\n",
    "\n",
    "            chunks = chunk_document(full_text)\n",
    "            chunk_embs = embed_texts(chunks)\n",
    "            chunk_embs_list.append(chunk_embs)\n",
    "            for i, txt in enumerate(chunks):\n",
    "                self.chunk_meta.append({\"doc_id\": doc_id, \"text\": txt, \"chunk_id\": i})\n",
    "\n",
    "        self.doc_index = faiss.IndexFlatIP(len(doc_embs[0]))\n",
    "        self.doc_index.add(np.vstack(doc_embs).astype(\"float32\"))\n",
    "\n",
    "        self.chunk_emb = np.vstack(chunk_embs_list).astype(\"float32\")\n",
    "        self.chunk_index = faiss.IndexHNSWFlat(self.chunk_emb.shape[1], 32)\n",
    "        self.chunk_index.add(self.chunk_emb)\n",
    "\n",
    "    def _is_global(self, query: str, thr: float = 0.78) -> bool:\n",
    "        examples = [\n",
    "            \"De quoi parle ce document ?\",\n",
    "            \"Quel est le sujet principal ?\",\n",
    "            \"Fais un rÃ©sumÃ© du document\",\n",
    "        ]\n",
    "        emb_q = embed_texts([query])[0]\n",
    "        emb_ex = embed_texts(examples)\n",
    "        sims = emb_ex @ emb_q / (np.linalg.norm(emb_ex, axis=1) * np.linalg.norm(emb_q) + 1e-6)\n",
    "        return float(np.max(sims)) >= thr\n",
    "\n",
    "    def _mmr(self, q: np.ndarray, cand: np.ndarray, k: int) -> List[int]:\n",
    "        selected, rest = [], list(range(len(cand)))\n",
    "        while len(selected) < min(k, len(rest)):\n",
    "            best, best_score = None, -1e9\n",
    "            for idx in rest:\n",
    "                sim_q = float(q @ cand[idx] / (np.linalg.norm(q) * np.linalg.norm(cand[idx]) + 1e-6))\n",
    "                sim_s = max(cosine_similarity(cand[idx][None, :], cand[selected])[0]) if selected else 0.\n",
    "                score = LAMBDA_DIVERSITY*sim_q - (1-LAMBDA_DIVERSITY)*sim_s\n",
    "                if score > best_score:\n",
    "                    best, best_score = idx, score\n",
    "            selected.append(best)\n",
    "            rest.remove(best)\n",
    "        return selected\n",
    "\n",
    "    def retrieve(self, query: str) -> Tuple[List[str], List[int], Optional[str]]:\n",
    "        q_emb = embed_texts([query])[0]\n",
    "        _, I_doc = self.doc_index.search(q_emb[None, :], DOC_TOP_K)\n",
    "        allowed = set(I_doc[0])\n",
    "\n",
    "        mask = [i for i, m in enumerate(self.chunk_meta) if m[\"doc_id\"] in allowed]\n",
    "        sub_emb = self.chunk_emb[mask]\n",
    "        sub_idx = faiss.IndexFlatIP(sub_emb.shape[1])\n",
    "        sub_idx.add(sub_emb)\n",
    "        D, I = sub_idx.search(q_emb[None, :], min(CANDIDATES_K, len(mask)))\n",
    "        pool = [mask[idx] for idx in I[0]]\n",
    "        pool = [idx for idx, d in zip(pool, D[0]) if 1-d <= SIM_THRESHOLD] or [mask[I[0][0]]]\n",
    "\n",
    "        cand_emb = self.chunk_emb[pool]\n",
    "        selected = [pool[i] for i in self._mmr(q_emb, cand_emb, CHUNK_TOP_K)]\n",
    "        expanded = {j for idx in selected for j in range(idx-NEIGHBORS, idx+NEIGHBORS+1)}\n",
    "        final = [i for i in expanded if 0 <= i < len(self.chunk_meta)][:CHUNK_TOP_K]\n",
    "\n",
    "        contexts = [self.chunk_meta[i][\"text\"] for i in final]\n",
    "        summary = self.doc_meta[int(I_doc[0][0])][\"summary\"] if self._is_global(query) else None\n",
    "        return contexts, final, summary\n",
    "\n",
    "# ---------------- Prompt builder -----------------------------------------\n",
    "def build_prompt(question: str, contexts: List[str], summary: Optional[str], history: List[Dict[str, str]] = None):\n",
    "    ctx_block = \"\\n\\n\".join(f\"[{i+1}] {c}\" for i, c in enumerate(contexts))\n",
    "    if summary:\n",
    "        ctx_block = f\"[RÃ©sumÃ©] {summary}\\n\\n\" + ctx_block\n",
    "    \n",
    "    system = (\n",
    "        \"Vous Ãªtes un assistant expert. Utilisez uniquement les informations suivantes pour rÃ©pondre en franÃ§ais. \"\n",
    "        \"Citez vos sources avec les balises [n]. Si l'information n'est pas trouvÃ©e, informez-en l'utilisateur.\"\n",
    "    )\n",
    "    \n",
    "    messages = [{\"role\": \"system\", \"content\": system}]\n",
    "    \n",
    "    if history and len(history) > 0:\n",
    "        previous_messages = history[:-1] if history[-1][\"role\"] == \"user\" else history\n",
    "        messages.extend(previous_messages)\n",
    "    \n",
    "    current_user_content = f\"CONTEXTE(S):\\n{ctx_block}\\n\\nQUESTION: {question}\\n\\nRÃ©ponse:\"\n",
    "    messages.append({\"role\": \"user\", \"content\": current_user_content})\n",
    "    \n",
    "    return messages\n",
    "\n",
    "# ---------------- Etat Streamlit ----------------------------------------\n",
    "if \"messages\" not in st.session_state:\n",
    "    st.session_state.messages = []\n",
    "if \"rag\" not in st.session_state:\n",
    "    st.session_state.rag = None\n",
    "if \"processing\" not in st.session_state:\n",
    "    st.session_state.processing = False\n",
    "\n",
    "# ---------------- Sidebar upload ----------------------------------------\n",
    "with st.sidebar:\n",
    "    st.header(\"ğŸ“š Documents\")\n",
    "    files = st.file_uploader(\"DÃ©posez vos PDF\", type=[\"pdf\"], accept_multiple_files=True)\n",
    "    if st.button(\"ğŸ”„ RÃ©initialiser\"):\n",
    "        st.session_state.clear()\n",
    "        st.rerun()\n",
    "\n",
    "# ---------------- Index construction ------------------------------------\n",
    "if files and st.session_state.rag is None:\n",
    "    with st.spinner(\"ğŸ“„ Indexation en coursâ€¦\"):\n",
    "        rag = RagIndex()\n",
    "        rag.build(files)\n",
    "        st.session_state.rag = rag\n",
    "    st.success(f\"{len(files)} document(s) indexÃ©(s) ! Posez vos questions.\")\n",
    "\n",
    "# ---------------- Chat display -----------------------------------------\n",
    "st.markdown(\"<div class='chat-container'>\", unsafe_allow_html=True)\n",
    "if not st.session_state.messages:\n",
    "    if st.session_state.rag is not None:\n",
    "        st.markdown(\n",
    "            \"\"\"\n",
    "            <div class=\"bot-msg\">\n",
    "            ğŸ‘‹ Bonjour ! Je suis votre assistant IA documentaire.\n",
    "            <br>Posez-moi une question sur le contenu de vos documents.\n",
    "            </div>\n",
    "            \"\"\",\n",
    "            unsafe_allow_html=True\n",
    "        )\n",
    "    else:\n",
    "        st.markdown(\n",
    "            \"\"\"\n",
    "            <div class=\"bot-msg\">\n",
    "            ğŸ‘‹ Bienvenue dans RAG PDF Chat !\n",
    "            <br>Commencez par tÃ©lÃ©charger un ou plusieurs documents PDF dans le panneau latÃ©ral.\n",
    "            </div>\n",
    "            \"\"\",\n",
    "            unsafe_allow_html=True\n",
    "        )\n",
    "else:\n",
    "    st.markdown(\"<div class='chat-area'>\", unsafe_allow_html=True)\n",
    "    for msg in st.session_state.messages:\n",
    "        css = \"user-msg\" if msg[\"role\"] == \"user\" else \"bot-msg\"\n",
    "        st.markdown(f'<div class=\"{css}\">{msg[\"content\"]}</div>', unsafe_allow_html=True)\n",
    "    st.markdown(\"</div>\", unsafe_allow_html=True)\n",
    "st.markdown(\"</div>\", unsafe_allow_html=True)\n",
    "\n",
    "# ---------------- Chat input -------------------------------------------\n",
    "query = st.chat_input(\"Votre questionâ€¦\", disabled=st.session_state.processing or st.session_state.rag is None)\n",
    "\n",
    "if query:\n",
    "    st.session_state.messages.append({\"role\": \"user\", \"content\": query})\n",
    "    st.markdown(f'<div class=\"user-msg\">{query}</div>', unsafe_allow_html=True)\n",
    "    st.session_state.processing = True\n",
    "\n",
    "    rag: RagIndex = st.session_state.rag  # type: ignore\n",
    "    contexts, indices, summary = rag.retrieve(query)\n",
    "    \n",
    "    prompt = build_prompt(query, contexts, summary, st.session_state.messages)\n",
    "\n",
    "    placeholder = st.empty()\n",
    "    collected_parts: List[str] = []\n",
    "\n",
    "    for chunk in _call_llm(prompt, temperature=TEMPERATURE, max_tokens=MAX_TOKENS, stream=True):\n",
    "        token = chunk[\"message\"][\"content\"]\n",
    "        collected_parts.append(token)\n",
    "        placeholder.markdown(f'<div class=\"bot-msg\">{\"\".join(collected_parts)}</div>', unsafe_allow_html=True)\n",
    "\n",
    "    full_answer = \"\".join(collected_parts)\n",
    "    st.session_state.messages.append({\"role\": \"assistant\", \"content\": full_answer})\n",
    "    st.session_state.processing = False\n",
    "\n",
    "    with st.expander(\"ğŸ” Contextes\"):\n",
    "        for i, ctx in enumerate(contexts):\n",
    "            st.text_area(f\"[{i+1}]\", ctx, height=120)\n",
    "'''\n",
    "\n",
    "# Ã‰criture du fichier app.py\n",
    "with open('app.py', 'w', encoding='utf-8') as f:\n",
    "    f.write(app_code)\n",
    "\n",
    "print(\"âœ… Fichier app.py crÃ©Ã© avec succÃ¨s !\")\n",
    "print(\"ğŸ“ L'application RAG PDF Chat est maintenant prÃªte.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26464fc6",
   "metadata": {},
   "source": [
    "## ğŸ§ª Section 8 : Test des composants principaux\n",
    "\n",
    "### ğŸ“˜ Description pÃ©dagogique (non technique)\n",
    "Avant de lancer l'application complÃ¨te, nous allons tester que tous les composants fonctionnent correctement :\n",
    "\n",
    "### ğŸ” Tests Ã  effectuer :\n",
    "1. **Connexion Ollama** : VÃ©rifier que le serveur rÃ©pond\n",
    "2. **ModÃ¨les disponibles** : Lister les modÃ¨les tÃ©lÃ©chargÃ©s\n",
    "3. **Test simple** : GÃ©nÃ©rer une rÃ©ponse de test\n",
    "4. **Test d'embedding** : Transformer du texte en vecteurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05cb9f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "import time\n",
    "\n",
    "print(\"ğŸ” Test de la connexion Ollama...\")\n",
    "try:\n",
    "    # Test de connexion\n",
    "    models = ollama.list()\n",
    "    print(\"âœ… Connexion Ollama rÃ©ussie !\")\n",
    "    print(f\"ğŸ“‹ ModÃ¨les disponibles : {len(models['models'])}\")\n",
    "    \n",
    "    for model in models['models']:\n",
    "        print(f\"  ğŸ¤– {model['name']}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Erreur de connexion : {e}\")\n",
    "    print(\"â³ Attente de 10 secondes supplÃ©mentaires...\")\n",
    "    time.sleep(10)\n",
    "\n",
    "print(\"\\nğŸ§ª Test de gÃ©nÃ©ration de texte...\")\n",
    "try:\n",
    "    response = ollama.chat(\n",
    "        model='llama3.2',\n",
    "        messages=[{\n",
    "            'role': 'user',\n",
    "            'content': 'Bonjour, peux-tu me dire en une phrase ce que tu peux faire ?'\n",
    "        }]\n",
    "    )\n",
    "    print(\"âœ… Test de gÃ©nÃ©ration rÃ©ussi !\")\n",
    "    print(f\"ğŸ¤– RÃ©ponse : {response['message']['content']}\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Erreur de gÃ©nÃ©ration : {e}\")\n",
    "\n",
    "print(\"\\nğŸ”¢ Test d'embedding...\")\n",
    "try:\n",
    "    embedding = ollama.embeddings(\n",
    "        model='nomic-embed-text',\n",
    "        prompt='Ceci est un test d\\'embedding'\n",
    "    )\n",
    "    print(\"âœ… Test d'embedding rÃ©ussi !\")\n",
    "    print(f\"ğŸ“Š Dimension du vecteur : {len(embedding['embedding'])}\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Erreur d'embedding : {e}\")\n",
    "\n",
    "print(\"\\nğŸ‰ Tous les tests sont terminÃ©s !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb694fb3",
   "metadata": {},
   "source": [
    "## ğŸš€ Section 9 : Lancement de l'application RAG PDF Chat\n",
    "\n",
    "### ğŸ“˜ Description pÃ©dagogique (non technique)\n",
    "Cette section permet de lancer une interface web (une mini-application) et de la rendre accessible depuis Internet grÃ¢ce Ã  un outil appelÃ© LocalTunnel.\n",
    "\n",
    "### ğŸ”§ Voici ce que chaque ligne fait :\n",
    "\n",
    "**`!npm install localtunnel`**\n",
    "- Installe l'outil LocalTunnel, qui va permettre de partager l'application Streamlit via un lien web.\n",
    "\n",
    "**`import urllib + print(...)`**\n",
    "- Affiche l'adresse IP publique de votre environnement pour rÃ©fÃ©rence (souvent inutile cÃ´tÃ© utilisateur, mais utile pour des logs ou du debug).\n",
    "\n",
    "**`!streamlit run app.py &>/content/logs.txt &`**\n",
    "- Lance l'application Streamlit (app.py) en arriÃ¨re-plan. C'est cette application qui permet d'interagir avec le modÃ¨le IA via une interface utilisateur.\n",
    "\n",
    "**`npx localtunnel --port 8501`**\n",
    "- CrÃ©e un lien temporaire et public vers l'application, utilisable depuis n'importe quel navigateur.\n",
    "\n",
    "### ğŸ’¡ Pourquoi on fait Ã§a ?\n",
    "On crÃ©e ici une interface simple et accessible (dans un navigateur) pour interagir avec le modÃ¨le IA, sans Ã©crire de code.\n",
    "Et comme Colab ou certains environnements locaux n'ont pas d'adresse web fixe, LocalTunnel sert de pont entre votre application et le reste du monde.\n",
    "\n",
    "### ğŸŒ Instructions importantes :\n",
    "1. **Cliquez sur le lien** qui apparaÃ®tra aprÃ¨s l'exÃ©cution\n",
    "2. **Entrez le mot de passe** qui s'affichera dans les logs\n",
    "3. **Profitez de votre application** RAG PDF Chat !\n",
    "\n",
    "### âš ï¸ Attention :\n",
    "- Le lien LocalTunnel est temporaire et change Ã  chaque exÃ©cution\n",
    "- L'application reste active tant que la cellule tourne\n",
    "- Pour arrÃªter, utilisez le bouton stop ou redÃ©marrez le runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9bb680e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!npm install localtunnel\n",
    "\n",
    "import urllib\n",
    "print(\"ğŸ”‘ Password/Endpoint IP for localtunnel is:\", urllib.request.urlopen('https://ipv4.icanhazip.com').read().decode('utf8').strip(\"\\n\"))\n",
    "print(\"\\nğŸš€ Lancement de l'application RAG PDF Chat...\")\n",
    "print(\"ğŸ“± L'interface sera disponible via le lien LocalTunnel qui va apparaÃ®tre.\")\n",
    "print(\"â³ Patientez quelques secondes...\")\n",
    "\n",
    "!streamlit run app.py &>/content/logs.txt & npx localtunnel --port 8501"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56dcdd0",
   "metadata": {},
   "source": [
    "## ğŸ“ Guide d'utilisation et conclusion\n",
    "\n",
    "### ğŸ¯ **FÃ©licitations !** \n",
    "\n",
    "Vous avez maintenant une application RAG complÃ¨tement fonctionnelle ! ğŸ‰\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“– **Guide d'utilisation de votre application :**\n",
    "\n",
    "#### 1. **ğŸ“„ Upload de documents**\n",
    "- Cliquez sur \"Browse files\" dans la sidebar\n",
    "- SÃ©lectionnez un ou plusieurs fichiers PDF\n",
    "- L'application va automatiquement les analyser et les indexer\n",
    "\n",
    "#### 2. **ğŸ’¬ Interaction avec l'IA**\n",
    "- Tapez votre question dans la zone de chat\n",
    "- L'IA va chercher les informations pertinentes dans vos documents\n",
    "- Elle formulera une rÃ©ponse basÃ©e uniquement sur le contenu de vos PDF\n",
    "\n",
    "#### 3. **ğŸ” VÃ©rification des sources**\n",
    "- Cliquez sur \"ğŸ” Contextes\" pour voir les extraits utilisÃ©s\n",
    "- Chaque rÃ©ponse est sourcÃ©e et vÃ©rifiable\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ¨ **Exemples de questions Ã  poser :**\n",
    "\n",
    "- \"De quoi parle ce document ?\"\n",
    "- \"Quels sont les points clÃ©s ?\"\n",
    "- \"Peux-tu rÃ©sumer les conclusions ?\"\n",
    "- \"Que dit le document sur [sujet spÃ©cifique] ?\"\n",
    "- \"Quelles sont les recommandations ?\"\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ› ï¸ **Ce que vous avez appris :**\n",
    "\n",
    "âœ… **Configuration d'un environnement IA** sur Google Colab  \n",
    "âœ… **Installation et utilisation d'Ollama** pour les modÃ¨les locaux  \n",
    "âœ… **CrÃ©ation d'un systÃ¨me RAG** avec indexation vectorielle  \n",
    "âœ… **DÃ©veloppement d'une interface web** avec Streamlit  \n",
    "âœ… **Traitement et analyse de documents PDF** avec l'IA  \n",
    "âœ… **Deployment avec LocalTunnel** pour partager votre application  \n",
    "\n",
    "---\n",
    "\n",
    "### ğŸš€ **Pour aller plus loin :**\n",
    "\n",
    "- **Personnaliser l'interface** : Modifier les couleurs et le style\n",
    "- **Ajouter d'autres formats** : Word, PowerPoint, etc.\n",
    "- **AmÃ©liorer les prompts** : Personnaliser les rÃ©ponses\n",
    "- **Optimiser les performances** : Ajuster les paramÃ¨tres RAG\n",
    "- **DÃ©ployer en production** : Utiliser des services cloud\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“š **Ressources pour continuer :**\n",
    "\n",
    "- [Documentation Ollama](https://ollama.ai/)\n",
    "- [Documentation Streamlit](https://docs.streamlit.io/)\n",
    "- [Guide FAISS](https://github.com/facebookresearch/faiss)\n",
    "- [LangChain Documentation](https://docs.langchain.com/)\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ¤ **Merci d'avoir suivi cette formation !**\n",
    "\n",
    "Vous maÃ®trisez maintenant les bases du RAG et pouvez crÃ©er vos propres applications d'IA documentaire. N'hÃ©sitez pas Ã  expÃ©rimenter et Ã  adapter le code Ã  vos besoins spÃ©cifiques !\n",
    "\n",
    "---\n",
    "\n",
    "**ğŸ’¡ Tip :** Sauvegardez ce notebook dans votre Google Drive pour pouvoir le rÃ©utiliser facilement !"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
