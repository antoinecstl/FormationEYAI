{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00b11ac5",
   "metadata": {},
   "source": [
    "# ðŸ¤– Formation RAG â€“ Notebook intÃ©gral"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e049e283",
   "metadata": {},
   "source": [
    "---\n",
    "## ðŸŽ¯ Objectifs pÃ©dagogiques\n",
    "- Comprendre chaque composant dâ€™un **RAG**\n",
    "- Manipuler le code dans Colab\n",
    "- Installer **Ollama** et les modÃ¨les requis\n",
    "- Tester un prototype sur un PDF dâ€™exemple\n",
    "- DÃ©couvrir les Ã©tapes de mise en production (Partieâ€¯2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928f3a19",
   "metadata": {},
   "source": [
    "## ðŸš§ SÃ©quenceâ€¯1.0Â â€“ Installation dâ€™Ollama & des modÃ¨les"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02783f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title ðŸ› ï¸ Installation Ollama (linux/Colab)\n",
    "!curl -fsSL https://ollama.com/install.sh | sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d2d146",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title ðŸš€ Lancement Ollama en arriÃ¨reâ€‘plan\n",
    "import subprocess, time\n",
    "ollama_proc = subprocess.Popen(\"ollama serve\", shell=True)\n",
    "time.sleep(5)\n",
    "print('âœ… Ollama est prÃªt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef6ba77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title ðŸ“¥ TÃ©lÃ©chargement des modÃ¨les\n",
    "!ollama pull llama3.2:3B\n",
    "!ollama pull nomic-embed-text:latest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08aca83",
   "metadata": {},
   "source": [
    "## ðŸ”— SÃ©quenceâ€¯1.1Â â€“ Bootstrap Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac33f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title ðŸ”Œ Connexion GoogleÂ Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2bb2a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title ðŸ“¥ Clone du dÃ©pÃ´t\n",
    "%cd /content/gdrive/MyDrive\n",
    "!git clone https://github.com/antoinecstl/FormationEYAI.git\n",
    "%cd FormationEYAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7a32c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title ðŸ› ï¸ Installation des dÃ©pendances\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d5143d",
   "metadata": {},
   "source": [
    "## ðŸ” SÃ©quenceâ€¯1.2Â â€“ Bases du RAGÂ : embeddings & similaritÃ©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360710df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, ollama\n",
    "\n",
    "EMBED_MODEL = \"nomic-embed-text:latest\"\n",
    "\n",
    "def embed_texts(texts):\n",
    "    \"\"\"Retourne un np.ndarray shape (n, d)\"\"\"\n",
    "    return np.array([ollama.embeddings(model=EMBED_MODEL, prompt=t)['embedding'] for t in texts], dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f380f8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'embed_texts' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m phrase1 = \u001b[33m\"\u001b[39m\u001b[33mLe soleil brille\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      3\u001b[39m phrase2 = \u001b[33m\"\u001b[39m\u001b[33mAujourd\u001b[39m\u001b[33m'\u001b[39m\u001b[33mhui il fait beau\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m vecs = \u001b[43membed_texts\u001b[49m([phrase1, phrase2])\n\u001b[32m      6\u001b[39m sim = \u001b[38;5;28mfloat\u001b[39m(vecs[\u001b[32m0\u001b[39m] @ vecs[\u001b[32m1\u001b[39m] / (np.linalg.norm(vecs[\u001b[32m0\u001b[39m])*np.linalg.norm(vecs[\u001b[32m1\u001b[39m])))\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSimilaritÃ© : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msim\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'embed_texts' is not defined"
     ]
    }
   ],
   "source": [
    "# @title ðŸ”¬ Test d'embedding\n",
    "# @markdown `phrase1` et `phrase2` sont des phrases Ã  comparer.\n",
    "# @markdown La similaritÃ© entre les phrases est calculÃ©e en utilisant le produit scalaire des\n",
    "# @markdown embeddings normalisÃ©s de chaque phrase.\n",
    "# @markdown La similaritÃ© est un nombre entre -1 et 1, oÃ¹ 1 signifie que les phrases sont trÃ¨s similaires,\n",
    "# @markdown 0 signifie qu'elles ne sont pas similaires, et -1 signifie qu'elles sont opposÃ©es.\n",
    "# @markdown Vous pouvez modifier les phrases pour tester d'autres exemples.\n",
    "# @markdown ExÃ©cutez la cellule pour voir le rÃ©sultat.\n",
    "\n",
    "phrase1 = \"The cat is sleeping on the sofa\"\n",
    "phrase2 = \"A cat is napping on the couch\"\n",
    "\n",
    "vecs = embed_texts([phrase1, phrase2])\n",
    "sim = float(vecs[0] @ vecs[1] / (np.linalg.norm(vecs[0])*np.linalg.norm(vecs[1])))\n",
    "print(f\"SimilaritÃ© : {sim:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad5dfe6",
   "metadata": {},
   "source": [
    "## ðŸ“ SÃ©quenceâ€¯1.3Â â€“ Chunking & nettoyage dâ€™un PDF dâ€™exemple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdede86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "def auto_chunk_size(tok:int)->int:\n",
    "    return 1024 if tok<8000 else 768 if tok<20000 else 512\n",
    "\n",
    "def chunk_document(text:str):\n",
    "    size=auto_chunk_size(len(text.split()))\n",
    "    splitter=RecursiveCharacterTextSplitter(\n",
    "        separators=[\"\\n\\n\",\"\\n\",\". \"],\n",
    "        chunk_size=size,\n",
    "        chunk_overlap=size//4,\n",
    "        length_function=len,\n",
    "        )\n",
    "    return [c for c in splitter.split_text(text) if len(c)>100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a544cc02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title ðŸ“– Chargement du PDF d'exemple\n",
    "from PyPDF2 import PdfReader\n",
    "\n",
    "sample_path = \"rapport.pdf\"  # fourni dans le repo\n",
    "pages = PdfReader(sample_path).pages\n",
    "full_text = \"\\n\".join(p.extract_text() or \"\" for p in pages)\n",
    "\n",
    "print(f\"ðŸ“„ Le document contient {len(full_text.split())} tokens environ.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d93bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title ðŸŒ³ Chunking du PDF\n",
    "chunks = chunk_document(full_text)\n",
    "print(f\"ðŸŒ³ {len(chunks)} chunks crÃ©Ã©s. AperÃ§u :\\n{chunks[0][:300]}â€¦\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5e0cab",
   "metadata": {},
   "source": [
    "## ðŸ“Š SÃ©quenceâ€¯1.4Â â€“ Index vectoriel FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce563806",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss, numpy as np\n",
    "\n",
    "def build_faiss_index(vectors:np.ndarray)->faiss.IndexFlatIP:\n",
    "    d=vectors.shape[1]\n",
    "    idx=faiss.IndexFlatIP(d)\n",
    "    idx.add(vectors.astype('float32'))\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202c0cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title ðŸ§ª Construction index chunks\n",
    "import numpy as np\n",
    "\n",
    "chunk_vecs = embed_texts(chunks)\n",
    "index = build_faiss_index(chunk_vecs)\n",
    "\n",
    "print(index.ntotal, \"vecteurs dans l'index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea4bdc9",
   "metadata": {},
   "source": [
    "## ðŸ§® SÃ©quenceâ€¯1.5Â â€“ Algorithme MMR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed16fd2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def mmr(query_vec:np.ndarray, cand:np.ndarray, k:int=5, Î»:float=0.3):\n",
    "    selected, rest = [], list(range(len(cand)))\n",
    "    while len(selected)<min(k,len(rest)):\n",
    "        best, best_score = None, -1e9\n",
    "        for idx in rest:\n",
    "            sim_q = float(query_vec @ cand[idx]/(np.linalg.norm(query_vec)*np.linalg.norm(cand[idx])+1e-6))\n",
    "            sim_s = max(cosine_similarity(cand[idx][None,:], cand[selected])[0]) if selected else 0.\n",
    "            score = Î»*sim_q - (1-Î»)*sim_s\n",
    "            if score>best_score:\n",
    "                best, best_score = idx, score\n",
    "        selected.append(best); rest.remove(best)\n",
    "    return selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad559f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title ðŸ”¬ Test MMR\n",
    "import numpy as np\n",
    "\n",
    "q_vec = embed_texts([\"Sujet principal du rapport ?\"])[0]\n",
    "sel = mmr(q_vec, chunk_vecs, 3)\n",
    "print(sel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e41f64f",
   "metadata": {},
   "source": [
    "## ðŸ§‘â€ðŸŽ¤ SÃ©quenceâ€¯1.6Â â€“ Prompt engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab34cd5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7085934",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(question:str, ctxs:list[str]):\n",
    "    ctx_block=\"\\n\\n\".join(f\"[{i+1}] {c}\" for i,c in enumerate(ctxs))\n",
    "    system=\"Vous Ãªtes un assistant expert. Utilisez uniquement les informations suivantes pour rÃ©pondre en franÃ§ais. Citez les sources [n].\"\n",
    "    return [\n",
    "        {\"role\":\"system\",\"content\":system},\n",
    "        {\"role\":\"user\",\"content\":f\"CONTEXTE(S):\\n{ctx_block}\\n\\nQUESTION: {question}\\n\\nRÃ©ponse:\"}\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84c1dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Â @title ðŸ”¬Â Prompt test (sans LLM pour l'instant)\n",
    "print(build_prompt(\"Pourquoi le ciel est bleuÂ ?\", [\"La diffusion Rayleigh explique la couleur du ciel.\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda2bf49",
   "metadata": {},
   "source": [
    "## ðŸ§‘â€ðŸŽ¤ SÃ©quenceâ€¯1.7Â â€“ Premier Appel au LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d80ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"llama3.2:3B\"\n",
    "\n",
    "def _call_llm(messages: List[Dict[str, str]], *, temperature: float = 0.1, max_tokens: int = 2048, stream: bool = False):\n",
    "    \"\"\"Enveloppe simple autour de ollama.chat pour usage direct.\"\"\"\n",
    "    return ollama.chat(\n",
    "        model=MODEL_NAME,\n",
    "        messages=messages,\n",
    "        stream=stream,\n",
    "        options={\"temperature\": temperature, \"num_predict\": max_tokens},\n",
    "    )\n",
    "\n",
    "# ðŸ§ª Exemple d'appel\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"Tu es un assistant concis\"},\n",
    "    {\"role\": \"user\", \"content\": \"Donne-moi la capitale de lâ€™Italie\"}\n",
    "]\n",
    "print(_call_llm(messages)[\"message\"][\"content\"].strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec4b714",
   "metadata": {},
   "source": [
    "## ðŸ”— SÃ©quenceâ€¯1.8Â â€“ Assemblage miniâ€‘RAG (prototype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f90fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask(question: str, chunks: List[str], vecs: np.ndarray, top_k: int = 3):\n",
    "    # Recherche des chunks pertinents\n",
    "    q_vec = embed_texts([question])[0]\n",
    "    _, I = index.search(q_vec[None, :], top_k)\n",
    "    ctx = [chunks[i] for i in I[0]]\n",
    "    # PrÃ©paration du prompt\n",
    "    prompt = build_prompt(question, ctx)\n",
    "    # Appel LLM et retour de la rÃ©ponse\n",
    "    answer = _call_llm(prompt)[\"message\"][\"content\"].strip()\n",
    "    return answer, I[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c72002e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title ðŸ§ª Prototype RAG sur le PDF\n",
    "question = \"Quel est le thÃ¨me principal de ce document ?\"\n",
    "print(ask(question, chunks, chunk_vecs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8c177e",
   "metadata": {},
   "source": [
    "---\n",
    "# ðŸ› ï¸ Partieâ€¯2Â â€“ Mise en production avec `app_finale.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027db9b9",
   "metadata": {},
   "source": [
    "## ðŸ—ï¸ SÃ©quenceâ€¯2.1Â â€“ PrÃ©parer lâ€™environnement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a106056c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title âš™ï¸ OptionnelÂ : crÃ©er un virtualenv local\n",
    "# !python -m venv venv && source venv/bin/activate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8131b6e",
   "metadata": {},
   "source": [
    "## ðŸ“¦ SÃ©quenceâ€¯2.2Â â€“ RÃ©cupÃ©rer `app_finale.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec6a38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title ðŸ“¥ Copier le script final\n",
    "%cp FormationEYAI/app_finale.py ./app_finale.py\n",
    "!ls -l app_finale.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a04218",
   "metadata": {},
   "source": [
    "## ðŸ¤– SÃ©quenceâ€¯2.3Â â€“ DÃ©marrer Ollama en production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf55a6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title ðŸš€ Lancement Ollama en arriÃ¨reâ€‘plan\n",
    "import subprocess, time, os, signal\n",
    "ollama_proc = subprocess.Popen(\"ollama serve\", shell=True)\n",
    "time.sleep(5)\n",
    "print('âœ… Ollama est prÃªt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3063d997",
   "metadata": {},
   "source": [
    "## ðŸ–¥ï¸ SÃ©quenceâ€¯2.4Â â€“ Lancer lâ€™application Streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98cfe6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title ðŸŽ›ï¸ Run Streamlit + LocalTunnel\n",
    "!pip install -q streamlit localtunnel\n",
    "!streamlit run app_finale.py &>/content/logs.txt & npx localtunnel --port 8501"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b0c256",
   "metadata": {},
   "source": [
    "## ðŸ“ˆ SÃ©quenceâ€¯2.5Â â€“ ObservabilitÃ©"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c38cea7",
   "metadata": {},
   "source": [
    "- Temps de rÃ©ponse, logs Streamlit (`tail -f content/logs.txt`)  \n",
    "- SÃ©curitÃ© des donnÃ©es\n",
    "- CoÃ»t GPU / CPU\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a5032b",
   "metadata": {},
   "source": [
    "## ðŸŽ“ Miniâ€‘projet final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fcd64b",
   "metadata": {},
   "source": [
    "Ajoutez un **second modÃ¨le dâ€™embedding** ou la prise en charge dâ€™un format `.docx`, puis prÃ©sentez vos rÃ©sultats.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
