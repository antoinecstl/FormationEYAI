{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00b11ac5",
   "metadata": {},
   "source": [
    "# 🤖 Formation RAG – Notebook intégral"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e049e283",
   "metadata": {},
   "source": [
    "---\n",
    "## 🎯 Objectifs pédagogiques\n",
    "- Comprendre chaque composant d’un **RAG**\n",
    "- Manipuler le code dans Colab\n",
    "- Installer **Ollama** et les modèles requis\n",
    "- Tester un prototype sur un PDF d’exemple\n",
    "- Découvrir les étapes de mise en production (Partie 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928f3a19",
   "metadata": {},
   "source": [
    "## 🚧 Séquence 1.0 – Installation d’Ollama & des modèles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02783f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title 🛠️ Installation Ollama (linux/Colab)\n",
    "!curl -fsSL https://ollama.com/install.sh | sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d2d146",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title 🚀 Lancement Ollama en arrière‑plan\n",
    "import subprocess, time\n",
    "ollama_proc = subprocess.Popen(\"ollama serve\", shell=True)\n",
    "time.sleep(5)\n",
    "print('✅ Ollama est prêt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef6ba77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title 📥 Téléchargement des modèles\n",
    "!ollama pull llama3.2:3B\n",
    "!ollama pull nomic-embed-text:latest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08aca83",
   "metadata": {},
   "source": [
    "## 🔗 Séquence 1.1 – Bootstrap Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac33f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title 🔌 Connexion Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2bb2a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title 📥 Clone du dépôt\n",
    "%cd /content/gdrive/MyDrive\n",
    "!git clone https://github.com/antoinecstl/FormationEYAI.git\n",
    "%cd FormationEYAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7a32c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title 🛠️ Installation des dépendances\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d5143d",
   "metadata": {},
   "source": [
    "## 🔍 Séquence 1.2 – Bases du RAG : embeddings & similarité"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360710df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, ollama\n",
    "\n",
    "EMBED_MODEL = \"nomic-embed-text:latest\"\n",
    "\n",
    "def embed_texts(texts):\n",
    "    \"\"\"Retourne un np.ndarray shape (n, d)\"\"\"\n",
    "    return np.array([ollama.embeddings(model=EMBED_MODEL, prompt=t)['embedding'] for t in texts], dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f380f8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'embed_texts' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m phrase1 = \u001b[33m\"\u001b[39m\u001b[33mLe soleil brille\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      3\u001b[39m phrase2 = \u001b[33m\"\u001b[39m\u001b[33mAujourd\u001b[39m\u001b[33m'\u001b[39m\u001b[33mhui il fait beau\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m vecs = \u001b[43membed_texts\u001b[49m([phrase1, phrase2])\n\u001b[32m      6\u001b[39m sim = \u001b[38;5;28mfloat\u001b[39m(vecs[\u001b[32m0\u001b[39m] @ vecs[\u001b[32m1\u001b[39m] / (np.linalg.norm(vecs[\u001b[32m0\u001b[39m])*np.linalg.norm(vecs[\u001b[32m1\u001b[39m])))\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSimilarité : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msim\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'embed_texts' is not defined"
     ]
    }
   ],
   "source": [
    "# @title 🔬 Test d'embedding\n",
    "# @markdown `phrase1` et `phrase2` sont des phrases à comparer.\n",
    "# @markdown La similarité entre les phrases est calculée en utilisant le produit scalaire des\n",
    "# @markdown embeddings normalisés de chaque phrase.\n",
    "# @markdown La similarité est un nombre entre -1 et 1, où 1 signifie que les phrases sont très similaires,\n",
    "# @markdown 0 signifie qu'elles ne sont pas similaires, et -1 signifie qu'elles sont opposées.\n",
    "# @markdown Vous pouvez modifier les phrases pour tester d'autres exemples.\n",
    "# @markdown Exécutez la cellule pour voir le résultat.\n",
    "\n",
    "phrase1 = \"The cat is sleeping on the sofa\"\n",
    "phrase2 = \"A cat is napping on the couch\"\n",
    "\n",
    "vecs = embed_texts([phrase1, phrase2])\n",
    "sim = float(vecs[0] @ vecs[1] / (np.linalg.norm(vecs[0])*np.linalg.norm(vecs[1])))\n",
    "print(f\"Similarité : {sim:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad5dfe6",
   "metadata": {},
   "source": [
    "## 📐 Séquence 1.3 – Chunking & nettoyage d’un PDF d’exemple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdede86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "def auto_chunk_size(tok:int)->int:\n",
    "    return 1024 if tok<8000 else 768 if tok<20000 else 512\n",
    "\n",
    "def chunk_document(text:str):\n",
    "    size=auto_chunk_size(len(text.split()))\n",
    "    splitter=RecursiveCharacterTextSplitter(\n",
    "        separators=[\"\\n\\n\",\"\\n\",\". \"],\n",
    "        chunk_size=size,\n",
    "        chunk_overlap=size//4,\n",
    "        length_function=len,\n",
    "        )\n",
    "    return [c for c in splitter.split_text(text) if len(c)>100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a544cc02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title 📖 Chargement du PDF d'exemple\n",
    "from PyPDF2 import PdfReader\n",
    "\n",
    "sample_path = \"rapport.pdf\"  # fourni dans le repo\n",
    "pages = PdfReader(sample_path).pages\n",
    "full_text = \"\\n\".join(p.extract_text() or \"\" for p in pages)\n",
    "\n",
    "print(f\"📄 Le document contient {len(full_text.split())} tokens environ.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d93bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title 🌳 Chunking du PDF\n",
    "chunks = chunk_document(full_text)\n",
    "print(f\"🌳 {len(chunks)} chunks créés. Aperçu :\\n{chunks[0][:300]}…\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5e0cab",
   "metadata": {},
   "source": [
    "## 📊 Séquence 1.4 – Index vectoriel FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce563806",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss, numpy as np\n",
    "\n",
    "def build_faiss_index(vectors:np.ndarray)->faiss.IndexFlatIP:\n",
    "    d=vectors.shape[1]\n",
    "    idx=faiss.IndexFlatIP(d)\n",
    "    idx.add(vectors.astype('float32'))\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202c0cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title 🧪 Construction index chunks\n",
    "import numpy as np\n",
    "\n",
    "chunk_vecs = embed_texts(chunks)\n",
    "index = build_faiss_index(chunk_vecs)\n",
    "\n",
    "print(index.ntotal, \"vecteurs dans l'index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea4bdc9",
   "metadata": {},
   "source": [
    "## 🧮 Séquence 1.5 – Algorithme MMR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed16fd2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def mmr(query_vec:np.ndarray, cand:np.ndarray, k:int=5, λ:float=0.3):\n",
    "    selected, rest = [], list(range(len(cand)))\n",
    "    while len(selected)<min(k,len(rest)):\n",
    "        best, best_score = None, -1e9\n",
    "        for idx in rest:\n",
    "            sim_q = float(query_vec @ cand[idx]/(np.linalg.norm(query_vec)*np.linalg.norm(cand[idx])+1e-6))\n",
    "            sim_s = max(cosine_similarity(cand[idx][None,:], cand[selected])[0]) if selected else 0.\n",
    "            score = λ*sim_q - (1-λ)*sim_s\n",
    "            if score>best_score:\n",
    "                best, best_score = idx, score\n",
    "        selected.append(best); rest.remove(best)\n",
    "    return selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad559f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title 🔬 Test MMR\n",
    "import numpy as np\n",
    "\n",
    "q_vec = embed_texts([\"Sujet principal du rapport ?\"])[0]\n",
    "sel = mmr(q_vec, chunk_vecs, 3)\n",
    "print(sel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e41f64f",
   "metadata": {},
   "source": [
    "## 🧑‍🎤 Séquence 1.6 – Prompt engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab34cd5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7085934",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(question:str, ctxs:list[str]):\n",
    "    ctx_block=\"\\n\\n\".join(f\"[{i+1}] {c}\" for i,c in enumerate(ctxs))\n",
    "    system=\"Vous êtes un assistant expert. Utilisez uniquement les informations suivantes pour répondre en français. Citez les sources [n].\"\n",
    "    return [\n",
    "        {\"role\":\"system\",\"content\":system},\n",
    "        {\"role\":\"user\",\"content\":f\"CONTEXTE(S):\\n{ctx_block}\\n\\nQUESTION: {question}\\n\\nRéponse:\"}\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84c1dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title 🔬 Prompt test (sans LLM pour l'instant)\n",
    "print(build_prompt(\"Pourquoi le ciel est bleu ?\", [\"La diffusion Rayleigh explique la couleur du ciel.\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda2bf49",
   "metadata": {},
   "source": [
    "## 🧑‍🎤 Séquence 1.7 – Premier Appel au LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d80ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"llama3.2:3B\"\n",
    "\n",
    "def _call_llm(messages: List[Dict[str, str]], *, temperature: float = 0.1, max_tokens: int = 2048, stream: bool = False):\n",
    "    \"\"\"Enveloppe simple autour de ollama.chat pour usage direct.\"\"\"\n",
    "    return ollama.chat(\n",
    "        model=MODEL_NAME,\n",
    "        messages=messages,\n",
    "        stream=stream,\n",
    "        options={\"temperature\": temperature, \"num_predict\": max_tokens},\n",
    "    )\n",
    "\n",
    "# 🧪 Exemple d'appel\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"Tu es un assistant concis\"},\n",
    "    {\"role\": \"user\", \"content\": \"Donne-moi la capitale de l’Italie\"}\n",
    "]\n",
    "print(_call_llm(messages)[\"message\"][\"content\"].strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec4b714",
   "metadata": {},
   "source": [
    "## 🔗 Séquence 1.8 – Assemblage mini‑RAG (prototype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f90fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask(question: str, chunks: List[str], vecs: np.ndarray, top_k: int = 3):\n",
    "    # Recherche des chunks pertinents\n",
    "    q_vec = embed_texts([question])[0]\n",
    "    _, I = index.search(q_vec[None, :], top_k)\n",
    "    ctx = [chunks[i] for i in I[0]]\n",
    "    # Préparation du prompt\n",
    "    prompt = build_prompt(question, ctx)\n",
    "    # Appel LLM et retour de la réponse\n",
    "    answer = _call_llm(prompt)[\"message\"][\"content\"].strip()\n",
    "    return answer, I[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c72002e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title 🧪 Prototype RAG sur le PDF\n",
    "question = \"Quel est le thème principal de ce document ?\"\n",
    "print(ask(question, chunks, chunk_vecs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8c177e",
   "metadata": {},
   "source": [
    "---\n",
    "# 🛠️ Partie 2 – Mise en production avec `app_finale.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027db9b9",
   "metadata": {},
   "source": [
    "## 🏗️ Séquence 2.1 – Préparer l’environnement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a106056c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title ⚙️ Optionnel : créer un virtualenv local\n",
    "# !python -m venv venv && source venv/bin/activate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8131b6e",
   "metadata": {},
   "source": [
    "## 📦 Séquence 2.2 – Récupérer `app_finale.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec6a38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title 📥 Copier le script final\n",
    "%cp FormationEYAI/app_finale.py ./app_finale.py\n",
    "!ls -l app_finale.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a04218",
   "metadata": {},
   "source": [
    "## 🤖 Séquence 2.3 – Démarrer Ollama en production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf55a6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title 🚀 Lancement Ollama en arrière‑plan\n",
    "import subprocess, time, os, signal\n",
    "ollama_proc = subprocess.Popen(\"ollama serve\", shell=True)\n",
    "time.sleep(5)\n",
    "print('✅ Ollama est prêt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3063d997",
   "metadata": {},
   "source": [
    "## 🖥️ Séquence 2.4 – Lancer l’application Streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98cfe6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title 🎛️ Run Streamlit + LocalTunnel\n",
    "!pip install -q streamlit localtunnel\n",
    "!streamlit run app_finale.py &>/content/logs.txt & npx localtunnel --port 8501"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b0c256",
   "metadata": {},
   "source": [
    "## 📈 Séquence 2.5 – Observabilité"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c38cea7",
   "metadata": {},
   "source": [
    "- Temps de réponse, logs Streamlit (`tail -f content/logs.txt`)  \n",
    "- Sécurité des données\n",
    "- Coût GPU / CPU\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a5032b",
   "metadata": {},
   "source": [
    "## 🎓 Mini‑projet final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fcd64b",
   "metadata": {},
   "source": [
    "Ajoutez un **second modèle d’embedding** ou la prise en charge d’un format `.docx`, puis présentez vos résultats.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
