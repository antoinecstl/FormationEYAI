{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "520257ac",
   "metadata": {},
   "source": [
    "# 🤖 Formation RAG PDF Chat - VERSION CORRIGÉE\n",
    "\n",
    "## Guide pour l'instructeur\n",
    "Ce document contient toutes les solutions des exercices. À utiliser pour aider les étudiants pendant la formation.\n",
    "\n",
    "## Architecture du système\n",
    "```\n",
    "PDF → Chunking → Embeddings → Vector Store → Retrieval → LLM → Response\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610d97cb",
   "metadata": {},
   "source": [
    "## 0. Configuration Google Colab\n",
    "\n",
    "### Installation d'Ollama et des modèles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5619d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation d'Ollama sur Google Colab\n",
    "import subprocess\n",
    "import time\n",
    "import os\n",
    "\n",
    "print(\"🚀 Installation d'Ollama...\")\n",
    "\n",
    "# Installation d'Ollama\n",
    "subprocess.run([\"curl\", \"-fsSL\", \"https://ollama.ai/install.sh\"], stdout=subprocess.PIPE)\n",
    "subprocess.run([\"sh\", \"-c\", \"curl -fsSL https://ollama.ai/install.sh | sh\"], shell=True)\n",
    "\n",
    "# Démarrage du service Ollama en arrière-plan\n",
    "print(\"🔄 Démarrage du service Ollama...\")\n",
    "process = subprocess.Popen([\"ollama\", \"serve\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "time.sleep(5)  # Attendre que le service démarre\n",
    "\n",
    "print(\"📥 Téléchargement des modèles...\")\n",
    "# Pull des modèles nécessaires\n",
    "subprocess.run([\"ollama\", \"pull\", \"llama3.2:3b\"])\n",
    "subprocess.run([\"ollama\", \"pull\", \"nomic-embed-text:latest\"])\n",
    "\n",
    "print(\"✅ Installation terminée!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd19acd",
   "metadata": {},
   "source": [
    "## 1. Installation des dépendances Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9753c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation des dépendances\n",
    "!pip install streamlit faiss-cpu numpy PyPDF2 langchain scikit-learn ollama-python\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import tempfile\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "\n",
    "import faiss  # type: ignore\n",
    "import numpy as np\n",
    "import streamlit as st\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ac82cc",
   "metadata": {},
   "source": [
    "## 2. Configuration de l'Interface - SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5e5097",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration de la page Streamlit\n",
    "st.set_page_config(page_title=\"🤖 RAG PDF Chat\", page_icon=\"🤖\", layout=\"wide\")\n",
    "\n",
    "# SOLUTION: CSS complet pour l'interface\n",
    "st.markdown(\n",
    "    \"\"\"\n",
    "<style>\n",
    "html, body {\n",
    "    font-family: 'Helvetica Neue', sans-serif;\n",
    "    background-color: #f4f7fa;\n",
    "}\n",
    ".sidebar .sidebar-content {\n",
    "    background-color: #ffffff;\n",
    "}\n",
    ".chat-container {\n",
    "    max-width: 1200px;\n",
    "    margin: auto;\n",
    "    padding: 1rem;\n",
    "}\n",
    ".user-msg {\n",
    "    background: #007bff;\n",
    "    color: white;\n",
    "    border-radius: 20px 20px 0px 20px;\n",
    "    padding: 1rem;\n",
    "    margin: 0.5rem 0;\n",
    "    align-self: flex-end;\n",
    "    max-width: 100%;\n",
    "    word-wrap: break-word;\n",
    "}\n",
    ".bot-msg {\n",
    "    background: #e9ecef;\n",
    "    color: #212529;\n",
    "    border-radius: 20px 20px 20px 0px;\n",
    "    padding: 1rem;\n",
    "    margin: 0.5rem 0;\n",
    "    align-self: flex-start;\n",
    "    max-width: 100%;\n",
    "    word-wrap: break-word;\n",
    "}\n",
    ".chat-area {\n",
    "    display: flex;\n",
    "    flex-direction: column;\n",
    "}\n",
    "</style>\n",
    "\"\"\",\n",
    "    unsafe_allow_html=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c9b273",
   "metadata": {},
   "source": [
    "## 3. Configuration des modèles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a86fed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration des modèles et paramètres\n",
    "MODEL_NAME = \"llama3.2:3b\"\n",
    "EMBEDDING_MODEL = \"nomic-embed-text:latest\"\n",
    "DOC_TOP_K = 3\n",
    "CHUNK_TOP_K = 5\n",
    "CANDIDATES_K = 20\n",
    "NEIGHBORS = 1\n",
    "LAMBDA_DIVERSITY = 0.3\n",
    "SIM_THRESHOLD = 0.25\n",
    "TEMPERATURE = 0.2\n",
    "MAX_TOKENS = 2048"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff861d3",
   "metadata": {},
   "source": [
    "## 4. Fonctions LLM - SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4429741",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION: Implémentation complète des fonctions LLM\n",
    "def _call_llm(messages: List[Dict[str, str]], *, temperature: float = 0.1, max_tokens: int = 2048, stream: bool = False):\n",
    "    \"\"\"Appel au modèle de langage via Ollama\"\"\"\n",
    "    return ollama.chat(\n",
    "        model=MODEL_NAME,\n",
    "        messages=messages,\n",
    "        stream=stream,\n",
    "        options={\"temperature\": temperature, \"num_predict\": max_tokens},\n",
    "    )\n",
    "\n",
    "def embed_texts(texts: List[str]) -> np.ndarray:\n",
    "    \"\"\"Génère les embeddings pour une liste de textes\"\"\"\n",
    "    return np.array([\n",
    "        ollama.embeddings(model=EMBEDDING_MODEL, prompt=t)[\"embedding\"] \n",
    "        for t in texts\n",
    "    ], dtype=\"float32\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2841bff6",
   "metadata": {},
   "source": [
    "## 5. Traitement PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504d0c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"Nettoie le texte extrait du PDF\"\"\"\n",
    "    import re\n",
    "    text = re.sub(r\"\\n{2,}\", \"\\n\\n\", text)\n",
    "    text = re.sub(r\"(\\w+)-\\n(\\w+)\", r\"\\1\\2\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    return text.strip()\n",
    "\n",
    "def extract_pdf_text(path: str) -> str:\n",
    "    \"\"\"Extrait le texte d'un fichier PDF\"\"\"\n",
    "    return clean_text(\"\\n\".join(page.extract_text() or \"\" for page in PdfReader(path).pages))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026c29f0",
   "metadata": {},
   "source": [
    "## 6. Chunking - SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c0fe58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION: Implémentation du chunking adaptatif\n",
    "def auto_chunk_size(tokens: int) -> int:\n",
    "    \"\"\"Détermine la taille optimale des chunks\"\"\"\n",
    "    return 1024 if tokens < 8000 else 768 if tokens < 20000 else 512\n",
    "\n",
    "def chunk_document(text: str) -> List[str]:\n",
    "    \"\"\"Segmente un document en chunks avec chevauchement\"\"\"\n",
    "    size = auto_chunk_size(len(text.split()))\n",
    "    \n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        separators=[\"\\n\\n\", \"\\n\", \". \"],\n",
    "        chunk_size=size,\n",
    "        chunk_overlap=int(size*0.25),\n",
    "        length_function=len,\n",
    "    )\n",
    "    \n",
    "    return [c for c in splitter.split_text(text) if len(c) > 100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1ffd3e",
   "metadata": {},
   "source": [
    "## 7. Génération de résumés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238dd8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_summary(text: str) -> str:\n",
    "    \"\"\"Génère un résumé structuré du document\"\"\"\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\", \n",
    "            \"content\": \"Vous êtes un expert en synthèse documentaire. Résumez le texte suivant en trois parties : (1) Contexte, (2) Points clés, (3) Conclusions. Répondez en français.\"\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": text[:120000]}\n",
    "    ]\n",
    "    return _call_llm(messages)[\"message\"][\"content\"].strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a0c6b2",
   "metadata": {},
   "source": [
    "## 8. Classe RagIndex - SOLUTION COMPLÈTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda2c5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION: Classe RagIndex complète\n",
    "class RagIndex:\n",
    "    \"\"\"Classe principale pour l'indexation et la recherche RAG\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.doc_index: Optional[faiss.IndexFlatIP] = None\n",
    "        self.chunk_index: Optional[faiss.Index] = None\n",
    "        self.doc_meta: List[Dict[str, Any]] = []\n",
    "        self.chunk_meta: List[Dict[str, Any]] = []\n",
    "        self.chunk_emb: Optional[np.ndarray] = None\n",
    "\n",
    "    def build(self, uploaded_files: List[st.runtime.uploaded_file_manager.UploadedFile]):\n",
    "        \"\"\"Construit l'index à partir des fichiers uploadés\"\"\"\n",
    "        doc_embs, chunk_embs_list = [], []\n",
    "        \n",
    "        for doc_id, uf in enumerate(uploaded_files):\n",
    "            # Sauvegarde temporaire du fichier\n",
    "            with tempfile.NamedTemporaryFile(delete=False, suffix=\".pdf\") as tmp:\n",
    "                tmp.write(uf.getbuffer())\n",
    "                path = tmp.name\n",
    "            \n",
    "            # SOLUTION: Extraction et traitement du document\n",
    "            full_text = extract_pdf_text(path)\n",
    "            os.unlink(path)\n",
    "\n",
    "            # SOLUTION: Génération du résumé et stockage des métadonnées\n",
    "            summary = make_summary(full_text)\n",
    "            self.doc_meta.append({\"filename\": uf.name, \"summary\": summary})\n",
    "            doc_embs.append(embed_texts([summary])[0])\n",
    "\n",
    "            # SOLUTION: Chunking et embeddings des chunks\n",
    "            chunks = chunk_document(full_text)\n",
    "            chunk_embs = embed_texts(chunks)\n",
    "            chunk_embs_list.append(chunk_embs)\n",
    "            \n",
    "            for i, txt in enumerate(chunks):\n",
    "                self.chunk_meta.append({\"doc_id\": doc_id, \"text\": txt, \"chunk_id\": i})\n",
    "\n",
    "        # SOLUTION: Création des index FAISS\n",
    "        self.doc_index = faiss.IndexFlatIP(len(doc_embs[0]))\n",
    "        self.doc_index.add(np.vstack(doc_embs).astype(\"float32\"))\n",
    "\n",
    "        self.chunk_emb = np.vstack(chunk_embs_list).astype(\"float32\")\n",
    "        self.chunk_index = faiss.IndexHNSWFlat(self.chunk_emb.shape[1], 32)\n",
    "        self.chunk_index.add(self.chunk_emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea9be87",
   "metadata": {},
   "source": [
    "## 9. Détection de questions globales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514caf85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _is_global(self, query: str, thr: float = 0.78) -> bool:\n",
    "    \"\"\"Détermine si une question porte sur l'ensemble du document\"\"\"\n",
    "    examples = [\n",
    "        \"De quoi parle ce document ?\",\n",
    "        \"Quel est le sujet principal ?\",\n",
    "        \"Fais un résumé du document\",\n",
    "    ]\n",
    "    \n",
    "    emb_q = embed_texts([query])[0]\n",
    "    emb_ex = embed_texts(examples)\n",
    "    \n",
    "    # Calcul de similarité cosinus\n",
    "    sims = emb_ex @ emb_q / (np.linalg.norm(emb_ex, axis=1) * np.linalg.norm(emb_q) + 1e-6)\n",
    "    return float(np.max(sims)) >= thr\n",
    "\n",
    "# Ajout à la classe RagIndex\n",
    "RagIndex._is_global = _is_global"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2924a8cd",
   "metadata": {},
   "source": [
    "## 10. Algorithme MMR - SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec69cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION: Implémentation complète de MMR\n",
    "def _mmr(self, q: np.ndarray, cand: np.ndarray, k: int) -> List[int]:\n",
    "    \"\"\"Maximum Marginal Relevance pour diversifier les résultats\"\"\"\n",
    "    selected, rest = [], list(range(len(cand)))\n",
    "    \n",
    "    while len(selected) < min(k, len(rest)):\n",
    "        best, best_score = None, -1e9\n",
    "        \n",
    "        for idx in rest:\n",
    "            # SOLUTION: Calcul de similarité avec la question\n",
    "            sim_q = float(q @ cand[idx] / (np.linalg.norm(q) * np.linalg.norm(cand[idx]) + 1e-6))\n",
    "            \n",
    "            # SOLUTION: Calcul de similarité maximale avec les sélectionnés\n",
    "            sim_s = max(cosine_similarity(cand[idx][None, :], cand[selected])[0]) if selected else 0.\n",
    "            \n",
    "            # SOLUTION: Score MMR\n",
    "            score = LAMBDA_DIVERSITY * sim_q - (1-LAMBDA_DIVERSITY) * sim_s\n",
    "            \n",
    "            if score > best_score:\n",
    "                best, best_score = idx, score\n",
    "        \n",
    "        selected.append(best)\n",
    "        rest.remove(best)\n",
    "        \n",
    "    return selected\n",
    "\n",
    "# Ajout à la classe RagIndex\n",
    "RagIndex._mmr = _mmr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccad2286",
   "metadata": {},
   "source": [
    "## 11. Fonction retrieve - SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082372d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION: Implémentation complète de la recherche\n",
    "def retrieve(self, query: str) -> Tuple[List[str], List[int], Optional[str]]:\n",
    "    \"\"\"Recherche les contextes pertinents pour une question\"\"\"\n",
    "    # SOLUTION: Génération de l'embedding de la question\n",
    "    q_emb = embed_texts([query])[0]\n",
    "    \n",
    "    # SOLUTION: Recherche des documents pertinents\n",
    "    _, I_doc = self.doc_index.search(q_emb[None, :], DOC_TOP_K)\n",
    "    allowed = set(I_doc[0])\n",
    "\n",
    "    # SOLUTION: Filtrage des chunks par document\n",
    "    mask = [i for i, m in enumerate(self.chunk_meta) if m[\"doc_id\"] in allowed]\n",
    "    sub_emb = self.chunk_emb[mask]\n",
    "    sub_idx = faiss.IndexFlatIP(sub_emb.shape[1])\n",
    "    sub_idx.add(sub_emb)\n",
    "    \n",
    "    # SOLUTION: Recherche des chunks candidats\n",
    "    D, I = sub_idx.search(q_emb[None, :], min(CANDIDATES_K, len(mask)))\n",
    "    pool = [mask[idx] for idx in I[0]]\n",
    "    \n",
    "    # SOLUTION: Application du seuil de similarité\n",
    "    pool = [idx for idx, d in zip(pool, D[0]) if 1-d <= SIM_THRESHOLD] or [mask[I[0][0]]]\n",
    "\n",
    "    # SOLUTION: Application de MMR\n",
    "    cand_emb = self.chunk_emb[pool]\n",
    "    selected = [pool[i] for i in self._mmr(q_emb, cand_emb, CHUNK_TOP_K)]\n",
    "    \n",
    "    # SOLUTION: Expansion du contexte\n",
    "    expanded = {j for idx in selected for j in range(idx-NEIGHBORS, idx+NEIGHBORS+1)}\n",
    "    final = [i for i in expanded if 0 <= i < len(self.chunk_meta)][:CHUNK_TOP_K]\n",
    "\n",
    "    # SOLUTION: Construction des contextes finaux\n",
    "    contexts = [self.chunk_meta[i][\"text\"] for i in final]\n",
    "    summary = self.doc_meta[int(I_doc[0][0])][\"summary\"] if self._is_global(query) else None\n",
    "    \n",
    "    return contexts, final, summary\n",
    "\n",
    "# Ajout à la classe RagIndex\n",
    "RagIndex.retrieve = retrieve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463b9a69",
   "metadata": {},
   "source": [
    "## 12. Construction des prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d1d708",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(question: str, contexts: List[str], summary: Optional[str], history: List[Dict[str, str]] = None):\n",
    "    \"\"\"Construit le prompt pour le modèle de langage\"\"\"\n",
    "    ctx_block = \"\\n\\n\".join(f\"[{i+1}] {c}\" for i, c in enumerate(contexts))\n",
    "    \n",
    "    if summary:\n",
    "        ctx_block = f\"[Résumé] {summary}\\n\\n\" + ctx_block\n",
    "    \n",
    "    system = (\n",
    "        \"Vous êtes un assistant expert. Utilisez uniquement les informations suivantes pour répondre en français. \"\n",
    "        \"Citez vos sources avec les balises [n]. Si l'information n'est pas trouvée, informez-en l'utilisateur.\"\n",
    "    )\n",
    "    \n",
    "    messages = [{\"role\": \"system\", \"content\": system}]\n",
    "    \n",
    "    if history and len(history) > 0:\n",
    "        previous_messages = history[:-1] if history[-1][\"role\"] == \"user\" else history\n",
    "        messages.extend(previous_messages)\n",
    "    \n",
    "    current_user_content = f\"CONTEXTE(S):\\n{ctx_block}\\n\\nQUESTION: {question}\\n\\nRéponse:\"\n",
    "    messages.append({\"role\": \"user\", \"content\": current_user_content})\n",
    "    \n",
    "    return messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90c1fe5",
   "metadata": {},
   "source": [
    "## 13. Interface Streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802ee5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialisation de l'état Streamlit\n",
    "if \"messages\" not in st.session_state:\n",
    "    st.session_state.messages = []\n",
    "if \"rag\" not in st.session_state:\n",
    "    st.session_state.rag = None\n",
    "if \"processing\" not in st.session_state:\n",
    "    st.session_state.processing = False\n",
    "\n",
    "# Sidebar pour l'upload de fichiers\n",
    "with st.sidebar:\n",
    "    st.header(\"📚 Documents\")\n",
    "    files = st.file_uploader(\"Déposez vos PDF\", type=[\"pdf\"], accept_multiple_files=True)\n",
    "    if st.button(\"🔄 Réinitialiser\"):\n",
    "        st.session_state.clear()\n",
    "        st.rerun()\n",
    "\n",
    "# Construction de l'index\n",
    "if files and st.session_state.rag is None:\n",
    "    with st.spinner(\"📄 Indexation en cours…\"):\n",
    "        rag = RagIndex()\n",
    "        rag.build(files)\n",
    "        st.session_state.rag = rag\n",
    "    st.success(f\"{len(files)} document(s) indexé(s) ! Posez vos questions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1410104",
   "metadata": {},
   "source": [
    "## 14. Interface de chat - SOLUTION COMPLÈTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39d844c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION: Interface de chat complète\n",
    "st.markdown(\"<div class='chat-container'>\", unsafe_allow_html=True)\n",
    "\n",
    "# Affichage des messages existants\n",
    "if not st.session_state.messages:\n",
    "    if st.session_state.rag is not None:\n",
    "        st.markdown(\n",
    "            \"\"\"\n",
    "            <div class=\"bot-msg\">\n",
    "            👋 Bonjour ! Je suis votre assistant IA documentaire.\n",
    "            <br>Posez-moi une question sur le contenu de vos documents.\n",
    "            </div>\n",
    "            \"\"\",\n",
    "            unsafe_allow_html=True\n",
    "        )\n",
    "    else:\n",
    "        st.markdown(\n",
    "            \"\"\"\n",
    "            <div class=\"bot-msg\">\n",
    "            👋 Bienvenue dans RAG PDF Chat !\n",
    "            <br>Commençez par télécharger un ou plusieurs documents PDF dans le panneau latéral.\n",
    "            </div>\n",
    "            \"\"\",\n",
    "            unsafe_allow_html=True\n",
    "        )\n",
    "else:\n",
    "    st.markdown(\"<div class='chat-area'>\", unsafe_allow_html=True)\n",
    "    for msg in st.session_state.messages:\n",
    "        css = \"user-msg\" if msg[\"role\"] == \"user\" else \"bot-msg\"\n",
    "        st.markdown(f'<div class=\"{css}\">{msg[\"content\"]}</div>', unsafe_allow_html=True)\n",
    "    st.markdown(\"</div>\", unsafe_allow_html=True)\n",
    "\n",
    "st.markdown(\"</div>\", unsafe_allow_html=True)\n",
    "\n",
    "# SOLUTION: Gestion de l'input utilisateur\n",
    "query = st.chat_input(\"Votre question…\", disabled=st.session_state.processing or st.session_state.rag is None)\n",
    "\n",
    "if query:\n",
    "    # SOLUTION: Traitement complet de la question\n",
    "    st.session_state.messages.append({\"role\": \"user\", \"content\": query})\n",
    "    st.markdown(f'<div class=\"user-msg\">{query}</div>', unsafe_allow_html=True)\n",
    "    st.session_state.processing = True\n",
    "\n",
    "    rag: RagIndex = st.session_state.rag\n",
    "    contexts, indices, summary = rag.retrieve(query)\n",
    "    \n",
    "    prompt = build_prompt(query, contexts, summary, st.session_state.messages)\n",
    "\n",
    "    placeholder = st.empty()\n",
    "    collected_parts: List[str] = []\n",
    "\n",
    "    # SOLUTION: Streaming de la réponse\n",
    "    for chunk in _call_llm(prompt, temperature=TEMPERATURE, max_tokens=MAX_TOKENS, stream=True):\n",
    "        token = chunk[\"message\"][\"content\"]\n",
    "        collected_parts.append(token)\n",
    "        placeholder.markdown(f'<div class=\"bot-msg\">{\"\".join(collected_parts)}</div>', unsafe_allow_html=True)\n",
    "\n",
    "    full_answer = \"\".join(collected_parts)\n",
    "    st.session_state.messages.append({\"role\": \"assistant\", \"content\": full_answer})\n",
    "    st.session_state.processing = False\n",
    "\n",
    "    # SOLUTION: Affichage des contextes\n",
    "    with st.expander(\"🔍 Contextes\"):\n",
    "        for i, ctx in enumerate(contexts):\n",
    "            st.text_area(f\"[{i+1}]\", ctx, height=120)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306cb8b3",
   "metadata": {},
   "source": [
    "## 15. Tests et validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223a8757",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests de validation\n",
    "print(\"✅ Configuration terminée\")\n",
    "print(f\"📊 Modèle LLM : {MODEL_NAME}\")\n",
    "print(f\"🔍 Modèle embeddings : {EMBEDDING_MODEL}\")\n",
    "print(\"🚀 Application prête pour les tests !\")\n",
    "\n",
    "# Pour Google Colab, utiliser cette commande pour lancer Streamlit :\n",
    "# !streamlit run formation_rag_chat_corrige.py --server.port 8501 &\n",
    "# !npx localtunnel --port 8501"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
